---
title: "Probability Models and EDA"
subtitle: "Lecture 03"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "January 28, 2026"
format:
    revealjs: default
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using LaTeXStrings
using Measures

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Review

## Statistics as Decision-Making Under Uncertainty

- Went over "standard" null hypothesis significance approach.
- Null vs. alternative hypotheses
- $p$-values: (continuous) assessment of probability of seeing test statistic under null hypothesis.

# Probability Models

## Why Do We Need Models For Data?

- **Data are imperfect**: Data $X$ are only one realization of the data that **could** have been observed and/or we can only measure indirect proxies of what we care about.
- Over time, statisticians learned to treat these imperfections as the results of **random** processes (for some of this history, see @Hacking1990-yr), requiring probability models.


## Predicting Random Variables

Let's say that we want to predict the value of a variable $y \sim Y$. We need a criteria to define the "best" point prediction.

Reasonable starting point: 

$$\text{MSE}(m) = \mathbb{E}\left[(Y - m)^2\right]$$

## MSE As (Squared) Bias + Variance

$$\begin{aligned}
\mathbb{V}(Y) &= \mathbb{E}\left[(Y - \mathbb{E}[Y])^2\right] \\
&= \mathbb{E}\left[Y^2 - 2Y\mathbb{E}[Y] + \mathbb{E}[Y]^2\right] \\
&= \mathbb{E}[Y^2] - 2\mathbb{E}[Y]^2 + \mathbb{E}[Y]^2 = \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 .
\end{aligned}$$

::: {.fragment .fade-in}
$$\Rightarrow \text{MSE}(m) = \mathbb{E}\left[(Y - m)^2\right] = \mathbb{E}\left[(Y-m)\right]^2 + \mathbb{V}(Y - m).$$
:::

## Bias-Variance Decomposition of MSE

Then:

$$\begin{aligned}
\text{MSE}(m) &= \mathbb{E}\left[(Y-m)\right]^2 + \mathbb{V}(Y - m)  \\
&= \mathbb{E}\left[(Y-m)\right]^2 + \mathbb{V}(Y) \\
&= \left(\mathbb{E}[Y] - m\right)^2 + \mathbb{V}(Y).
\end{aligned}$$

This is the source of the so-called "bias-variance tradeoff" (more on this later).

## Optimizing...

We want to find the minimum value of $\text{MSE}(m)$ (denote the optimal prediction by $\mu$):

::: {.fragment .fade-in}
$$
\begin{aligned}
\frac{d\text{MSE}}{dm} &= -2(\mathbb{E}[Y] - m) + 0 \\
0 = \left.\frac{d\text{MSE}}{dm}\right|_{m = \mu} &= -2(\mathbb{E}[Y] - \mu) 
\end{aligned}$$

$$\Rightarrow \mu = \mathbb{E}[Y].$$
:::

## Expected Value As Best Prediction

In other words, **the best predicted value of a random variable is its expectation**.

But:

1. We usually don't have enough data to know what $\mathbb{E}[Y]$ is: need to make probabilistic assumptions;
2. In many applications, we don't just want a point estimate, we need some estimate of ranges of values **we might observe**.

## Uses of Models

What can we use a probability model for?

1. **Summaries** of data: store $y = f(\mathbf{x})$ instead of all data points $(y, x_1, \ldots, x_n)$
2. **Smooth** values by removing noise
3. **Predict** new data (interpolation/extrapolation): $\hat{f} = f(\hat{\mathbf{x}})$
4. **Infer** relationships between variables (interpret coefficients of $f$)

## My Philosophical Position

:::: {.columns}
::: {.column width=50%}
- Probability theory helps us deduce logical implications of theories **conditional on our assumptions**
- Cannot use an "objective" procedure to avoid **subjective responsibility**
:::

::: {.column width=50%}
::: {.center}
![Spiderman Meme](memes/peter_parker_method_assumptions.png){width=90%}
:::
:::
::::

# Linear Regression

## Linear Regression

The "simplest" model is **linear**:

:::: {.columns}
::: {.column width=50%}
$$\begin{aligned}
y_i &\sim N(\mu_i, \sigma^2) \\
\mu_i &= \sum_j \beta_j x^j_i
\end{aligned}
$$
:::
::: {.column width=50%}
$$\begin{aligned}
y_i &= \sum_j \beta_j x^j_i + \varepsilon_i \\
\varepsilon_i &\sim N(0, \sigma^2)
\end{aligned}
$$
:::
::::


## Why Linear Models?

:::: {.columns}
::: {.column width=60%}
Two main reasons to use linear models/normal distributions:

1. **Inferential**: "Least informative" distribution assuming only finite mean/variance;
2. **Generative**: Central Limit Theorem (summed fluctuations are asymptotically normal)

:::
::: {.column width=40%}
![Weight stack Gaussian distribution](https://i.redd.it/zl5mo1n45wyb1.jpg)

::: {.caption}
Source: r/GymMemes
:::
:::
::::

::: {.notes}
One key thing: normal distributions are the "least informative" distribution given constraints on mean and variance. So all else being equal, this is a useful machine if all we're interested in are those two moments.
:::

## Linear Regression As Probability Model

$$
y &= \underbrace{\sum_j \beta_j x^j_i)}_{\mathbb{E}[y | x^j]} + \underbrace{\varepsilon_i}_{\text{noise}}, \quad \varepsilon_i \sim N(0, \sigma^2) 
$$

1. $\mathbb{E}[y | x^j]$: Best linear prediction of $y$ conditional on choice of predictors $x^j$.
2. The noise defines the probability distribution (here: Gaussian) of the observations: $$y \sim N(\sum_j \beta_j x^j_i), \sigma^2).$$


## Implications of Gaussian Noise

Without explicit modeling of sources of measurement uncertainty, in LR we can't separate a noisy system state (due to *e.g.* omitted variables) from an uncertain measurement error (more on this later).

$$
\begin{array}{l}
X \sim N(\mu_1, \sigma_1^2) \\
Y \sim N(\mu_2, \sigma_2^2) \\
Z = X + Y
\end{array}
\qquad \Rightarrow \qquad Z \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
$$

# Linear Model Example

## 


## Central Limit Theorem 

If 

- $\mathbb{E}[X_i] = \mu$ 
- and $\text{Var}(X_i) = \sigma^2 < \infty$, 

$$\begin{aligned}
\lim_{n \to \infty} \sqrt{n}(\bar{X}_n - \mu ) &= \mathcal{N}(0, \sigma^2) \\
\Rightarrow & \bar{X}_n \overset{\text{approx}}{\sim} \mathcal{N}(\mu, \sigma^2/n)
\end{aligned}$$

## Central Limit Theorem (More Intuitive)

:::: {.columns}
::: {.column width=50%}
For **a large enough set of samples**: 

The sampling distribution of a sum or mean of random variables is approximately normal distribution, **even if the random variables themselves are not**.
:::
::: {.column width=50%}
![Small n Meme](memes/sampling_distribution_small_n.jpg)

::: {.caption}
Source: Unknown
:::
:::
::::

# Describing Uncertainty

## Confidence Intervals

:::: {.columns}
::: {.column width=50%}
Frequentist estimates have **confidence intervals**, which will contain the "true" parameter value for $\alpha$% of data samples.

No guarantee that an individual CI contains the true value (with any "probability")!
:::

::: {.column width=50%}

![Horseshoe Illustration](https://www.wikihow.com/images/thumb/2/20/Throw-a-Horseshoe-Step-4-Version-4.jpg/aid448076-v4-728px-Throw-a-Horseshoe-Step-4-Version-4.jpg){width=90%}

::: {.caption}
Source: <https://www.wikihow.com/Throw-a-Horseshoe>
:::

:::
::::

::: {.notes}
Confidence intervals only capture uncertainty in **parameter inferences** due to data uncertainty, though this language sometimes gets misused to also refer to data/estimand uncertainty. 

:::


## Example: 95% CIs for N(0.4, 2)

```{julia}
#| label: fig-cis
#| code-fold: true
#| code-overflow: wrap
#| echo: true
#| layout-ncol: 2
#| fig-cap: "Display of 95% confidence intervals"
#| fig-subcap: 
#|  - "Sample Size 100"
#|  - "Sample Size 1,000"

# set up distribution
mean_true = 0.4
n_cis = 100 # number of CIs to compute
dist = Normal(mean_true, 2)

# use sample size of 100
samples = rand(dist, (100, n_cis))
# mapslices broadcasts over a matrix dimension, could also use a loop
sample_means = mapslices(mean, samples; dims=1)
sample_sd = mapslices(std, samples; dims=1) 
mc_sd = 1.96 * sample_sd / sqrt(100)
mc_ci = zeros(n_cis, 2) # preallocate
for i = 1:n_cis
    mc_ci[i, 1] = sample_means[i] - mc_sd[i]
    mc_ci[i, 2] = sample_means[i] + mc_sd[i]
end
# find which CIs contain the true value
ci_true = (mc_ci[:, 1] .< mean_true) .&& (mc_ci[:, 2] .> mean_true)
# compute percentage of CIs which contain the true value
ci_frac1 = 100 * sum(ci_true) ./ n_cis

# plot CIs
p1 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:deepskyblue, label="95% Confidence Interval", title="Sample Size 100", yticks=:false, legend=:false)
for i = 2:n_cis
    if ci_true[i]
        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:deepskyblue, label=:false)
    else
        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)
    end
end
vline!(p1, [mean_true], color=:black, linewidth=2, linestyle=:dash, label="True Value") # plot true value as a vertical line
xaxis!(p1, "Estimate")
plot!(p1, size=(500, 350)) # resize to fit slide

# use sample size of 1000
samples = rand(dist, (1000, n_cis))
# mapslices broadcasts over a matrix dimension, could also use a loop
sample_means = mapslices(mean, samples; dims=1)
sample_sd = mapslices(std, samples; dims=1) 
mc_sd = 1.96 * sample_sd / sqrt(1000)
mc_ci = zeros(n_cis, 2) # preallocate
for i = 1:n_cis
    mc_ci[i, 1] = sample_means[i] - mc_sd[i]
    mc_ci[i, 2] = sample_means[i] + mc_sd[i]
end
# find which CIs contain the true value
ci_true = (mc_ci[:, 1] .< mean_true) .&& (mc_ci[:, 2] .> mean_true)
# compute percentage of CIs which contain the true value
ci_frac2 = 100 * sum(ci_true) ./ n_cis

# plot CIs
p2 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:deepskyblue, label="95% Confidence Interval", title="Sample Size 1,000", yticks=:false, legend=:false)
for i = 2:n_cis
    if ci_true[i]
        plot!(p2, [mc_ci[i, :]], [i, i], linewidth=2, color=:deepskyblue, label=:false)
    else
        plot!(p2, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)
    end
end
vline!(p2, [mean_true], color=:black, linewidth=2, linestyle=:dash, label="True Value") # plot true value as a vertical line
xaxis!(p2, "Estimate")
plot!(p2, size=(500, 350)) # resize to fit slide

display(p1)
display(p2)
```

`{julia} Int64(round(ci_frac1))`% of the CIs contain the true value (left) vs. `{julia} Int64(round(ci_frac2))`% (right)

## Predictive Intervals

:::: {.columns}
::: {.column width=50%}
**Predictive intervals** capture uncertainty in an estimand.

**With what probability would I see a particular outcome in the future?**

Often need to construct these using **simulation**.
:::
::: {.column width=50%}
```{julia}
#| label: fig-credible-interval
#| fig-cap: Two different 95% credible intervals.

plot(Gamma(7.5), linewidth=3, xlabel="Data/Parameter", label=:false, legend=:outerbottom)
q1 = quantile(Gamma(7.5), [0.05, 0.95])
q2 = quantile(Gamma(7.5), [0.01, 0.91])
q3 = quantile(Gamma(7.5), [0.09, 0.99])
gamma_pdf(x) = pdf(Gamma(7.5), x)
plot!(q1[1]:0.01:q1[2], gamma_pdf(q1[1]:0.01:q1[2]), fillrange=zero(q1[1]:0.01:q1[2]), alpha=0.2, label="90% Interval 1")
plot!(q2[1]:0.01:q2[2], gamma_pdf(q2[1]:0.01:q2[2]), fillrange=zero(q2[1]:0.01:q2[2]), alpha=0.2, label="90% Interval 2")
plot!(q3[1]:0.01:q3[2], gamma_pdf(q3[1]:0.01:q3[2]), fillrange=zero(q3[1]:0.01:q3[2]), alpha=0.2, label="90% Interval 3")
plot!(size=(600, 650))
```
:::
::::

::: {.notes}
Due to this non-uniqueness, the typical convention is to use the "equal tailed" interval based on quantiles.
:::



# Key Points

## Probability Models

- Almost anything we want to use data for involves an assumption about the underlying probability model for the data.
- Our goal this semester is to develop a toolkit to specify, test, and use these probability models.

# Upcoming Schedule

## Next Classes

**Wednesday**: Fitting models with maximum likelihood

**Friday**: Maximum likelihood uncertainty

## Assessments

**Homework 1** Due Friday, 2/6.

# References

## References (Scroll for Full List)
