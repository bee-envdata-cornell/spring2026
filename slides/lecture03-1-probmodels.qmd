---
title: "Probability Models and EDA"
subtitle: "Lecture 03"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "January 28, 2026"
format:
    revealjs: default
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using LaTeXStrings
using Measures

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Review

## Statistics as Decision-Making Under Uncertainty

- Went over "standard" null hypothesis significance approach.
- Null vs. alternative hypotheses
- $p$-values: (continuous) assessment of probability of seeing test statistic under null hypothesis.

# Probability Models

## Why Do We Need Models For Data?

- **Data are imperfect**: Data $X$ are only one realization of the data that **could** have been observed and/or we can only measure indirect proxies of what we care about.
- Over time, statisticians learned to treat these imperfections as the results of **random** processes (for some of this history, see @Hacking1990-yr), requiring probability models.


## Predicting Random Variables

Let's say that we want to predict the value of a variable $y \sim Y$. We need a criteria to define the "best" point prediction.

Reasonable starting point: 

$$\text{MSE}(m) = \mathbb{E}\left[(Y - m)^2\right]$$

## MSE As (Squared) Bias + Variance

$$\begin{aligned}
\mathbb{V}(Y) &= \mathbb{E}\left[(Y - \mathbb{E}[Y])^2\right] \\
&= \mathbb{E}\left[Y^2 - 2Y\mathbb{E}[Y] + \mathbb{E}[Y]^2\right] \\
&= \mathbb{E}[Y^2] - 2\mathbb{E}[Y]^2 + \mathbb{E}[Y]^2 = \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 .
\end{aligned}$$

::: {.fragment .fade-in}
$$\Rightarrow \text{MSE}(m) = \mathbb{E}\left[(Y - m)^2\right] = \mathbb{E}\left[(Y-m)\right]^2 + \mathbb{V}(Y - m).$$
:::

## Bias-Variance Decomposition of MSE

Then:

$$\begin{aligned}
\text{MSE}(m) &= \mathbb{E}\left[(Y-m)\right]^2 + \mathbb{V}(Y - m)  \\
&= \mathbb{E}\left[(Y-m)\right]^2 + \mathbb{V}(Y) \\
&= \left(\mathbb{E}[Y] - m\right)^2 + \mathbb{V}(Y).
\end{aligned}$$

This is the source of the so-called "bias-variance tradeoff" (more on this later).

## Optimizing...

We want to find the minimum value of $\text{MSE}(m)$ (denote the optimal prediction by $\mu$):

::: {.fragment .fade-in}
$$
\begin{aligned}
\frac{d\text{MSE}}{dm} &= -2(\mathbb{E}[Y] - m) + 0 \\
0 = \left.\frac{d\text{MSE}}{dm}\right|_{m = \mu} &= -2(\mathbb{E}[Y] - \mu) 
\end{aligned}$$

$$\Rightarrow \mu = \mathbb{E}[Y].$$
:::

## Expected Value As Best Prediction

In other words, **the best predicted value of a random variable is its expectation**.

But:

1. We usually don't have enough data to know what $\mathbb{E}[Y]$ is: need to make probabilistic assumptions;
2. In many applications, we don't just want a point estimate, we need some estimate of ranges of values **we might observe**.

## Uses of Models

What can we use a probability model for?

1. **Summaries** of data: store $y = f(\mathbf{x})$ instead of all data points $(y, x_1, \ldots, x_n)$
2. **Smooth** values by removing noise
3. **Predict** new data (interpolation/extrapolation): $\hat{f} = f(\hat{\mathbf{x}})$
4. **Infer** relationships between variables (interpret coefficients of $f$)

## My Philosophical Position

:::: {.columns}
::: {.column width=50%}
- Probability theory helps us deduce logical implications of theories **conditional on our assumptions**
- Cannot use an "objective" procedure to avoid **subjective responsibility**
:::

::: {.column width=50%}
::: {.center}
![Spiderman Meme](memes/peter_parker_method_assumptions.png){width=90%}
:::
:::
::::

# Linear Regression

## Linear Regression

The "simplest" model is **linear**:

:::: {.columns}
::: {.column width=50%}
$$\begin{aligned}
y_i &\sim N(\mu_i, \sigma^2) \\
\mu_i &= \sum_j \beta_j x^j_i
\end{aligned}
$$
:::
::: {.column width=50%}
$$\begin{aligned}
y_i &= \sum_j \beta_j x^j_i + \varepsilon_i \\
\varepsilon_i &\sim N(0, \sigma^2)
\end{aligned}
$$
:::
::::


## Why Linear Models?

:::: {.columns}
::: {.column width=60%}
Two main reasons to use linear models/normal distributions:

1. **Inferential**: "Least informative" distribution assuming only finite mean/variance;
2. **Generative**: Central Limit Theorem (summed fluctuations are asymptotically normal)

:::
::: {.column width=40%}
![Weight stack Gaussian distribution](https://i.redd.it/zl5mo1n45wyb1.jpg)

::: {.caption}
Source: r/GymMemes
:::
:::
::::

::: {.notes}
One key thing: normal distributions are the "least informative" distribution given constraints on mean and variance. So all else being equal, this is a useful machine if all we're interested in are those two moments.
:::

## Linear Regression As Probability Model

$$
y &= \underbrace{\sum_j \beta_j x^j_i)}_{\mathbb{E}[y | x^j]} + \underbrace{\varepsilon_i}_{\text{noise}}, \quad \varepsilon_i \sim N(0, \sigma^2) 
$$

1. $\mathbb{E}[y | x^j]$: Best linear prediction of $y$ conditional on choice of predictors $x^j$.
2. The noise defines the probability distribution (here: Gaussian) of the observations: $$y \sim N(\sum_j \beta_j x^j_i), \sigma^2).$$


## Implications of Gaussian Noise

Without explicit modeling of sources of measurement uncertainty, in LR we can't separate a noisy system state (due to *e.g.* omitted variables) from an uncertain measurement error (more on this later).

$$
\begin{array}{l}
X \sim N(\mu_1, \sigma_1^2) \\
Y \sim N(\mu_2, \sigma_2^2) \\
Z = X + Y
\end{array}
\qquad \Rightarrow \qquad Z \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
$$

# Linear Model Example

## 



# Key Points

## Probability Models

- Almost anything we want to use data for involves an assumption about the underlying probability model for the data.
- Our goal this semester is to develop a toolkit to specify, test, and use these probability models.

# Upcoming Schedule

## Next Classes

**Wednesday**: Fitting models with maximum likelihood

**Friday**: Maximum likelihood uncertainty

## Assessments

**Homework 1** Due Friday, 2/6.

# References

## References (Scroll for Full List)
