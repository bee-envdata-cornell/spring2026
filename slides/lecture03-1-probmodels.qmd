---
title: "Probability Models and Linear Regression"
subtitle: "Lecture 05"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "February 02, 2026"
format:
    revealjs: default
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using LaTeXStrings
using Measures
using DataFrames
using CSV   

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Review

## EDA and Data Visualization

- Is my data fit for purpose?
- What are features of data?
- EDA methods are valuable even once we start to fit models, to check appropriateness of assumptions.

# Probability Models

## Why Do We Need Models For Data?

- **Data are imperfect**: Data $X$ are only one realization of the data that **could** have been observed and/or we can only measure indirect proxies of what we care about.
- Over time, statisticians learned to treat these imperfections as the results of **random** processes (for some of this history, see @Hacking1990-yr), requiring probability models.


## Predicting Random Variables

Let's say that we want to predict the value of a variable $y \sim Y$. We need a criteria to define the "best" point prediction.

Reasonable starting point: 

$$\text{MSE}(m) = \mathbb{E}\left[(Y - m)^2\right]$$

## MSE As (Squared) Bias + Variance

$$\begin{aligned}
\mathbb{V}(Y) &= \mathbb{E}\left[(Y - \mathbb{E}[Y])^2\right] \\
&= \mathbb{E}\left[Y^2 - 2Y\mathbb{E}[Y] + \mathbb{E}[Y]^2\right] \\
&= \mathbb{E}[Y^2] - 2\mathbb{E}[Y]^2 + \mathbb{E}[Y]^2 = \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 .
\end{aligned}$$

::: {.fragment .fade-in}
$$\Rightarrow \text{MSE}(m) = \mathbb{E}\left[(Y - m)^2\right] = \mathbb{E}\left[(Y-m)\right]^2 + \mathbb{V}(Y - m).$$
:::

## Bias-Variance Decomposition of MSE

Then:

$$\begin{aligned}
\text{MSE}(m) &= \mathbb{E}\left[(Y-m)\right]^2 + \mathbb{V}(Y - m)  \\
&= \mathbb{E}\left[(Y-m)\right]^2 + \mathbb{V}(Y) \\
&= \left(\mathbb{E}[Y] - m\right)^2 + \mathbb{V}(Y).
\end{aligned}$$

This is the source of the so-called "bias-variance tradeoff" (more on this later).

## Optimizing...

We want to find the minimum value of $\text{MSE}(m)$ (denote the optimal prediction by $\mu$):

::: {.fragment .fade-in}
$$
\begin{aligned}
\frac{d\text{MSE}}{dm} &= -2(\mathbb{E}[Y] - m) + 0 \\
0 = \left.\frac{d\text{MSE}}{dm}\right|_{m = \mu} &= -2(\mathbb{E}[Y] - \mu) 
\end{aligned}$$

$$\Rightarrow \mu = \mathbb{E}[Y].$$
:::

## Expected Value As Best Prediction

In other words, **the best predicted value of a random variable is its expectation**.

But:

1. We usually don't have enough data to know what $\mathbb{E}[Y]$ is: need to make probabilistic assumptions;
2. In many applications, we don't just want a point estimate, we need some estimate of ranges of values **we might observe**.

## Uses of Models

What can we use a probability model for?

1. **Summaries** of data: store $y = f(\mathbf{x})$ instead of all data points $(y, x_1, \ldots, x_n)$
2. **Smooth** values by removing noise
3. **Predict** new data (interpolation/extrapolation): $\hat{f} = f(\hat{\mathbf{x}})$
4. **Infer** relationships between variables (interpret coefficients of $f$)

## My Philosophical Position

:::: {.columns}
::: {.column width=50%}
- Probability theory helps us deduce logical implications of theories **conditional on our assumptions**
- Cannot use an "objective" procedure to avoid **subjective responsibility**
:::

::: {.column width=50%}
::: {.center}
![Spiderman Meme](memes/peter_parker_method_assumptions.png){width=90%}
:::
:::
::::

# Linear Regression

## Linear Regression

The "simplest" model is **linear**:

:::: {.columns}
::: {.column width=50%}
$$\begin{aligned}
y_i &\sim N(\mu_i, \sigma^2) \\
\mu_i &= \sum_j \beta_j x^j_i
\end{aligned}
$$
:::
::: {.column width=50%}
$$\begin{aligned}
y_i &= \sum_j \beta_j x^j_i + \varepsilon_i \\
\varepsilon_i &\sim N(0, \sigma^2)
\end{aligned}
$$
:::
::::

# Linear Model Example

## How Does River Flow Affect TDS?

:::: {.columns}
::: {.column width=50%}

**Question**: Does river flow affect the concentration of total dissolved solids?

**Data**: Cuyahoga River (1969 -- 1973), from @Helsel2020-nq [Chapter 9].

:::
::: {.column width=50%}
```{julia}
tds = let
    fname = "data/tds/cuyaTDS.csv" # CHANGE THIS!
    tds = DataFrame(CSV.File(fname))
    tds[!, [:date, :discharge_cms, :tds_mgL]]
end
p = scatter(
    tds.discharge_cms,
    tds.tds_mgL,
    xlabel=L"Discharge (m$^3$/s)",
    ylabel="Total dissolved solids (mg/L)",
    markersize=5,
    label="Observations"
)
plot!(p, size=(600, 600))
```
:::
::::

## How Does River Flow Affect TDS?

:::: {.columns}

::: {.column width=50%}
**Question**: Does river flow affect the concentration of total dissolved solids?

**Model**: 
$$D \rightarrow S \ {\color{purple}\leftarrow U}$$
$$S = f(D, U)$$
:::
::: {.column width=50%}
```{julia}
p
```
:::
::::

## Transforming Data

:::: {.columns}
::: {.column width=50%}
Can address non-linear relationship in this case with a transformation.

Better in general to transform $S$: $\mathbb{E}[f(Y)] \neq f(\mathbb{E}[Y])$ (and these may be very different).

$$
\begin{align*}
S &= \beta_0 + \beta_1 \log(D) + U\\
U &\sim \mathcal{N}(0, \sigma^2)
\end{align*}
$$


:::
::: {.column width=50%}
```{julia}
p1 = scatter(
    log.(tds.discharge_cms),
    tds.tds_mgL,
    xlabel=L"Log-Discharge (log(m$^3$/s))",
    ylabel="Total dissolved solids (mg/L)",
    markersize=5,
    label="Observations"
)
plot!(p1, size=(600, 600))
```
:::
::::

## Ordinary Least Squares

**Idea** (from Gauss and others): $S &= \beta_0 + \beta_1 \log(D)$ is an overdetermined system of equations.

Since we can't solve, use minimization of sum of squared residuals as heuristic for a "good fit".

$$S(b) = \sum_{i=1}^n (Y_i - X_i \beta)^2$$

If the columns of $X$ are linearly independent, this has a unique solution: $$\hat{\beta} = \left(X^TX)^{-1}X^TY$$



## Variance of Errors

To get an estimate of the error variance $\hat{\sigma}^2$:

$$\hat{\sigma}^2 = \frac{(Y-X\hat{\beta})^T(Y-X\hat{\beta})}{n-p},$$

where $n$ is the number of data points and $p$ is the number of terms (here p=2).

## Linear Model Fitting: OLS

In this case:

```{julia}
X = [ones(nrow(tds)) log.(tds.discharge_cms)] # predictors
Y = tds.tds_mgL # predicands
@show β = inv(X' * X) * X' * Y; # OLS formula
ε = Y - X * β 
@show s² = (ε' * ε) / (nrow(tds) - 2);  # error estimate
```

## Properties of OLS Solution

The OLS solution is the **optimal linear predictor** given the least-squares criterion.

If you insist on a linear model, this is the best you can do.

This property does not require any assumptions on the distribution of $\varepsilon$.

## Linear Models Can Be Bad

:::: {.columns}
::: {.column width=50%}
While the linear predictor has no bias (averaged over all $X$), $\mathbb{E}[Y - X \hat{\beta}] = 0$...

The conditional expected error is usually non-zero: $\mathbb{E}[Y - X \hat{\beta} | X = x] = 0$

:::
::: {.column width=50%}
```{julia}
#| code-fold: true
#| echo: true
#| label: fig-lr-fit
#| fig-cap: Linear regression fit to the TDS data.

# get predictions
x_pred = log.(0.1:0.1:60)
y_pred = β[1] .+ β[2] * x_pred
plot(x_pred, y_pred, label="OLS Fit", color=:black, linewidth=3,
    xlabel=L"Log-Discharge (log(m$^3$/s))",
    ylabel="Total dissolved solids (mg/L)",
    size=(600, 600))
scatter!(
    log.(tds.discharge_cms),
    tds.tds_mgL,
    markersize=5,
    label="Observations"
)
```
:::
::::


# Linear Regression As Probability Model

## Probabilistic Assumptions for LR

$$
y &= \underbrace{\sum_j \beta_j x^j_i)}_{\mathbb{E}[y | x^j]} + \underbrace{\varepsilon_i}_{\text{noise}}, \quad \varepsilon_i \sim N(0, \sigma^2) 
$$

1. $\mathbb{E}[y | x^j]$: Best linear prediction of $y$ conditional on choice of predictors $x^j$.
2. The noise defines the probability distribution (here: Gaussian) of the observations: $$y \sim N(\sum_j \beta_j x^j_i), \sigma^2).$$


## Why Linear Models?

:::: {.columns}
::: {.column width=60%}
Two main reasons to use linear models/normal distributions:

1. **Inferential**: "Least informative" distribution assuming only finite mean/variance;
2. **Generative**: Central Limit Theorem (summed fluctuations are asymptotically normal)
3. **Approximation**: Taylor expand "true" nonlinear model: maybe the higher order terms are small?

:::
::: {.column width=40%}
![Weight stack Gaussian distribution](https://i.redd.it/zl5mo1n45wyb1.jpg)

::: {.caption}
Source: r/GymMemes
:::
:::
::::

::: {.notes}
One key thing: normal distributions are the "least informative" distribution given constraints on mean and variance. So all else being equal, this is a useful machine if all we're interested in are those two moments.
:::

## Central Limit Theorem 

If 

- $\mathbb{E}[X_i] = \mu$ 
- and $\text{Var}(X_i) = \sigma^2 < \infty$, 

$$\begin{aligned}
\lim_{n \to \infty} \sqrt{n}(\bar{X}_n - \mu ) &= \mathcal{N}(0, \sigma^2) \\
\Rightarrow & \bar{X}_n \overset{\text{approx}}{\sim} \mathcal{N}(\mu, \sigma^2/n)
\end{aligned}$$

## Central Limit Theorem (More Intuitive)

:::: {.columns}
::: {.column width=50%}
For **a large enough set of samples**: 

The sampling distribution of a sum or mean of random variables is approximately normal distribution, **even if the random variables themselves are not**.
:::
::: {.column width=50%}
![Small n Meme](memes/sampling_distribution_small_n.jpg)

::: {.caption}
Source: Unknown
:::
:::
::::


## Implications of Gaussian Noise

Without explicit modeling of sources of measurement uncertainty, in LR we can't separate a noisy system state (due to *e.g.* omitted variables) from an uncertain measurement error (more on this later).

$$
\begin{array}{l}
X \sim N(\mu_1, \sigma_1^2) \\
Y \sim N(\mu_2, \sigma_2^2) \\
Z = X + Y
\end{array}
\qquad \Rightarrow \qquad Z \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
$$


## Why Add Distributional Assumptions?

The distributional assumption of independent Gaussian residuals is **strong**.

One main benefit: we can obtain $\hat{\beta}$ and $\hat{\sigma}$ through **maximum likelihood** and can gener

# Key Points

## Probability Models

- Almost anything we want to use data for involves an assumption about the underlying probability model for the data.
- Our goal this semester is to develop a toolkit to specify, test, and use these probability models.

# Upcoming Schedule

## Next Classes

**Wednesday**: Fitting models with maximum likelihood

**Friday**: Maximum likelihood uncertainty

## Assessments

**Homework 1** Due Friday, 2/6.

# References

## References (Scroll for Full List)
