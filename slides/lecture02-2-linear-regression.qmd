---
title: "Probability Fundamentals"
subtitle: "Lecture 03"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "January 29, 2025"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        mermaid: 
            theme: dark
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Dates
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using GLM
using Optim
using LaTeXStrings
using Measures

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Review

## Null Hypothesis Significance Testing

- Binary "significant"/"non-significant" decision framework;
- Accept or reject null hypothesis $\mathcal{H}_0$ based on p-value of test statistic (relative to significance level $\alpha$);
- "Typical" statistical models for $\mathcal{H}_0$ often chosen out of computational convenience
- **There be dragons**: Multiple comparisons, lack of statistical power, reading rejection of $\mathcal{H}_0$ as acceptance of alternative $\mathcal{H}$

## What NHST Means (and Doesn't)

- **Rejection of $\mathcal{H}_0$ does not mean $\mathcal{H}$ is true**
- Not rejecting $\mathcal{H}_0$ does not mean $\mathcal{H}$ is false.
- Says nothing about $p(\mathcal{H}_0 | y)$
- "Simply" a measure of surprise at seeing data under null model.

# Probability "Review"

## What is Uncertainty?

::: {.fragment .fade-in}
::: {.quote}
> ...A  departure  from  the  (unachievable)  ideal  of  complete  determinism...

::: {.cite}
--- @Walker2003-zi
:::
:::
:::

## Types of Uncertainty

::: {.fragment .fade-in}
:::: {.columns}
::: {.column width=60%}

| Uncertainty Type | Source | Example(s) |
|:----------------:|:-------|:-----------|
| ***Aleatory uncertainty*** | Randomness | Dice rolls, Instrument imprecision |
| ***Epistemic uncertainty*** | Lack of knowledge | Climate sensitivity, Premier League champion|

:::
::: {.column width=40%}

![Which Uncertainty Type Meme](memes/uncertainty_types.png){width=75%}

:::
::::
:::

::: {.notes}
Note that the distinction between aleatory and epistemic uncertainty is somewhat arbitrary (aside from maybe some quantum effects). For example, we often think of coin tosses as aleatory, but if we had perfect information about the toss, we might be able to predict the outcome with less uncertainty. There's a famous paper by Persi Diaconis where he collaborated with engineers to build a device which could arbitrary bias a "fair" coin toss.

But in practice, this doesn't really matter: the key thing is whether for a given model we're treating the uncertainty as entirely random (e.g. white noise) versus being interested in the impacts of that uncertainty on the outcome of interest. And there's a representation theorem by the Bayesian actuary Bruno de Finetti which shows that, under a condition called *exchangeability*, we can think of any random sequence as arising from an independent and identically distributed process, so the practical difference can collapse further.
:::

## Probability

Probability is a language for expressing uncertainty.

The **axioms of probability** are straightforward:

1. $\mathcal{P}(E) \geq 0$;
2. $\mathcal{P}(\Omega) = 1$;
3. $\mathcal{P}(\cup_{i=1}^\infty E_i) = \sum_{i=1}^\infty \mathcal{P}(E_i)$ for disjoint $E_i$.

::: {.notes}
The third is a generalization of the definition of independent events to sets of outcomes.
:::

## Frequentist vs Bayesian Probability

:::: {.columns}
::: {.column width=50%}
**Frequentist**:

- Probability as frequency over repeated observations.
- Data are random, but parameters are not.
- How consistent are estimates for different data?
:::

::: {.column width=50%}
::: {.fragment .fade-in}
**Bayesian**:

- Probability as degree of belief/betting odds.
- Data and parameters are random variables;
- Emphasis on **conditional probability**.

:::
:::
::::


## But What, Like, **Is** Probability?

:::: {.columns}
::: {.column width=50%}
Frequentist vs. Bayesian: different interpretations with some different methods and formalisms.

We will freely borrow from each school depending on the purpose and goal of an analysis.
:::

::: {.column width=50%}
![Definitions of Probability Meme](memes/probability_definitions.png)

:::
::::

## Probability Distributions

Distributions are mathematical representations of probabilities over a range of possible outcomes.

$$x \to \mathbb{P}_{\color{green}\mathcal{D}}[x] = p_{\color{green}\mathcal{D}}\left(x | {\color{purple}\theta}\right)$$

- ${\color{green}\mathcal{D}}$: probability distribution (often implicit);
- ${\color{purple}\theta}$: distribution parameters

## Sampling Notation

To write $x$ is sampled from $\mathcal{D}(\theta) = p(x|\theta)$:
$$x \sim \mathcal{D}(\theta)$$

For example, for a normal distribution:
$$x \overset{\text{i.i.d.}}{\sim} \mathcal{N}(\mu, \sigma)$$

::: {.notes}
"i.i.d." means "identically and independently distributed.""
:::

## Probability Density Function

A continuous distribution $\mathcal{D}$ has a probability density function (PDF) $f_\mathcal{D}(x) = p(x | \theta)$.

The probability of $x$ occurring in an interval $(a, b)$ is
$$\mathbb{P}[a \leq x \leq b] = \int_a^b f_\mathcal{D}(x)dx.$$

**Important**: $\mathbb{P}(x = x^*)$ is zero!

## Probability Mass Functions

Discrete distributions have *probability mass functions* (PMFs) which are defined at point values, e.g. $p(x = x^*) \neq 0$.

::: {.notes}
Unlike continuous distributions, we can talk about the probability of individual values for discrete distributions, which a PMF provides versus a PDF. But in general these are the same things.
:::


## Cumulative Density Functions

:::: {.columns}
::: {.column width=50%}
If $\mathcal{D}$ is a distribution with PDF $f_\mathcal{D}(x)$, the **cumulative density function** (CDF) of $\mathcal{D}$ is $F_\mathcal{D}(x)$:

$$F_\mathcal{D}(x) = \int_{-\infty}^x f_\mathcal{D}(u)du.$$

:::
::: {.column width=50%}
```{julia}
#| label: fig-cdf-pdf
#| fig-cap: Relationship of CDF and PDF
#| layout-nrow: 2
dist = TDist(4)
x = -5:0.01:5
q = 0.25
p1 = plot(x, x -> cdf(dist, x), ylabel="Cumulative Density", xlabel=L"$x$", linewidth=3, label=false)
plot!([-5, quantile(dist, q)], [q, q], color=:gray, linestyle=:dash, linewidth=2, label=false)
plot!([quantile(dist, q), quantile(dist, q)], [0, q], color=:gray, linestyle=:dash, linewidth=2, label=false, size=(500, 300))

p2 = plot(x, x -> pdf(dist, x), linewidth=3, ylabel="Density", xlabel=L"$x$", label=false)
xpdf = -5:0.01:quantile(dist, q)
plot!(xpdf, zeros(length(xpdf)), fillrange=pdf.(dist, xpdf), fillalpha=0.5, color=:gray, label=false, size=(500, 300))

display(p1)
display(p2)
```
:::
::::

## Relationship Between PDFs and CDFs

Since $$F_\mathcal{D}(x) = \int_{-\infty}^x f_\mathcal{D}(u)du,$$

if $f_\mathcal{D}$ is continuous at $x$, the Fundamental Theorem of Calculus gives:
$$f_\mathcal{D}(x) = \frac{d}{dx}F_\mathcal{D}(x).$$

::: {.notes}
The value of the CDF is the amount of probability "below" the value. So e.g. for a one-sided statistical test, the p-value is the complement of the CDF at the value of the test statistic.
:::

## Quantiles

:::: {.columns}
::: {.column width=50%}
The quantile function is the **inverse of the CDF**:

$$q(\alpha) = F^{-1}_\mathcal{D}(\alpha)$$

So $$x_0 = q(\alpha) \iff \mathbb{P}_\mathcal{D}(X < x_0) = \alpha.$$

:::
::: {.column width=50%}
```{julia}
#| label: fig-cdf-pdf-2
#| fig-cap: Relationship of CDF and PDF
#| layout-nrow: 2
dist = TDist(4)
x = -5:0.01:5
q = 0.25
p1 = plot(x, x -> cdf(dist, x), ylabel="Cumulative Density", xlabel=L"$x$", linewidth=3, label=false)
plot!([-5, quantile(dist, q)], [q, q], color=:gray, linestyle=:dash, linewidth=2, label=false)
plot!([quantile(dist, q), quantile(dist, q)], [0, q], color=:gray, linestyle=:dash, linewidth=2, label=false, size=(500, 300))

p2 = plot(x, x -> pdf(dist, x), linewidth=3, ylabel="Density", xlabel=L"$x$", label=false)
xpdf = -5:0.01:quantile(dist, q)
plot!(xpdf, zeros(length(xpdf)), fillrange=pdf.(dist, xpdf), fillalpha=0.5, color=:gray, label=false, size=(500, 300))

display(p1)
display(p2)
```
:::
::::

# Selecting a Distribution

## Distributions Are Assumptions

**Specifying a distribution is making an assumption about observations and any applicable constraints.**

Examples: If your observations are...

- Continuous and fat-tailed? **Cauchy distribution**
- Continuous and bounded? **Beta distribution**
- Sums of positive random variables? **Gamma or Normal distribution**.

## Why Use a Normal Distribution?

:::: {.columns}
::: {.column width=60%}
Two main reasons to use linear models/normal distributions:

1. **Inferential**: "Least informative" distribution assuming knowledge of just mean and variance;
2. **Generative**: Central Limit Theorem (summed fluctuations are asymptotically normal)

:::
::: {.column width=40%}
![Weight stack Gaussian distribution](https://i.redd.it/zl5mo1n45wyb1.jpg)

::: {.caption}
Source: r/GymMemes
:::
:::
::::

::: {.notes}
One key thing: normal distributions are the "least informative" distribution given constraints on mean and variance. So all else being equal, this is a useful machine if all we're interested in are those two moments.
:::

## Statistics of Random Variables are Random Variables

**The sum or mean of a random sample is itself a random variable**:

$$\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i \sim \mathcal{D}_n$$

::: {.fragment .fade-in}
$\mathcal{D}_n$: The ***sampling distribution*** of the mean (or sum, or other estimate of interest).
:::

## Sampling Distributions

![Illustration of the Sampling Distribution](figures/true-sampling.png)

## Central Limit Theorem 

If 

- $\mathbb{E}[X_i] = \mu$ 
- and $\text{Var}(X_i) = \sigma^2 < \infty$, 

$$\begin{align*}
&\bbox[yellow, 10px, border:5px solid red]
{\lim_{n \to \infty} \sqrt{n}(\bar{X}_n - \mu ) = \mathcal{N}(0, \sigma^2)} \\
\Rightarrow &\bbox[yellow, 10px, border:5px solid red] {\bar{X}_n \overset{\text{approx}}{\sim} \mathcal{N}(\mu, \sigma^2/n)}
\end{align*}$$

## Central Limit Theorem (More Intuitive)

:::: {.columns}
::: {.column width=50%}
For **a large enough set of samples**, the sampling distribution of a sum or mean of random variables is approximately a normal distribution, even if the random variables themselves are not.
:::
::: {.column width=50%}
![Small n Meme](memes/sampling_distribution_small_n.jpg)

::: {.caption}
Source: Unknown
:::
:::
::::

## "What Distribution Should I Use?"

**There is no right answer to this, no matter what a statistical test tells you.**

- What assumptions are justifiable from theory?
- What information do you have? 

## "What Distribution Should I Use?"

For example, suppose our data are counts of events:

- If you know something about **rates**, you can use a Poisson distribution
- If you know something about **probabilities**, you can use a Binomial distribution. 

## Q-Q Plots

::: {.columns}
::: {.column width=50%}
One exploratory method to see if your data is reasonably described by a theoretical distribution is a **Q-Q plot**.
:::
::: {.column width=50%}
```{julia}
#| label: fig-norm-qq
#| code-fold: true
#| code-overflow: wrap
#| echo: true

samps = rand(Normal(0, 3), 20)
qqplot(Normal, samps, tickfontsize=16, guidefontsize=18, linewidth=3, markersize=6)
xlabel!("Theoretical Quantiles")
ylabel!("Empirical Quantiles")
plot!(size=(500, 450))
```
:::
::::

## Fat-Tailed Data and Q-Q Plots

```{julia}
#| label: fig-cauchy-qq
#| echo: false
#| layout-nrow: 2
#| fig-cap: "Q-Q Plot for Cauchy Data and Normal Distribution"
#| fig-subcap: 
#|  - "Normal vs Cauchy Distribution"
#|  - "Q-Q Plot"

## generate fat-tailed residuals
cauchy_samps = rand(Cauchy(0, 0.05), 50)

# make plots
# scatterplot of observations
p1 = plot(fit(Normal, cauchy_samps), linewidth=3, color=:green, label="Normal Distribution", yaxis=false, legend=:outerright)
plot!(p1, fit(Cauchy, cauchy_samps), linewidth=3, color=:orange, linestyle=:dash, label="Cauchy Distribution")
scatter!(p1, cauchy_samps, zeros(length(cauchy_samps)), markersize=3, color=:black, label="Data")
xlims!(p1, (-2, 2))
xlabel!("Value")
plot!(p1, size=(1000, 250))

# densities of residual distributions
p2 = qqplot(Normal, cauchy_samps, tickfontsize=16, guidefontsize=18, linewidth=3, markersize=6, title="Normal Q-Q Plot")
xlabel!(p2, "Theoretical Values")
ylabel!(p2, "Empirical Values")
p3 = qqplot(Cauchy, cauchy_samps, tickfontsize=16, guidefontsize=18, linewidth=3, markersize=6, title="Cauchy Q-Q Plot")
xlabel!(p3, "Theoretical Values")
ylabel!(p3, "Empirical Values")
p = plot(p2, p3, layout=(1, 2), size=(900, 350))

display(p1)
display(p)
```

# Maximum Likelihood

## Likelihood

How do we "fit" distributions to a dataset?

**Likelihood** of data to have come from distribution $f(\mathbf{x} | \theta)$:

$$\mathcal{L}(\theta | \mathbf{x}) = \underbrace{f(\mathbf{x} | \theta)}_{\text{PDF}}$$

::: {.notes}
The likelihood gives us a measure of how probable a dataset is from a given distribution. It's the PDF of the distribution at the data.

But the perspective is flipped: instead of fixing a distribution and calculating the probability of some data, we fix the data and look at how the probability of observing that data changes as the distribution changes. 
:::


## Normal Distribution PDF

$$f_\mathcal{D}(x) = p(x | \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{1}{2}\left(\frac{x - \mu}{\sigma}^2\right)\right)$$

::: {.center}
```{julia}
#| label: fig-normal
#| fig-align: center

plot(Normal(0, sqrt(3)), linewidth=3, color=:blue, label=L"$\mu=0$, $\sigma=\sqrt{3}$", guidefontsize=20, legendfontsize=20, tickfontsize=14)
plot!(Normal(2, 1), linewidth=3, color=:orange, label=L"$\mu=2$, $\sigma=1$")
plot!(Normal(0, 1), linewidth=3, color=:red, label=L"$\mu=0$, $\sigma=1$")
plot!(size=(1200, 400), left_margin=10mm, bottom_margin=10mm)
vline!([0.5], color=:black, linestyle=:dash)
xlabel!(L"$x$")
ylabel!("Likelihood")
xlims!((-5, 5))
```
:::

## Likelihood of Multiple Samples

For multiple (independent) samples $\mathbf{x} = \{x_1, \ldots, x_n\}$:

$$\mathcal{L}(\theta | \mathbf{x}) = \prod_{i=1}^n \mathcal{L}(\theta | x_i).$$

## Likelihood Example

:::: {.columns}
::: {.column width=50%}
```{julia}

dist = Normal(-0.5, 2)
x = rand(dist, 10)
plot(Normal(0, 1), linewidth=3, ylabel="Density", xlabel=L"$x$", legend=false, color=:blue, size=(600, 400))
vline!(x, color=:red)
xlims!((-9, 6))
```
:::
::: {.column width=50%}
| Distribution | Likelihood |
|:------------:|:-----------|
| $N(0, 1)$ | `{julia} round(prod(pdf.(Normal(0, 1), x)); sigdigits=2)` |
:::
::::

## Likelihood Example

:::: {.columns}
::: {.column width=50%}
```{julia}

plot(Normal(0, 1), linewidth=3, ylabel="Density", xlabel=L"$x$", alpha=0.2, legend=false, color=:blue, size=(600, 400))
plot!(Normal(-1, 2), linewidth=3, color=:blue)
vline!(x, color=:red)
xlims!((-9, 6))
```
:::
::: {.column width=50%}
| Distribution | Likelihood |
|:------------:|:-----------|
| $N(0, 1)$ | `{julia} round(prod(pdf.(Normal(0, 1), x)); sigdigits=2)` |
| $N(-1, 2)$ | `{julia} round(prod(pdf.(Normal(-1, 2), x)); sigdigits=2)` |
:::
::::

## Likelihood Example

:::: {.columns}
::: {.column width=50%}
```{julia}

plot(Normal(0, 1), linewidth=3, ylabel="Density", xlabel=L"$x$", alpha=0.2, legend=false, color=:blue, size=(600, 400))
plot!(Normal(-1, 2), linewidth=3, color=:blue, alpha=0.2)
plot!(Normal(-1, 1), linewidth=3, color=:blue)
vline!(x, color=:red)
xlims!((-9, 6))
```
:::
::: {.column width=50%}
| Distribution | Likelihood |
|:------------:|:-----------|
| $N(0, 1)$ | `{julia} round(prod(pdf.(Normal(0, 1), x)); sigdigits=2)` |
| $N(-1, 2)$ | `{julia} round(prod(pdf.(Normal(-1, 2), x)); sigdigits=2)` |
| $N(-1, 1)$ | `{julia} round(prod(pdf.(Normal(-1, 1), x)); sigdigits=2)` |
:::
::::

## Maximizing Likelihood

To find the parameters $\hat{\theta}$ which best fit the data:

$\hat{\theta} = \max_\theta \mathcal{L}(\theta | \mathbf{x})$

## Generally Maximizing Likelihood

:::: {.columns}
::: {.column width=50%}
Can use optimization algorithms to maximize $\theta \to \mathcal{L}(\theta | x).$

**Dragons**: Probability calculations tend to under- and overflow due to floating point precision.
:::
::: {.column width=50%}
![Floating Point Logarithms meme](memes/floating_point_logs.png){width=85%}
:::
::::

# Describing Uncertainty


## Confidence Intervals

:::: {.columns}
::: {.column width=50%}
Frequentist estimates have **confidence intervals**, which will contain the "true" parameter value for $\alpha$% of data samples.

No guarantee that an individual CI contains the true value (with any "probability")!
:::

::: {.column width=50%}

![Horseshoe Illustration](https://www.wikihow.com/images/thumb/2/20/Throw-a-Horseshoe-Step-4-Version-4.jpg/aid448076-v4-728px-Throw-a-Horseshoe-Step-4-Version-4.jpg){width=90%}

::: {.caption}
Source: <https://www.wikihow.com/Throw-a-Horseshoe>
:::

:::
::::

::: {.notes}
Confidence intervals only capture uncertainty in **parameter inferences** due to data uncertainty, though this language sometimes gets misused to also refer to data/estimand uncertainty. 

:::


## Example: 95% CIs for N(0.4, 2)

```{julia}
#| label: fig-cis
#| code-fold: true
#| code-overflow: wrap
#| echo: true
#| layout-ncol: 2
#| fig-cap: "Display of 95% confidence intervals"
#| fig-subcap: 
#|  - "Sample Size 100"
#|  - "Sample Size 1,000"

# set up distribution
mean_true = 0.4
n_cis = 100 # number of CIs to compute
dist = Normal(mean_true, 2)

# use sample size of 100
samples = rand(dist, (100, n_cis))
# mapslices broadcasts over a matrix dimension, could also use a loop
sample_means = mapslices(mean, samples; dims=1)
sample_sd = mapslices(std, samples; dims=1) 
mc_sd = 1.96 * sample_sd / sqrt(100)
mc_ci = zeros(n_cis, 2) # preallocate
for i = 1:n_cis
    mc_ci[i, 1] = sample_means[i] - mc_sd[i]
    mc_ci[i, 2] = sample_means[i] + mc_sd[i]
end
# find which CIs contain the true value
ci_true = (mc_ci[:, 1] .< mean_true) .&& (mc_ci[:, 2] .> mean_true)
# compute percentage of CIs which contain the true value
ci_frac1 = 100 * sum(ci_true) ./ n_cis

# plot CIs
p1 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:deepskyblue, label="95% Confidence Interval", title="Sample Size 100", yticks=:false, legend=:false)
for i = 2:n_cis
    if ci_true[i]
        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:deepskyblue, label=:false)
    else
        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)
    end
end
vline!(p1, [mean_true], color=:black, linewidth=2, linestyle=:dash, label="True Value") # plot true value as a vertical line
xaxis!(p1, "Estimate")
plot!(p1, size=(500, 350)) # resize to fit slide

# use sample size of 1000
samples = rand(dist, (1000, n_cis))
# mapslices broadcasts over a matrix dimension, could also use a loop
sample_means = mapslices(mean, samples; dims=1)
sample_sd = mapslices(std, samples; dims=1) 
mc_sd = 1.96 * sample_sd / sqrt(1000)
mc_ci = zeros(n_cis, 2) # preallocate
for i = 1:n_cis
    mc_ci[i, 1] = sample_means[i] - mc_sd[i]
    mc_ci[i, 2] = sample_means[i] + mc_sd[i]
end
# find which CIs contain the true value
ci_true = (mc_ci[:, 1] .< mean_true) .&& (mc_ci[:, 2] .> mean_true)
# compute percentage of CIs which contain the true value
ci_frac2 = 100 * sum(ci_true) ./ n_cis

# plot CIs
p2 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:deepskyblue, label="95% Confidence Interval", title="Sample Size 1,000", yticks=:false, legend=:false)
for i = 2:n_cis
    if ci_true[i]
        plot!(p2, [mc_ci[i, :]], [i, i], linewidth=2, color=:deepskyblue, label=:false)
    else
        plot!(p2, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)
    end
end
vline!(p2, [mean_true], color=:black, linewidth=2, linestyle=:dash, label="True Value") # plot true value as a vertical line
xaxis!(p2, "Estimate")
plot!(p2, size=(500, 350)) # resize to fit slide

display(p1)
display(p2)
```

`{julia} Int64(round(ci_frac1))`% of the CIs contain the true value (left) vs. `{julia} Int64(round(ci_frac2))`% (right)

## Predictive Intervals

:::: {.columns}
::: {.column width=50%}
**Predictive intervals** capture uncertainty in an estimand.

**With what probability would I see a particular outcome in the future?**

Often need to construct these using **simulation**.
:::
::: {.column width=50%}
```{julia}
#| label: fig-credible-interval
#| fig-cap: Two different 95% credible intervals.

plot(Gamma(7.5), linewidth=3, xlabel="Data/Parameter", label=:false, legend=:outerbottom)
q1 = quantile(Gamma(7.5), [0.05, 0.95])
q2 = quantile(Gamma(7.5), [0.01, 0.91])
q3 = quantile(Gamma(7.5), [0.09, 0.99])
gamma_pdf(x) = pdf(Gamma(7.5), x)
plot!(q1[1]:0.01:q1[2], gamma_pdf(q1[1]:0.01:q1[2]), fillrange=zero(q1[1]:0.01:q1[2]), alpha=0.2, label="90% Interval 1")
plot!(q2[1]:0.01:q2[2], gamma_pdf(q2[1]:0.01:q2[2]), fillrange=zero(q2[1]:0.01:q2[2]), alpha=0.2, label="90% Interval 2")
plot!(q3[1]:0.01:q3[2], gamma_pdf(q3[1]:0.01:q3[2]), fillrange=zero(q3[1]:0.01:q3[2]), alpha=0.2, label="90% Interval 3")
plot!(size=(600, 650))
```
:::
::::

::: {.notes}
Due to this non-uniqueness, the typical convention is to use the "equal tailed" interval based on quantiles.
:::

## OLS Estimate of Regression Coefficients

Write regression as $$\mathbf{y} = \mathbf{X} \beta.$$

Goal is to minimize the squared error:

$$\hat{\mathbf{\beta}} = \underset{\beta}{\operatorname{argmin}} S(\beta),$$

where $$S(\beta) = \sum_{i=1}^n \left|y_i - \sum_{j=1}^p X_{ij} \beta_j\right|^2 = \|\mathbf{y} - \mathbf{X}\mathbf{\beta}\|^2.$$

## OLS Solution

If the columns of $\mathbf{X}$ are linearly independent, the solution is:

$$\hat{\beta} = \left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T \mathbf{y}.$$

Estimate of the variance:

$$\sigma^2_\text{OLS} = \frac{S(\hat{\mathbf{\beta}})}{n-p}.$$

## Linear Regression Likelihood

How do we find $\beta_i$ and $\sigma$?

**Likelihood** of data to have come from distribution $f(\mathbf{x} | \theta)$:
$$\mathcal{L}(\theta | \mathbf{x}) = \underbrace{f(\mathbf{x} | \theta)}_{\text{PDF}}$$

Here the randomness comes from $U$:
$$S \sim \mathcal{N}(\beta_0 + \beta_1 \log(D), \sigma^2)$$


# OLS vs. Maximum Likelihood

## Maximizing Gaussian Likelihood &hArr; Least Squares

$$y_i \sim \mathcal{N}(F(x_i), \sigma^2)$$

::: {.fragment .fade-in}
$$\mathcal{L}(\theta | \mathbf{y}; F) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} \exp(-\frac{y_i - F(x_i)^2}{2\sigma^2})$$
:::

::: {.fragment .fade-in}

$$\log \mathcal{L}(\theta | \mathbf{y}; F) = \sum_{i=1}^n \left[\log \frac{1}{\sqrt{2\pi}} - \frac{1}{2\sigma^2}(y_i - F(x_i))^2 \right]$$

:::

## {#simplifying-log-likelihood data-menu-title="Simplifying the Log Likelihood"}

$$
\begin{aligned}
\log \mathcal{L}(\theta | \mathbf{y}, F) &= \sum_{i=1}^n \left[\log \frac{1}{\sqrt{2\pi}} - \frac{1}{2\sigma^2}(y_i - F(x_i))  ^2 \right] \\
&= n \log \frac{1}{\sqrt{2\pi}} - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - F(x_i))^2
\end{aligned}
$$

## {#simplifying-constants-ignore data-menu-title="Simplifying by Ignoring Constants"}

Ignoring constants (including $\sigma$):

$$\log \mathcal{L}(\theta | \mathbf{y}, F) \propto -\sum_{i=1}^n (y_i - F(x_i))^2.$$

::: {.fragment .fade-in}
Maximizing $f(x)$ is equivalent to minimizing $-f(x)$:

$$
-\log \mathcal{L}(\theta | \mathbf{y}, F) \propto \sum_{i=1}^n (y_i - F(x_i))^2 = \text{MSE}
$$

**Note**: OLS vs. MLE estimates of $\sigma^2$ can differ by a factor of $\frac{n-p}{n}$.

:::

## Implication of MLE/OLS Equivalence

- Can rely on numerical optimization techniques of the likelihood instead of solving OLS equations.
- Maximizing likelihood generalizes to a broader category of models with more flexible forms (as we will see over the semester).
- Downside: numerical optimization can be finicky! May break down when the likelihood surface is "flat" (next week),


# Example

## How Does River Flow Affect TDS?

:::: {.columns}
::: {.column width=50%}

**Question**: How does river flow affect the concentration of total dissolved solids?

**Data**: Cuyahoga River (1969 -- 1973), from @Helsel2020-nq [Chapter 9].

:::
::: {.column width=50%}
```{julia}
#| label: fig-discharge-data
#| fig-cap: Data

tds = let
    fname = "data/tds/cuyaTDS.csv" 
    tds = DataFrame(CSV.File(fname))
    tds[!, [:date, :discharge_cms, :tds_mgL]]
end
p = scatter(
    tds.discharge_cms,
    tds.tds_mgL,
    xlabel=L"Discharge (m$^3$/s)",
    ylabel="Total dissolved solids (mg/L)",
    markersize=5,
    label="Observations"
)
plot!(p, size=(600, 600))
```
:::
::::

## How Does River Flow Affect TDS?

:::: {.columns}

::: {.column width=50%}
**Question**: Does river flow affect the concentration of total dissolved solids?

**Model**: 
$$D \rightarrow S \ {\color{purple}\leftarrow U}$$
$$S = f(D, U)$$
:::
::: {.column width=50%}
```{julia}
p
```
:::
::::


## Log-Linear Relationships

:::: {.columns}
::: {.column width=50%}
$$
\begin{aligned}
S &= \beta_0 + \beta_1 \log(D) + U\\
U &\sim \mathcal{N}(0, \sigma^2)
\end{aligned}
$$

:::
::: {.column width=50%}
```{julia}
xx = [2, 5, 10, 20, 50, 100]
p1 = plot(p, xaxis=:log, xticks=(xx, string.(xx)))
```
:::
::::

## Optimizing Log-Likelihood

```{julia}
#| echo: true
#| output-location: fragment
#| code-line-numbers: "|4-9|11-14"

# tds_riverflow_loglik: function to compute the log-likelihood for the tds model
# θ: vector of model parameters (coefficients β₀ and β₁ and stdev σ)
# tds, flow: vectors of data
function tds_riverflow_loglik(θ, tds, flow)
    β₀, β₁, σ = θ # unpack parameter vector
    μ = β₀ .+ β₁ * log.(flow) # find mean
    ll = sum(logpdf.(Normal.(μ, σ), tds)) # compute log-likelihood
    return ll
end

# set bounds (lb, ub) for the parameters and an initial value (θ₀) for the optimization
lb = [0.0, -1000.0, 1.0]
ub = [1000.0, 1000.0, 100.0]
θ₀ = [500.0, 0.0, 50.0]
# this minimizes the negative log-likelihood
optim_out = Optim.optimize(θ -> -tds_riverflow_loglik(θ, tds.tds_mgL, tds.discharge_cms), lb, ub, θ₀)
θ_mle = round.(optim_out.minimizer; digits=0)
@show θ_mle;
```
So our model is 

## Maximum Likelihood Results



```{julia}
#| echo: true
#| output-location: column
#| code-line-numbers: "|2-8|10"

# simulate 10,000 predictions
x = 1:0.1:60
μ = θ_mle[1] .+ θ_mle[2] * log.(x)

y_pred = zeros(length(x), 10_000)
for i = 1:length(x)
    y_pred[i, :] = rand(Normal(μ[i], θ_mle[3]), 10_000)
end
# take quantiles to find prediction intervals
y_q = mapslices(v -> quantile(v, [0.05, 0.5, 0.95]), y_pred; dims=2)

plot(p1, 
    x,
    y_q[:, 2],
    ribbon = (y_q[:, 2] - y_q[:, 1], y_q[:, 3] - y_q[:, 2]),
    linewidth=3, 
    fillalpha=0.2, 
    label="Best Fit",
    size=(600, 550))
```



# More General Models

## Example: Modeling Counts



```{julia}
#| echo: true
#| code-fold: true
#| output: true

fish = CSV.File("data/ecology/Fish.csv") |> DataFrame
fish[1:3, :]
```

## What Distribution?

::: {.fragment .fade-in}
Count data can be modeled using a Poisson or negative binomial distribution.

```{julia}
#| label: fig-count-dists
#| fig-cap: 
#|  - "Poisson"
#|  - "Negative Binomial"
#| layout-ncol: 2

p1 = plot(Poisson(2), size=(500, 450), title="Poisson(2)", xlabel="Value")
p2 = plot(NegativeBinomial(2, 0.5), size=(500, 450), title="Negative Binomial(2, 0.5)", xlabel="Value")
display(p1)
display(p2)
```
:::

## Exploring the Data

```{julia}
#| label: fig-fishing-data
#| fig-cap: Fishing Data

histogram(fish.fish_caught, xlabel="Caught Fish", ylabel="Count", size=(1000, 400))
```

- Mean: `{julia} round(mean(fish.fish_caught); digits=1)`
- Variance: `{julia} round(var(fish.fish_caught); digits=1)`

## Model Specification

We might hypothesize that more people fishing for more hours results in a greater chance of catching fish.

$$\begin{align*}
y_i &\sim Poisson(\lambda_i) \\
f(\lambda_i) &= \beta_0 + \beta_1 P_i + \beta_2 H_i
\end{align*}
$$

::: {.fragment .fade-in}
$\lambda_i$: positive "rate"

$f(\cdot)$: maps positive reals (rate scale) to all reals (linear model scale)
:::

## Link Functions

:::: {.columns}
::: {.column width=50%}
$\color{brown}f$ is the **link function**.

$\color{royalblue}f^{-1}$ is the **inverse link**.
:::
::: {.column width=50%}
$$\begin{align*}
y_i &\sim Poisson(\lambda_i) \\
{\color{brown}f}(\lambda_i) &= \beta_0 + \beta_1 P_i + \beta_2 H_i
\end{align*}
$$

$$
\lambda_i = {\color{royalblue}f^{-1}}\left(\beta_0 + \beta_1 P_i + \beta_2 H_i\right)
$$
:::
::::

## Choosing a Link Function

Link functions are typically linked to distributions.

- Poisson models usually use the **log link** (positives &rarr; reals).
- Binomial/Bernoulli models use the **logit** link ($[0, 1]$ &rarr; reals) $$\text{logit}(p) = \log\left(\frac{p}{1-p}\right)$$

## Fitting the Model

```{julia}
#| echo: true
#| code-line-numbers: "|1-5|7-11"

function fish_model(params, persons, hours, fish_caught)
    β₀, β₁, β₂ = params
    λ = exp.(β₀ .+ β₁ * persons + β₂ * hours)
    loglik = sum(logpdf.(Poisson.(λ), fish_caught))
end

lb = [-100.0, -100.0, -100.0]
ub = [100.0, 100.0, 100.0]
init = [0.0, 0.0, 0.0]

optim_out = optimize(θ -> -fish_model(θ, fish.persons, fish.hours, fish.fish_caught), lb, ub, init)
θ_mle = optim_out.minimizer
@show round.(θ_mle; digits=1);
```

## Evaluating Model Fit

```{julia}
#| label: fig-fish-skill
#| fig-cap: Predictive distribution for fitted fish model.
#| output-location: column
#| code-line-numbers: "|1-3|5-8"

P = 1:4 # number of persons
# simulate samples for each P
λ = exp.(θ_mle[1] .+ θ_mle[2] * P)
# draw 10,000 samples for each
fish_sim = zeros(10_000, length(P)) # initialize matrix to store simulations
for i = 1:length(P)
    fish_sim[:, i] = rand(Poisson(λ[i]), 10_000)
end

# plot predictive interval against return periods
boxplot(fish_sim, fillalpha=0.3, size=(900, 600), label=:none, color=:blue, ylabel="Fish Caught", xlabel="Persons")
scatter!(fish.persons .+ 0.5, fish.fish_caught, label="Observations", color=:black)
```

## What Was The Problem?

- Model neglected other plausible contributors, *e.g.* live bait.
- Data is **over-dispersed**: higher variance than mean.
- Might be better described by a zero-inflated Poisson model: 
  - Visitors who spent little time are likely to catch no fish.
  - Visitors who spent more time are likely to catch positive fish.



# Upcoming Schedule

## Next Classes

**Next Week**: Probability Models for Data

## Assessments

**Homework 1** due next Friday (2/7).

**Quiz**: Due before next class.

**Reading**: Annotate/submit writing before next class, will reserve time for discussion.

# References

## References (Scroll for Full List)
