---
title: "Statistics as Decision-Making"
subtitle: "Lecture 03"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "January 26, 2026"
format:
    revealjs: default
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Dates
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using GLM
using LaTeXStrings
using Measures

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Review

## Probability Fundamentals

- Bayesian vs. Frequentist Interpretations
- Distributions reflect assumptions on probability of data.
- PDFs, CDFs, quantiles
- Likelihood
- Communicating uncertainty: confidence vs. predictive intervals.

# Statistics and Decision-Making

## Science as Decision-Making Under Uncertainty

:::: {.columns}
::: {.column width=60%}
Goal is to draw insights:

- About causes and effects;
- About interventions.

:::

::: {.column width=40%}
![XKCD 2440](https://imgs.xkcd.com/comics/epistemic_uncertainty_2x.png){width=90%}

::: {.caption}
Source: [XKCD 2440](https://xkcd.com/2440)
:::
:::
::::


## Data Generation Approximates Reality

:::: {.columns}
::: {.column width=33%}
![Estimand Estimator Cake](memes/estimand_cake.png){width=100%}
:::
::: {.column width=33%}
::: {.fragment .fade-in}
![Estimand Estimator Cake](memes/estimator_cake.png){width=100%}
:::
:::
::: {.column width=33%}
::: {.fragment .fade-in}
![Estimate Cake](memes/estimate_cake.png){width=100%}
:::
:::
::::

::: {.caption}
Source: Richard McElreath
:::

::: {.notes}
Goal is to start with some "true" process, then apply a procedure (experimental/observational + statistical) and recover what is hopefully a good estimate.

But lots can go wrong in this process!
:::

  
## Questions We Might Like To Answer

::: {.incremental}
- Are high water levels influenced by environmental change?
- Does some environmental condition have an effect on water quality/etc?
- Does a drug or treatment have some effect?
:::

## Onus probandi incumbit ei qui dicit, non ei qui negat

:::: {.columns width=50%}
::: {.column width=50%}
**Core assumption**: Burden of proof is on someone claiming an effect (or a similar hypothesis).

:::

::: {.column width=50%}
![Null Hypothesis Meme](memes/skinner_null_hypothesis.png){width=50%}
:::
::::

::: {.notes}
The title of this slide is a reference to the burden of proof is on the person who affirms, not one who denies. Can think of this as similar to Ockham's razor or someone mantras: we want to propose hypotheses about scientific phenomena and then see if the evidence supports it. 
:::


## Null Hypothesis Significance Testing

:::: {.columns}
::: {.column width=60%}
- Check if the data is consistent with a "null" model;
- If the data is unlikely from the null model (to some level of **significance**), this is evidence for the alternative.
- If the data is consistent with the null, there is no need for an alternative hypothesis.
:::

::: {.column width=40%}
![Alternative Hypothesis Meme](memes/mordor_alternative_hypothesis.png)
:::
::::

::: {.notes}
For scientific hypotheses, this has been encoded in the NHST paradigm:

- Think of a "null" hypothesis and look for evidence that it is reasonably consistent with the data.
- If the data can be explained by the null, then we have no clear evidence for the alternative hypothesis.
- If the data is highly unlikely under the null hypothesis, then that gives us reason to reject the null and favor the alternative.

:::

## From Null Hypothesis to Null Model

::: {.quote}
> ...**the null hypothesis must be exact**, that is free of vagueness and ambiguity, **because it must supply the basis of the 'problem of distribution,'** of which the test of significance is the solution.

::: {.cite} 
--- R. A. Fisher, *The Design of Experiments*, 1935.
:::
:::

::: {.notes}
The trick is to go from a null scientific hypothesis to a null statistical model, hence Fisher's comment about the need for a null hypothesis to be "exact".
:::

## Example: High Water Nonstationarity

```{julia}
#| output: true
#| echo: true
#| code-fold: true
#| code-overflow: wrap
#| fig-align: center
#| label: fig-surge-data
#| fig-cap: Annual maxima surge data from the San Francisco, CA tide gauge.

# load SF tide gauge data
# read in data and get annual maxima
function load_data(fname)
    date_format = DateFormat("yyyy-mm-dd HH:MM:SS")
    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data
    df = @chain fname begin
        CSV.read(DataFrame; header=false)
        rename("Column1" => "year", "Column2" => "month", "Column3" => "day", "Column4" => "hour", "Column5" => "gauge")
        # need to reformat the decimal date in the data file
        @transform :datetime = DateTime.(:year, :month, :day, :hour)
        # replace -99999 with missing
        @transform :gauge = ifelse.(abs.(:gauge) .>= 9999, missing, :gauge)
        select(:datetime, :gauge)
    end
    return df
end

dat = load_data("data/surge/h551.csv")

# detrend the data to remove the effects of sea-level rise and seasonal dynamics
ma_length = 366
ma_offset = Int(floor(ma_length/2))
moving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]
dat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))

# group data by year and compute the annual maxima
dat_ma = dropmissing(dat_ma) # drop missing data
dat_annmax = combine(dat_ma -> dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime => x->year.(x)), :datetime_function))
delete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet
rename!(dat_annmax, :datetime_function => :Year)
select!(dat_annmax, [:Year, :residual])
dat_annmax.residual = dat_annmax.residual / 1000 # convert to m

# make plots
p1 = plot(
    dat_annmax.Year,
    dat_annmax.residual;
    xlabel="Year",
    ylabel="Annual Max Tide Level (m)",
    label=false,
    marker=:circle,
    markersize=5,
    tickfontsize=16,
    guidefontsize=18,
    left_margin=5mm, 
    bottom_margin=5mm
)

n = nrow(dat_annmax)
linfit = lm(@formula(residual ~ Year), dat_annmax)
pred = coef(linfit)[1] .+ coef(linfit)[2] * dat_annmax.Year

plot!(p1, dat_annmax.Year, pred, linewidth=3, label="Linear Trend")
```

::: {.notes}
For example, consider this annual extreme high water dataset from San Francisco from 1897 through 2022. A linear fit gives us an observed trend of 0.4 mm/yr. Is that meaningful? 

Note that we didn't do anything yet to justify whether linear regression is a non-stupid thing to do (hint: it's a stupid thing to do): we'll talk about this more later.
:::

## The Null: Is The Trend Real?

$\mathcal{H}_0$ (Null Hypothesis):

- The "trend" is just due to chance, there is no "true" long-term trend in the data.
  
::: {.fragment .fade-in}
- Statistically: 

$$y = \underbrace{b}_{\text{constant}} + \underbrace{\varepsilon}_{\text{residuals}}, \qquad \varepsilon \underbrace{\sim}_{\substack{\text{distributed} \\ {\text{according to}}}} \mathcal{N}(0, \sigma^2) $$

:::

## An Alternative Hypothesis

$\mathcal{H}$:

- The trend is *likely* non-zero in time.
  
::: {.fragment .fade-in}
- Statistically: 

$$y = a \times t + b + \varepsilon, \qquad \varepsilon \sim Normal(0, \sigma^2) $$

:::

## Null Test

Comparing $\mathcal{H}$ with $\mathcal{H}_0$:

- $\mathcal{H}$: $a \neq 0$
- $\mathcal{H}_0$: $a = 0$

::: {.note}
In this example, our null is an example of a *point-null* hypothesis.
:::

## Computing the Test Statistic

For this type of null hypothesis test, our **test statistic** is the slope of the linear fit. OLS estimate: $$\hat{a} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{(x_i - \bar{x})^2}.$$

Idea is that even **assuming the null hypothesis**, we could obtain many different datasets, **some of which will have a non-zero slope by chance**.


## Sampling Distribution of Test Statistic

The distribution of all of these slopes is the **sampling distribution**.

Standard result (don't worry if this isn't familiar to you):

**Assuming the null**, the **sampling distribution** of the slope statistic is given by a t-distribution: 

$$\frac{\hat{a}}{SE_{\hat{a}}} \sim t_{n-2}.$$

## Statistical Significance

Is the value of the test statistic consistent with the null hypothesis?

:::: {.columns}
::: {.column width=40%}
More formally, could the test statistic have been reasonably observed from a random sample **given the null hypothesis**?
:::

::: {.column width=60%}
```{julia}
null_test = TDist(n-2)
plot(null_test, label="Test Distribution (Null)")
vline!([coef(linfit)[2] / stderror(linfit)[2]], color=:red, label="OLS Test Statistic")
plot!(size=(700, 400))
```
:::
::::

## p-Values: Quantification of "Surprise"

:::: {.columns}
::: {.column width=50%}

One-Tailed Test: 

```{julia}
#| output: true
#| echo: false
#| label: fig-p-value-one-tail
#| fig-cap: Illustration of a p-value

test_dist = Normal(0, 3)
x = -10:0.01:10
plot(x, pdf.(test_dist, x), linewidth=3, legend=false, color=:black, xticks=false,  yticks=false, ylabel="Probability", xlabel="Test Statistic", bottom_margin=10mm, left_margin=10mm, guidefontsize=16)
vline!([5], linestyle=:dash, color=:purple, linewidth=3, )
areaplot!(5:0.01:10, pdf.(test_dist, 5:0.01:10), color=:green, alpha=0.4)
quiver!([-4.5], [0.095], quiver=([1], [-0.02]), color=:black, linewidth=2)
annotate!([-5], [0.11], text("Null\nSampling\nDistribution", color=:black))
quiver!([6.5], [0.03], quiver=([-1], [-0.015]), color=:green, linewidth=2)
annotate!([6.85], [0.035], text("p-value", :green))
quiver!([3.5], [0.02], quiver=([1.5], [0]), color=:purple, linewidth=2)
annotate!([2], [0.02], text("Observed\nTest Statistic", :purple))
plot!(size=(600, 400))
```
:::
::: {.column width=50%}
Two-Tailed Test:

```{julia}
#| output: true
#| echo: false
#| label: fig-p-value-two-tail
#| fig-cap: Illustration of a two-tailed p-value

vline!([-5], linestyle=:dash, color=:purple, linewidth=3, )
areaplot!(-10:0.01:-5, pdf.(test_dist, -10:0.01:-5), color=:green, alpha=0.4)
plot!(size=(600, 400))
```
:::
::::



# Statistical Significance

## Error Types

<table>
  <tr>
    <td></td>
    <td></td>
    <td colspan="2">**Null Hypothesis Is**</td>
  </tr>
  <tr>
    <td></td>
    <td></td>
    <td>True</td>
    <td>False </td>
  </tr>
  <tr>
    <td rowspan="2">**Decision About Null Hypothesis**</td>
    <td>Don't reject</td>
    <td>True negative (probability $1-\alpha$)</td>
    <td>Type II error (probability $\beta$)</td>
  </tr>
  <tr>
    <td>Reject</td>
    <td>Type I Error (probability $\alpha$)</td>
    <td>True positive (probability $1-\beta$)</td>
  </tr>
</table>

::: {.notes}
The general testing framework is built around Type I (false positive) and Type II (false negative) errors.
:::

## Navigating Type I and II Errors

The standard null hypothesis significance framework is based on balancing the chance of making **Type I (false positive)** and **Type II (false negative)** errors.

**Idea**: Set a significance level $\alpha$ which is an "acceptable" probability of making a Type I error.

**Aside**: The probability $1-\beta$ of correctly rejecting $H_0$ is the *power*.

::: {.notes}
Note that these are frequentist concepts, not applicable to a single dataset (which give rise to p-values, which are random values).

If we only run a single experiment all we can claim is that if we had run a long series of experiments we would have had 100α% false positives had H0 been true and 100β% false negatives had H1 been true provided we got the power calculations right. Note the conditionals.
:::

## p-Value and Significance

**Common practice**: If the p-value is sufficiently small (below $\alpha$), **reject the null hypothesis** with $1-\alpha$ confidence, or declare that **the alternative hypothesis is statistically significant** at the $1-\alpha$ level.

This can mean:

::: {.fade-in .fragment}
1. The null hypothesis is not true for that data-generating process;
2. The null hypothesis *is* true but the data is an outlying sample.
:::

::: {.notes}
This is a strange hybrid of two schools of frequentist statistics, that of Fisher and of Neyman-Pearson. Fisher viewed p-values as weak evidence which was part of an inductive process (even when assuming unbiased sampling and accurate measurement), while the Neyman-Pearson significance framework was based on quantitative specifications of alternative hypotheses with explicitly calculated power.
:::

## What p-Values Are Not

:::: {.columns}
::: {.column width=50%}
1. Probability that the null hypothesis is true (this is **never computed**);
2. An indication of the effect size (or the stakes of that effect).
:::
::: {.column width=50%}
$$ \underbrace{p(S \geq \hat{S}) | \mathcal{H}_0)}_{\text{p-value}} \neq \underbrace{p(\mathcal{H}_0 | S \geq \hat{S})}_{\substack{\text{probability of} \\ \text{null}}}!$$
:::
::::

::: {.notes}
A p-value is a random variable which depends on the data. It's evidence which can be collected from a single experiment but can not establish . 

Over repeated experiments, reasoning about validity of the null should have the right properties assuming the experiments are conducted faithfully.
:::


# Problems with Null Hypothesis Testing

## Statistical Significance &ne; Scientific Significance

:::: {.columns}
::: {.column width=50%}

Statistical significance does not mean anything about whether the alternative hypothesis is:

1. "true";
2. an accurate reflection of the data-generating process.

:::
::: {.column width=50%}
![Hypothesis vs. Causal Meme](memes/godzilla_doge_causal.png)
:::
:::

## What is Any Statistical Test Doing?

1. Assume the null hypothesis $\mathcal{H}_0$.
2. Compute the test statistic $\hat{S}$ for the sample.
3. **Obtain the sampling distribution of the test statistic $S$ under $H_0$.**
4. Calculate $\mathbb{P}(S > \hat{S})$ (*the p-value*).


## Non-Uniqueness of "Null" Models

:::: {.columns}
::: {.column width=50%}
Is there a trend in the SF tide gauge trend data?

- Trend as regression ($p\text{-value} \approx 0.02$)
- Mann-Kendall test for monotonic trend ($p\text{-value} \approx 0.5$)
:::
::: {.column width=50%}
![Non-Uniqueness of Null Models](figures/mcelreath_hypothesis_nonunique.png)

::: {.caption}
Source: @mcelreath2020statistical [Fig. 1.2]
:::
:::
::::


## Multiple Comparisons

:::: {.columns}
::: {.column width=50%}
If you conduct multiple statistical tests, you **must** account for all of these in the p-value computation and assessment of significance.

**Important**: This includes model selection!
:::

::: {.column width=50%}
![Multiple Comparisons Meme](memes/multiple_comparisons.png)
:::
::::

::: {.notes}
The core issue is the standard test statistics have the right Type I/Type II properties for each individual test, but multiple tests distort these frequencies, sometimes quite dramatically.

For example, suppose each individual test has a 5% Type I error rate. If you test 100 different models, and the errors are independent, you would expect 5 false positives, one of which would be selected by minimizing the p-value. The probability of at least one type I error is >99%.

There are a number of corrections (Bonferroni being the most common), but sometimes stepwise tests are subtle, including cases of model selection followed by model fitting. You can also use simulation to estimate the false-positive rate for the procedure under a null data-generating process, which ties into the broader methods we'll discuss.
:::

## Results Are Flashy, But Meaningless Without Methods

![Elton John Results Section Meme](memes/elton_john_results.jpg)

::: {.caption}
Source: Richard McElreath
:::


## Note: This Does Not Mean Null Hypothesis Testing Is Useless!

:::: {.columns}
::: {.column width=50%}
Examining and testing the implications of competing models is important, including "null" models!

:::
::: {.column width=50%}
![Null Hypothesis Selection Good Vs. Bad](memes/geordi_null_choice.png){width=75%}
:::
::::

::: {.notes}
Deborah Mayo discusses this interpretation as "severe testing": apply different levels of scrutiny to a scientific model to see what level of severity breaks the model.

The idea of a p-value (not necessarily "significance") being a piece of inductive evidence rather than a threshold for validity (especially for an individual experiment) reflects the original views of Fisher.
:::

## What Might Be More Satisfying?

::: {.incremental}
- Consideration of multiple plausible (possibly more nuanced) hypotheses.
- Assessment/quantification of evidence consistent with different hypotheses.
- Identification of opportunities to design experiments/learn.
- Insight into the effect size.
:::


# Key Points

## Hypothesis Testing

- Classical framework: Compare a null hypothesis (no effect) to an alternative (some effect)
- $p$-value: probability (under $H_0$) of more extreme test statistic than observed.
- "Significant" if $p$-value is below a significance level reflecting acceptable Type I error rate.

## Problems with NHST framework

- Real "null" hypotheses are often more nuanced than in typical tests (which were often developed for controlled experiments or for computational convenience).
- Decisions are often not binary ("significant/not significant").
- $p$-values are often over-interpreted and are often be incorrectly calculated, with negative outcomes!
- **Important**: "Big" data can make things worse, as NHST is highly sensitive to small but evidence effects.

# Upcoming Schedule

## Next Classes

**Wednesday**: Probability Models and Exploratory (Graphical) Analysis

**Friday**: Multiple Linear Regression.

## Assessments

**Homework 1** available; due *next* Friday (2/6).

# References

## References (Scroll for Full List)
