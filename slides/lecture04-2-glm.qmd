---
title: "Bayesian Workflow Example"
subtitle: "Lecture 09"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "February 24, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        mermaid: 
            theme: dark
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using Distributions
using DataFrames
using DataFramesMeta
using CSV
using Dates
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using Optim
using LaTeXStrings
using Measures

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Logistic Regression Example

## Forecasting Precipitation

```{julia}
#| label: fig-snowqualmie
#| fig-cap: Snoqualmie Falls, WA precipitation dataset.
#| echo: true
#| code-fold: true
#| warning: false

# read in data; each year is a row and each column is a day
dat = CSV.read("data/weather/snoqualmie_falls.txt", delim=" ", ignorerepeated=true, silencewarnings=true, skipto=2, DataFrame)
years = 1948:1983
# the last "day" of each non-leap year is NA, so need to skip over these
days_per_year = repeat([366, 365, 365, 365], outer=Int(ceil(length(years) / 4)))
snoq = zeros(sum(days_per_year))
for i = 1:length(years)
    # need to use values() to get the vector of elements in the row; this is a quirk of DataFrames.jl
    snoq[1+sum(days_per_year[1:i-1]):sum(days_per_year[1:i])] .= values(dat[i, 1:days_per_year[i]])
end
# want to arrange dataframe with today's precipitation (predictor) and tomorrow's (prediction)
snoq_dat = DataFrame(today=snoq[1:end-1], tomorrow=snoq[2:end])
p1 = histogram(snoq_dat.today, xlabel="Precipitation (1/100 inch)", ylabel="Count", label=false)
p2 = scatter(snoq_dat.today, snoq_dat.tomorrow, xlabel="Precip Today (1/100 inch)", ylabel="Precip Tomorrow (1/100 inch)", markersize=2, alpha=0.2, label=false)
plot(p1, p2, layout=(1, 2), size=(1200, 500))
```

## Forecasting Model

Let's try to forecast when there will be non-zero precipitation ($Y=1$) vs. zero precipitation ($Y=0$) from the previous day's precipitation.

$$\text{logit}(p_i) = \text{logit}(p(Y_i = 1)) = \beta_0 + \beta_1 x_i$$

## Maximum Likelihood Estimation

```{julia}
#| echo: true
#| code-fold: false

invlogit(x) = exp(x) / (1 + exp(x)) # define inverse logit
function logreg_loglik(params, y, x)
    b0, b1 = params
    p = invlogit.(b0 .+ b1 * x) # logistic regression
    ll = sum(logpdf.(Bernoulli.(p), y)) # use Bernoulli log-pdf as log-likelihood
    return ll
end

lb = [-1.0, -1.0]
ub = [1.0, 1.0]
p0 = [0.0, 0.0]
optim_out = Optim.optimize(p -> -logreg_loglik(p, snoq_dat.tomorrow .> 0, snoq_dat.today), lb, ub, p0)
mle = round.(optim_out.minimizer; digits=2)
```

## Visualizing Results


```{julia}
#| label: fig-forecast-results
#| fig-cap: Modeled probability of precipitation tomorrow as a function of precipitation today.
#| echo: true
#| code-fold: true

x_pred = 0:0.1:maximum(snoq_dat.today)
p_pred = invlogit.(mle[1] .+ mle[2] * x_pred)
plot(x_pred, p_pred, linewidth=3, label="Modeled Probability", xlabel="Precip Today (1/100 inch)", ylabel="Positive Precip Tomorrow?", size=(1000, 400))
scatter!(snoq_dat.today, snoq_dat.tomorrow .> 0, alpha=0.2, label="Data")
```

## Calibration

Does the predicted CDF $F(y)$ align with the "true" distribution of observations $y$? 
  $$\mathbb{P}(y \leq F^{-1}(\tau)) = \tau \qquad \forall \tau \in [0, 1]$$
  
In this case: does the model produce the "right" probabilities of events?

## Example: Calibration

We have a total of 6,919 days with precipitation. To see the expected number from the model, sum up the probabilities:

```{julia}
#| echo: true
#| code-fold: false

p_fit = invlogit.(mle[1] .+ mle[2] * snoq_dat.today)
round(sum(p_fit); digits=0)
```

We're off by about 1.3%. **Is that good?**

## Example: Calibration

How well calibrated is the prediction for a no-precipitation day followed by a precipitation day?

:::: {.columns}
::: {.column width=50%}
The actual fraction: 

```{julia}
#| echo: true
#| code-fold: false

emp_pnp = mean(snoq_dat[snoq_dat.today .== 0, :tomorrow] .> 0)
round(emp_pnp; digits=3)
```
:::
::: {.column width=50%}
The modeled fraction:
```{julia}
#| echo: true
#| code-fold: false

mod_pnp = mean(p_fit[snoq_dat.today .== 0])
round(mod_pnp; digits=3)
```
:::
::::

## Potential Model Improvements

**What might we try to improve this forecast?**

# Generalized Linear Models

## Link Functions

Logistic regression is an example of a **generalized linear model (GLM)**.

Start with a distributional assumption: $y_i \sim \text{Bernoulli}(p_i)$.

Then model parameters using a **link function** and a regression:

$$\underbrace{\logit}_{\text{link}}(p_i) = \sum_j \beta_j x^i_j$$

## 

# Poisson Regression Example

## Modeling Counts

A common use of GLMs is to model **counts**.


