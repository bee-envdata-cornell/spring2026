---
title: "Generalized Linear Models"
subtitle: "Lecture 09"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "February 10, 2026"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        mermaid: 
            theme: dark
engine: julia
filters:
  - code-fullscreen
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using Distributions
using DataFrames
using DataFramesMeta
using CSV
using Dates
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using Optim
using LaTeXStrings
using Measures

Random.seed!(1)

plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=3, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    guidefontsize=18,
    legendfontsize=16,
    tickfontsize=16,
    titlefontsize=20,
    bottom_margin=10mm,
    left_margin=5mm
)
```

# Review

## Logistic Regression

- Used to model probabilities for classification.
- Two-class (*e.g.* success or failure:

$$\begin{aligned}
y_i &\sim Bernoulli(p_i) \\
\text{logit}(p_i) &= \sum_j \beta_j x^i_j
\end{aligned}$$

where $\text{logit}(p) = \log\left(p / (1 - p)\right)$.

## Multi-Class Logistic Regression

Treat one class as a reference. Other classes $c$ will have its own intercept $\beta_0^c$ and coefficients $\beta^c$.

$$p(Y = c | X = \mathbf{x}) = \frac{\exp\left(\beta_0^c + x \cdot \beta^c \right)}{\sum_c \exp\left(\beta_0^c + x \cdot \beta^c\right) }$$

## Interpretation of Coefficients

$\exp(\beta^c_j)$ gives the **relative change in the odds of class c** from a unit change in $x$.

# Logistic Regression Example

## Forecasting Precipitation

```{julia}
#| label: fig-snowqualmie
#| fig-cap: Snoqualmie Falls, WA precipitation dataset.
#| echo: true
#| code-fold: true
#| warning: false

# read in data; each year is a row and each column is a day
dat = CSV.read("data/weather/snoqualmie_falls.txt", delim=" ", ignorerepeated=true, silencewarnings=true, skipto=2, DataFrame)
years = 1948:1983
# the last "day" of each non-leap year is NA, so need to skip over these
days_per_year = repeat([366, 365, 365, 365], outer=Int(ceil(length(years) / 4)))
snoq = zeros(sum(days_per_year))
for i = 1:length(years)
    # need to use values() to get the vector of elements in the row; this is a quirk of DataFrames.jl
    snoq[1+sum(days_per_year[1:i-1]):sum(days_per_year[1:i])] .= values(dat[i, 1:days_per_year[i]])
end
# want to arrange dataframe with today's precipitation (predictor) and tomorrow's (prediction)
snoq_dat = DataFrame(today=snoq[1:end-1], tomorrow=snoq[2:end])
p1 = histogram(snoq_dat.today, xlabel="Precipitation (1/100 inch)", ylabel="Count", label=false)
p2 = scatter(snoq_dat.today, snoq_dat.tomorrow, xlabel="Precip Today (1/100 inch)", ylabel="Precip Tomorrow (1/100 inch)", markersize=2, alpha=0.2, label=false)
plot(p1, p2, layout=(1, 2), size=(1200, 500))
```

## Forecasting Model

Let's try to forecast when there will be non-zero precipitation ($Y=1$) vs. zero precipitation ($Y=0$) from the previous day's precipitation.

$$\text{logit}(p_i) = \text{logit}(p(Y_i = 1)) = \beta_0 + \beta_1 x_i$$

## Maximum Likelihood Estimation

```{julia}
#| echo: true
#| code-fold: false

invlogit(x) = exp(x) / (1 + exp(x)) # define inverse logit
function logreg_loglik(params, y, x)
    b0, b1 = params
    p = invlogit.(b0 .+ b1 * x) # logistic regression
    ll = sum(logpdf.(Bernoulli.(p), y)) # use Bernoulli log-pdf as log-likelihood
    return ll
end

lb = [-1.0, -1.0]
ub = [1.0, 1.0]
p0 = [0.0, 0.0]
optim_out = Optim.optimize(p -> -logreg_loglik(p, snoq_dat.tomorrow .> 0, snoq_dat.today), lb, ub, p0)
mle = round.(optim_out.minimizer; digits=2)
```

## Visualizing Results


```{julia}
#| label: fig-forecast-results
#| fig-cap: Modeled probability of precipitation tomorrow as a function of precipitation today.
#| echo: true
#| code-fold: true

x_pred = 0:0.1:maximum(snoq_dat.today)
p_pred = invlogit.(mle[1] .+ mle[2] * x_pred)
plot(x_pred, p_pred, linewidth=3, label="Modeled Probability", xlabel="Precip Today (1/100 inch)", ylabel="Positive Precip Tomorrow?", size=(1000, 400))
scatter!(snoq_dat.today, snoq_dat.tomorrow .> 0, alpha=0.2, label="Data")
```

## Calibration

Does the predicted CDF $F(y)$ align with the "true" distribution of observations $y$? 
  $$\mathbb{P}(y \leq F^{-1}(\tau)) = \tau \qquad \forall \tau \in [0, 1]$$
  
In this case: does the model produce the "right" probabilities of events?

## Example: Calibration

We have a total of 6,919 days with precipitation. To see the expected number from the model, sum up the probabilities:

```{julia}
#| echo: true
#| code-fold: false

p_fit = invlogit.(mle[1] .+ mle[2] * snoq_dat.today)
round(sum(p_fit); digits=0)
```

We're off by about 1.3%. **Is that good?**

## Example: Calibration

How well calibrated is the prediction for a no-precipitation day followed by a precipitation day?

:::: {.columns}
::: {.column width=50%}
The actual fraction: 

```{julia}
#| echo: true
#| code-fold: false

emp_pnp = mean(snoq_dat[snoq_dat.today .== 0, :tomorrow] .> 0)
round(emp_pnp; digits=3)
```
:::
::: {.column width=50%}
The modeled fraction:
```{julia}
#| echo: true
#| code-fold: false

mod_pnp = mean(p_fit[snoq_dat.today .== 0])
round(mod_pnp; digits=3)
```
:::
::::

## Potential Model Improvements

**What might we try to improve this forecast?**

# Generalized Linear Models

## Linear vs. Logistic Regression

**Linear Regression**: Model the **conditional expectation function** $$\mu(x)  = \mathbb{E}[Y | X=\mathbf{x}] \approx \beta_0 + \sum_i \beta_i x_i.$$

**Logistic Regression**: 

$$\mu(x)  = \mathbb{E}[Y | X=\mathbf{x}] = \mathbb{P}[Y = 1 | X = \mathbf{x}].$$

We transform the conditional expectation to be linear (using $g(x) = \text{logit}(x)$):

$$g(\mu(x)) = \eta(x) \approx  \beta_0 + \sum_i \beta_i x_i.$$

## Link Functions

Logistic regression is an example of a **generalized linear model (GLM)**.

Start with a distributional assumption: $y_i \sim \text{Bernoulli}(p_i)$.

Then model parameters using a **link function** and a linear predictor:

$$\underbrace{\logit}_{\text{link}}(p_i) = \underbrace{\sum_j \beta_j x^i_j}_{\text{predictor}}$$

## Choice of Distribution

Many possible distributions depending on the problem at hand. Common ones:

- **Binomial**: Integer count from 0 to some upper limit $n$; $$y_i \sim \text{Binomial}(p_i)$$
- **Poisson**: Modeling counts with no fixed upper limit, but low base probability of "success": $$y_i \sim \text{Poisson}(\lambda_i)$$

## Choosing a Link Function

Link functions are typically linked to distributions.

- Poisson models usually use the **log link** (positives &rarr; reals).
- Binomial/Bernoulli models use the **logit** link ($[0, 1]$ &rarr; reals) $$\text{logit}(p) = \log\left(\frac{p}{1-p}\right)$$

## ...But A Link Function Is A Choice

:::: {.columns}
::: {.column width=50%}
These are "canonical" choices for their associated distributions because they are flexible and have nice mathematical properties, but you can make others as relevant for your data and problem.
:::
::: {.column width=50%}
```{julia}
#| echo: true
#| code-fold: true
#| label: fig-logit
#| fig-cap: Logit Function

p = 0:0.01:1
logit(p) = log(p / (1 - p))
plot(p, logit.(p), xlabel=L"$x$", ylabel=L"$y$", linewidth=3, legend=false, size=(600, 450))
```
:::
::::

# Poisson Regression Example

## Modeling Counts

```{julia}
#| echo: true
#| code-fold: true
#| output: true

fish = CSV.File("data/ecology/Fish.csv") |> DataFrame
fish[1:3, :]
```

## What Distribution?

::: {.fragment .fade-in}
Count data can be modeled using a Poisson or negative binomial distribution.

```{julia}
#| label: fig-count-dists
#| fig-cap: 
#|  - "Poisson"
#|  - "Negative Binomial"
#| layout-ncol: 2

p1 = plot(Poisson(2), size=(500, 450), title="Poisson(2)", xlabel="Value")
p2 = plot(NegativeBinomial(2, 0.5), size=(500, 450), title="Negative Binomial(2, 0.5)", xlabel="Value")
display(p1)
display(p2)
```
:::

## What Distribution?

Let's start with a Poisson. 

One property of $y \sim \text{Poisson}(\lambda)$: $\mathbb{E}(y) = \text{Var}(y) = \lambda$.

- Mean: `{julia} round(mean(fish.fish_caught); digits=1)`
- Variance: `{julia} round(var(fish.fish_caught); digits=1)`


## What Does This Mean?

:::: {.columns}
::: {.column width=50%}
- The data is **overdispersed** relative to a Poisson distribution.
- But a Poisson GLM may give rise to overdispersed data, since every observation has a different $\lambda_i$ (hence mean and variance.)
:::
::: {.column width=50%}

```{julia}
#| label: fig-fishing-data
#| fig-cap: Fishing Data

histogram(fish.fish_caught, xlabel="Caught Fish", ylabel="Count", size=(600, 500))
```
:::
::::

## Be Cautious...

:::: {.columns}
::: {.column width=45%}
- Over-relying on plots like histograms can lead to mis-specifying GLM dynamics.
- Think of linear regression: the question is **not if the data are Gaussian but the residuals**.
:::
::: {.column width=55%}
![Brain on Regression Meme](memes/brain_on_regression.jpg){width=45%}

::: {.caption}
Source: Richard McElreath
:::
:::
::::

## Model Specification

We might hypothesize that the number of fish caught is influenced by the number of children (noisy, distracting, impatient) and whether the party brought a camper.

$$\begin{align*}
y_i &\sim \text{Poisson}(\lambda_i) \\
f(\lambda_i) &= \beta_0 + \beta_1 \text{child}_i + \beta_2 \text{camper}_i
\end{align*}
$$

::: {.fragment .fade-in}
$\lambda_i$: positive "rate"

$f(\cdot)$: maps positive reals (rate scale) to all reals (linear model scale)
:::


## Fitting the Model

```{julia}
#| echo: true
#| code-fold: false
#| code-line-numbers: "|1-5|7-11"

function fish_model(params, child, camper, fish_caught)
    β₀, β₁, β₂ = params
    λ = exp.(β₀ .+ β₁ * child + β₂ * camper)
    loglik = sum(logpdf.(Poisson.(λ), fish_caught))
end

lb = [-100.0, -100.0, -100.0]
ub = [100.0, 100.0, 100.0]
init = [0.0, 0.0, 0.0]

optim_out = optimize(θ -> -fish_model(θ, fish.child, fish.camper, fish.fish_caught), lb, ub, init)
θ_mle = optim_out.minimizer
@show round.(θ_mle; digits=2);
@show round.(exp.(θ_mle); digits=2);
```

## Interpretation of Coefficients

Since $\log(\lambda_i) = \sum_j \beta_j x^i_j$: 

$\exp(\beta_j)$ says how much a unit increase in $x_j$ multiplies the rate (hence expected value).

So $\beta_1 =$ `{julia} round(θ_mle[2]; digits=1)` says that each extra child reduces the expected number of fish caught by a factor of `{julia} round(exp(θ_mle[2]); digits=1)`.

## Predictive Checks To Evaluate Model

- Linear regression: model checking was straightforward: residuals should have been Gaussian distributed.
- GLMs: Less clear how to check distributional assumptions.

**Strategy**: check predictive distribution of relevant summary statistic(s) by simulation; do you capture the empirical statistic?

## Predictive Checks

In other words, for some statistic $T$:

- Estimate $\hat{T}$ from the data.
- Simulate synthetic datasets using your model and calculate $\tilde{T}_n$ for each one;
- Examine the distribution of $(\tilde{T})$ to see where $\hat{T}$ falls.

**Teaser**: This is closely related to the idea of a $p$-value...

## Checking Proportion of Zero Fish

```{julia}
#| label: fig-fish-skill
#| fig-cap: Predictive distribution for fitted fish model.
#| code-line-numbers: "|1-3|5-8"

# simulate samples for each P
λ = exp.(θ_mle[1] .+ θ_mle[2] * fish.child .+ θ_mle[3] * fish.camper)
# draw 10,000 samples for each
fish_sim = zeros(10_000, nrow(fish)) # initialize matrix to store simulations
for i = 1:nrow(fish)
    fish_sim[:, i] = rand(Poisson(λ[i]), 10_000)
end

# calculate frequency of zeros
fish_zeros = mapslices(row -> sum(row .== 0) / length(row), fish_sim; dims=1)
histogram(fish_zeros', xlabel="Proportion of Zeros", ylabel="Count", label=false)
vline!([sum(fish.fish_caught .== 0) / nrow(fish)], color=:red, linewidth=3, label="Empirical Statistic")
```

## What Was The Problem?

- The model produces far fewer zero-fish counts than the data.
- This is typical of a lot of environmental count data; often have far more zeros than would be expected from a Poisson regression.
- Alternative: Use a **zero-inflated model**.


# Key Points

## GLMs

- Generalized linear models: use an alternative distribution but let parameters vary.
- Require a choice of distribution, a link function, and a linear model.
- Lots of different choices of distributions --- think about structure of the data and generative properties of the distributions.
- Simulation is valuable for checking model fit.


# Upcoming Schedule

## Upcoming Schedule

**Friday**: Quiz 1 for first 25 minutes.

**Monday**: No class!

**Next Week**: Modeling extreme values.