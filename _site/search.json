[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Class Schedule",
    "section": "",
    "text": "This page contains a schedule of the topics, content, and assignments for the semester. Note that this schedule will be updated as necessary the semester progresses, with all changes documented here.\nInstructions to save the slides as PDFs can be found here. The key is to hit E when the slides are open in your browser (ideally Chrome, but it may work in others) to toggle into PDF print mode.\nReadings can be accessed using the link (when needed, through the Cornell library with a Cornell login) or on Canvas.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nSlides\nReading\nHomework\nQuiz\n\n\n\n\n1\n1/21\nClass Introduction\n\nO’Hagan (2004)\n\n\n\n\n\n1/23\nProbability Review\n\n\n\n\n\n\n2\n1/26\nExploratory Data Analysis\n\n\n\n\n\n\n\n1/28\nData Visualization\n\n\n\n\n\n\n\n1/30\nVisualization Discussion"
  },
  {
    "objectID": "tutorials/julia-plots.html",
    "href": "tutorials/julia-plots.html",
    "title": "Tutorial: Making Plots with Julia",
    "section": "",
    "text": "This tutorial will give some examples of plotting and plotting features in Julia, as well as providing references to some relevant resources. The main plotting library is Plots.jl, but there are some others that provide useful features.",
    "crumbs": [
      "Tutorial: Making Plots with Julia"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html#overview",
    "href": "tutorials/julia-plots.html#overview",
    "title": "Tutorial: Making Plots with Julia",
    "section": "",
    "text": "This tutorial will give some examples of plotting and plotting features in Julia, as well as providing references to some relevant resources. The main plotting library is Plots.jl, but there are some others that provide useful features.",
    "crumbs": [
      "Tutorial: Making Plots with Julia"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html#some-resources",
    "href": "tutorials/julia-plots.html#some-resources",
    "title": "Tutorial: Making Plots with Julia",
    "section": "Some Resources",
    "text": "Some Resources\n\nPlots.jl useful tips\nPlots.jl examples\nPlot attributes\nAxis attributes\nColor names",
    "crumbs": [
      "Tutorial: Making Plots with Julia"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html#demos",
    "href": "tutorials/julia-plots.html#demos",
    "title": "Tutorial: Making Plots with Julia",
    "section": "Demos",
    "text": "Demos\n\nusing Plots\nusing Random\nRandom.seed!(1);\n\n\nLine Plots\nTo generate a basic line plot, use plot.\n\ny = rand(5)\nplot(y, label=\"original data\", legend=:topright)\n\n\n\n\nThere’s a lot of customization here that can occur, a lot of which is discussed in the docs or can be found with some Googling.\n\n\nAdding Plot Elements\nNow we can add some other lines and point markers.\n\ny2 = rand(5)\ny3 = rand(5)\nplot!(y2, label=\"new data\")\nscatter!(y3, label=\"even more data\")\n\n\n\n\nRemember that an exclamation mark (!) at the end of a function name means that function modifies an object in-place, so plot! and scatter! modify the current plotting object, they don’t create a new plot.\n\n\nRemoving Plot Elements\nSometimes we want to remove legends, axes, grid lines, and ticks.\n\nplot!(legend=false, axis=false, grid=false, ticks=false)\n\n\n\n\n\n\nAspect Ratio\nIf we want to have a square aspect ratio, use ratio = 1.\n\nv = rand(5)\nplot(v, ratio=1, legend=false)\nscatter!(v)\n\n\n\n\n\n\nHeatmaps\nA heatmap is effectively a plotted matrix with colors chosen according to the values. Use clim to specify a fixed range for the color limits.\n\nA = rand(10, 10)\nheatmap(A, clim=(0, 1), ratio=1, legend=false, axis=false, ticks=false)\n\n\n\n\n\nM = [ 0 1 0; 0 0 0; 1 0 0]\nwhiteblack = [RGBA(1,1,1,0), RGB(0,0,0)]\nheatmap(c=whiteblack, M, aspect_ratio = 1, ticks=.5:3.5, lims=(.5,3.5), gridalpha=1, legend=false, axis=false, ylabel=\"i\", xlabel=\"j\")\n\n\n\n\n\nCustom Colors\n\nusing Colors\n\nmycolors = [colorant\"lightslateblue\",colorant\"limegreen\",colorant\"red\"]\nA = [i for i=50:300, j=1:100]\nheatmap(A, c=mycolors, clim=(1,300))\n\n\n\n\n\n\n\nPlotting Areas Under Curves\n\ny = rand(10)\nplot(y, fillrange= y.*0 .+ .5, label= \"above/below 1/2\", legend =:top)\n\n\n\n\n\nx = LinRange(0,2,100)\ny1 = exp.(x)\ny2 = exp.(1.3 .* x)\nplot(x, y1, fillrange = y2, fillalpha = 0.35, c = 1, label = \"Confidence band\", legend = :topleft)\n\n\n\n\n\nx = -3:.01:3\nareaplot(x, exp.(-x.^2/2)/√(2π),alpha=.25,legend=false)\n\n\n\n\n\nM = [1 2 3; 7 8 9; 4 5 6; 0 .5 1.5]\nareaplot(1:3, M, seriescolor = [:red :green :blue ], fillalpha = [0.2 0.3 0.4])\n\n\n\n\n\nusing SpecialFunctions\nf = x-&gt;exp(-x^2/2)/√(2π)\nδ = .01\nplot()\nx = √2 .* erfinv.(2 .*(δ/2 : δ : 1) .- 1)\nareaplot(x, f.(x), seriescolor=[ :red,:blue], legend=false)\nplot!(x, f.(x),c=:black)\n\n\n\n\n\n\nPlotting Shapes\n\nrectangle(w, h, x, y) = Shape(x .+ [0,w,w,0], y .+ [0,0,h,h])\ncircle(r,x,y) = (θ = LinRange(0,2π,500); (x.+r.*cos.(θ), y.+r.*sin.(θ)))\nplot(circle(5,0,0), ratio=1, c=:red, fill=true)\nplot!(rectangle(5*√2,5*√2,-2.5*√2,-2.5*√2),c=:white,fill=true,legend=false)\n\n\n\n\n\n\nPlotting Distributions\nThe StatsPlots.jl package is very useful for making various plots of probability distributions.\n\nusing Distributions, StatsPlots\nplot(Normal(2, 5))\n\n\n\n\n\nscatter(LogNormal(0.8, 1.5))\n\n\n\n\nWe can also use this functionality to plot distributions of data in tabular data structures like DataFrames.\n\nusing DataFrames\ndat = DataFrame(a = 1:10, b = 10 .+ rand(10), c = 10 .* rand(10))\n@df dat density([:b :c], color=[:black :red])\n\n\n\n\n\n\nEditing Plots Manually\n\npl = plot(1:4,[1, 4, 9, 16])\n\n\n\n\n\npl.attr\n\nRecipesPipeline.DefaultsDict with 30 entries:\n  :dpi                      =&gt; 96\n  :background_color_outside =&gt; :match\n  :plot_titlefontvalign     =&gt; :vcenter\n  :warn_on_unsupported      =&gt; true\n  :background_color         =&gt; RGBA{Float64}(1.0, 1.0, 1.0, 1.0)\n  :inset_subplots           =&gt; nothing\n  :size                     =&gt; (672, 480)\n  :display_type             =&gt; :auto\n  :overwrite_figure         =&gt; true\n  :html_output_format       =&gt; :auto\n  :plot_titlefontfamily     =&gt; :match\n  :plot_titleindex          =&gt; 0\n  :foreground_color         =&gt; RGB{N0f8}(0.0, 0.0, 0.0)\n  :window_title             =&gt; \"Plots.jl\"\n  :plot_titlefontrotation   =&gt; 0.0\n  :extra_plot_kwargs        =&gt; Dict{Any, Any}()\n  :pos                      =&gt; (0, 0)\n  :plot_titlefonthalign     =&gt; :hcenter\n  :tex_output_standalone    =&gt; false\n  ⋮                         =&gt; ⋮\n\n\n\npl.series_list[1]\n\nPlots.Series(RecipesPipeline.DefaultsDict(:plot_object =&gt; Plot{Plots.GRBackend() n=1}, :subplot =&gt; Subplot{1}, :label =&gt; \"y1\", :fillalpha =&gt; nothing, :linealpha =&gt; nothing, :linecolor =&gt; RGBA{Float64}(0.0, 0.6056031704619725, 0.9786801190138923, 1.0), :x_extrema =&gt; (NaN, NaN), :series_index =&gt; 1, :markerstrokealpha =&gt; nothing, :markeralpha =&gt; nothing…))\n\n\n\npl[:size]=(300,200)\n\n(300, 200)\n\n\n\npl\n\n\n\n\n\n\nLog-Scaled Axes\n\nxx = .1:.1:10\nplot(xx.^2, xaxis=:log, yaxis=:log)\n\n\n\n\n\nplot(exp.(x), yaxis=:log)",
    "crumbs": [
      "Tutorial: Making Plots with Julia"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html",
    "href": "tutorials/julia-basics.html",
    "title": "Tutorial: Julia Basics",
    "section": "",
    "text": "This tutorial will give some examples of basic Julia commands and syntax.",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#overview",
    "href": "tutorials/julia-basics.html#overview",
    "title": "Tutorial: Julia Basics",
    "section": "",
    "text": "This tutorial will give some examples of basic Julia commands and syntax.",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#getting-help",
    "href": "tutorials/julia-basics.html#getting-help",
    "title": "Tutorial: Julia Basics",
    "section": "Getting Help",
    "text": "Getting Help\n\nCheck out the official documentation for Julia: https://docs.julialang.org/en/v1/.\nStack Overflow is a commonly-used resource for programming assistance.\nAt a code prompt or in the REPL, you can always type ?functionname to get help.",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#comments",
    "href": "tutorials/julia-basics.html#comments",
    "title": "Tutorial: Julia Basics",
    "section": "Comments",
    "text": "Comments\nComments hide statements from the interpreter or compiler. It’s a good idea to liberally comment your code so readers (including yourself!) know why your code is structured and written the way it is. Single-line comments in Julia are preceded with a #. Multi-line comments are preceded with #= and ended with =#",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#suppressing-output",
    "href": "tutorials/julia-basics.html#suppressing-output",
    "title": "Tutorial: Julia Basics",
    "section": "Suppressing Output",
    "text": "Suppressing Output\nYou can suppress output using a semi-colon (;).\n\n4+8;\n\nThat didn’t show anything, as opposed to:\n\n4+8\n\n12",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#variables",
    "href": "tutorials/julia-basics.html#variables",
    "title": "Tutorial: Julia Basics",
    "section": "Variables",
    "text": "Variables\nVariables are names which correspond to some type of object. These names are bound to objects (and hence their values) using the = operator.\n\nx = 5\n\n5\n\n\nVariables can be manipulated with standard arithmetic operators.\n\n4 + x\n\n9\n\n\nAnother advantage of Julia is the ability to use Greek letters (or other Unicode characters) as variable names. For example, type a backslash followed by the name of the Greek letter (i.e. \\alpha) followed by TAB.\n\nα = 3\n\n3\n\n\nYou can also include subscripts or superscripts in variable names using \\_ and \\^, respectively, followed by TAB. If using a Greek letter followed by a sub- or super-script, make sure you TAB following the name of the letter before the sub- or super-script. Effectively, TAB after you finish typing the name of each \\character.\n\nβ₁ = 10 # The name of this variable was entered with \\beta + TAB + \\_1 + TAB\n\n10\n\n\nHowever, try not to overwrite predefined names! For example, you might not want to use π as a variable name…\n\nπ\n\nπ = 3.1415926535897...\n\n\nIn the grand scheme of things, overwriting π is not a huge deal unless you want to do some trigonometry. However, there are more important predefined functions and variables that you may want to be aware of. Always check that a variable or function name is not predefined!",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#data-types",
    "href": "tutorials/julia-basics.html#data-types",
    "title": "Tutorial: Julia Basics",
    "section": "Data Types",
    "text": "Data Types\nEach datum (importantly, not the variable which is bound to it) has a data type. Julia types are similar to C types, in that they require not only the type of data (Int, Float, String, etc), but also the precision (which is related to the amount of memory allocated to the variable). Issues with precision won’t be a big deal in this class, though they matter when you’re concerned about performance vs. decimal accuracy of code.\nYou can identify the type of a variable or expression with the typeof() function.\n\ntypeof(\"This is a string.\")\n\nString\n\n\n\ntypeof(x)\n\nInt64\n\n\n\nNumeric types\nA key distinction is between an integer type (or Int) and a floating-point number type (or float). Integers only hold whole numbers, while floating-point numbers correspond to numbers with fractional (or decimal) parts. For example, 9 is an integer, while 9.25 is a floating point number. The difference between the two has to do with the way the number is stored in memory. 9, an integer, is handled differently in memory than 9.0, which is a floating-point number, even though they’re mathematically the same value.\n\ntypeof(9)\n\nInt64\n\n\n\ntypeof(9.25)\n\nFloat64\n\n\nSometimes certain function specifications will require you to use a Float variable instead of an Int. One way to force an Int variable to be a Float is to add a decimal point at the end of the integer.\n\ntypeof(9.)\n\nFloat64\n\n\n\n\nStrings\nStrings hold characters, rather than numeric values. Even if a string contains what seems like a number, it is actually stored as the character representation of the digits. As a result, you cannot use arithmetic operators (for example) on this datum.\n\n\"5\" + 5\n\n\nMethodError: no method matching +(::String, ::Int64)\nThe function `+` exists, but no method is defined for this combination of argument types.\n\nClosest candidates are:\n  +(::Any, ::Any, ::Any, ::Any...)\n   @ Base operators.jl:596\n  +(::Missing, ::Number)\n   @ Base missing.jl:123\n  +(::Complex{Bool}, ::Real)\n   @ Base complex.jl:323\n  ...\n\nStacktrace:\n [1] top-level scope\n   @ ~/Teaching/BEE4850/spring2026/tutorials/julia-basics.qmd:108\n\n\n\nHowever, you can try to tell Julia to interpret a string encoding a numeric character as a numeric value using the parse() function. This can also be used to encode a numeric data as a string.\n\nparse(Int64, \"5\") + 5\n\n10\n\n\nTwo strings can be concatenated using *:\n\n\"Hello\" * \" \" * \"there\"\n\n\"Hello there\"\n\n\n\n\nBooleans\nBoolean variables (or Bools) are logical variables, that can have true or false as values.\n\nb = true\n\ntrue\n\n\nNumerical comparisons, such as ==, !=, or &lt;, return a Bool.\n\nc = 9 &gt; 11\n\nfalse\n\n\nBools are important for logical flows, such as if-then-else blocks or certain types of loops.",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#mathematical-operations",
    "href": "tutorials/julia-basics.html#mathematical-operations",
    "title": "Tutorial: Julia Basics",
    "section": "Mathematical operations",
    "text": "Mathematical operations\nAddition, subtraction, multiplication, and division work as you would expect. Just pay attention to types! The type of the output is influenced by the type of the inputs: adding or multiplying an Int by a Float will always result in a Float, even if the Float is mathematically an integer. Division is a little special: dividing an Int by another Int will still return a float, because Julia doesn’t know ahead of time if the denominator is a factor of the numerator.\n\n3 + 5\n\n8\n\n\n\n3 * 2\n\n6\n\n\n\n3 * 2.\n\n6.0\n\n\n\n6 - 2\n\n4\n\n\n\n9 / 3\n\n3.0\n\n\nRaising a base to an exponent uses ^, not **.\n\n3^2\n\n9\n\n\nJulia allows the use of updating operators to simplify updating a variable in place (in other words, using x += 5 instead of x = x + 5.\n\nBoolean algebra\nLogical operations can be used on variables of type Bool. Typical operators are && (and), || (or), and ! (not).\n\ntrue && true\n\ntrue\n\n\n\ntrue && false\n\nfalse\n\n\n\ntrue || false\n\ntrue\n\n\n\n!true\n\nfalse\n\n\nComparisons can be chained together.\n\n3 &lt; 4 || 8 == 12\n\ntrue\n\n\nWe didn’t do this above, since Julia doesn’t require it, but it’s easier to understand these types of compound expressions if you use parentheses to signal the order of operations. This helps with debugging!\n\n(3 &lt; 4) || (8 == 12)\n\ntrue",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#data-structures",
    "href": "tutorials/julia-basics.html#data-structures",
    "title": "Tutorial: Julia Basics",
    "section": "Data Structures",
    "text": "Data Structures\nData structures are containers which hold multiple values in a convenient fashion. Julia has several built-in data structures, and there are many extensions provided in additional packages.\n\nTuples\nTuples are collections of values. Julia will pay attention to the types of these values, but they can be mixed. Tuples are also immutable: their values cannot be changed once they are defined.\nTuples can be defined by just separating values with commas.\n\ntest_tuple = 4, 5, 6\n\n(4, 5, 6)\n\n\nTo access a value, use square brackets and the desired index. Note: Julia indexing starts at 1, not 0!\n\ntest_tuple[1]\n\n4\n\n\nAs mentioned above, tuples are immutable. What happens if we try to change the value of the first element of test_tuple?\n\ntest_tuple[1] = 5\n\n\nMethodError: no method matching setindex!(::Tuple{Int64, Int64, Int64}, ::Int64, ::Int64)\nThe function `setindex!` exists, but no method is defined for this combination of argument types.\nStacktrace:\n [1] top-level scope\n   @ ~/Teaching/BEE4850/spring2026/tutorials/julia-basics.qmd:226\n\n\n\nTuples also do not have to hold the same types of values.\n\ntest_tuple_2 = 4, 5., 'h'\ntypeof(test_tuple_2)\n\nTuple{Int64, Float64, Char}\n\n\nTuples can also be defined by enclosing the values in parentheses.\ntest_tuple_3 = (4, 5., 'h')\ntypeof(test_tuple_3)\n\n\nArrays\nArrays also hold multiple values, which can be accessed based on their index position. Arrays are commonly defined using square brackets.\n\ntest_array = [1, 4, 7, 8]\ntest_array[2]\n\n4\n\n\nUnlike tuples, arrays are mutable, and their contained values can be changed later.\n\ntest_array[1] = 6\ntest_array\n\n4-element Vector{Int64}:\n 6\n 4\n 7\n 8\n\n\nArrays also can hold multiple types. Unlike tuples, this causes the array to no longer care about types at all.\n\ntest_array_2 = [6, 5., 'h']\ntypeof(test_array_2)\n\n\nVector{Any} (alias for Array{Any, 1})\n\n\n\nCompare this with test_array:\n\ntypeof(test_array)\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\n\nDictionaries\nInstead of using integer indices based on position, dictionaries are indexed by keys. They are specified by passing key-value pairs to the Dict() method.\n\ntest_dict = Dict(\"A\"=&gt;1, \"B\"=&gt;2)\ntest_dict[\"B\"]\n\n2\n\n\n\n\nComprehensions\nCreating a data structure with more than a handful of elements can be tedious to do by hand. If your desired array follows a certain pattern, you can create structures using a comprehension. Comprehensions iterate over some other data structure (such as an array) implicitly and populate the new data structure based on the specified instructions.\n\n[i^2 for i in 0:1:5]\n\n6-element Vector{Int64}:\n  0\n  1\n  4\n  9\n 16\n 25\n\n\nFor dictionaries, make sure that you also specify the keys.\n\nDict(string(i) =&gt; i^2 for i in 0:1:5)\n\nDict{String, Int64} with 6 entries:\n  \"4\" =&gt; 16\n  \"1\" =&gt; 1\n  \"5\" =&gt; 25\n  \"0\" =&gt; 0\n  \"2\" =&gt; 4\n  \"3\" =&gt; 9",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#functions",
    "href": "tutorials/julia-basics.html#functions",
    "title": "Tutorial: Julia Basics",
    "section": "Functions",
    "text": "Functions\nA function is an object which accepts a tuple of arguments and maps them to a return value. In Julia, functions are defined using the following syntax.\n\nfunction my_actual_function(x, y)\n    return x + y\nend\nmy_actual_function(3, 5)\n\n8\n\n\nFunctions in Julia do not require explicit use of a return statement. They will return the last expression evaluated in their definition. However, it’s good style to explicitly return function outputs. This improves readability and debugging, especially when functions can return multiple expressions based on logical control flows (if-then-else blocks).\nFunctions in Julia are objects, and can be treated like other objects. They can be assigned to new variables or passed as arguments to other functions.\n\ng = my_actual_function\ng(3, 5)\n\n8\n\n\n\nfunction function_of_functions(f, x, y)\n    return f(x, y)\nend\nfunction_of_functions(g, 3, 5)\n\n8\n\n\n\nShort and Anonymous Functions\nIn addition to the long form of the function definition shown above, simple functions can be specified in more compact forms when helpful.\nThis is the short form:\n\nh₁(x) = x^2 # make the subscript using \\_1 + &lt;TAB&gt;\nh₁(4)\n\n16\n\n\nThis is the anonymous form:\n\nx-&gt;sin(x)\n(x-&gt;sin(x))(π/4)\n\n0.7071067811865475\n\n\n\n\nMutating Functions\nThe convention in Julia is that functions should not modify (or mutate) their input data. The reason for this is to ensure that the data is preserved. Mutating functions are mainly appropriate for applications where performance needs to be optimized, and making a copy of the input data would be too memory-intensive.\nIf you do write a mutating function in Julia, the convention is to add a ! to its name, like my_mutating_function!(x).\n\n\nOptional arguments\nThere are two extremes with regard to function parameters which do not always need to be changed. The first is to hard-code them into the function body, which has a clear downside: when you do want to change them, the function needs to be edited directly. The other extreme is to treat them as regular arguments, passing them every time the function is called. This has the downside of potentially creating bloated function calls, particularly when there is a standard default value that makes sense for most function evaluations.\nMost modern languages, including Julia, allow an alternate solution, which is to make these arguments optional. This involves setting a default value, which is used unless the argument is explicitly defined in a function call.\n\nfunction setting_optional_arguments(x, y, c=0.5)\n    return c * (x + y)\nend\n\nsetting_optional_arguments (generic function with 2 methods)\n\n\nIf we want to stick with the fixed value \\(c=0.5\\), all we have to do is call setting_optional_arguments with the x and y arguments.\n\nsetting_optional_arguments(3, 5)\n\n4.0\n\n\nOtherwise, we can pass a new value for c.\n\nsetting_optional_arguments(3, 5, 2)\n\n16\n\n\n\n\nPassing data structures as arguments\nInstead of passing variables individually, it may make sense to pass a data structure, such as an array or a tuple, and then unpacking within the function definition. This is straightforward in long form: access the appropriate elements using their index.\nIn short or anonymous form, there is a trick which allows the use of readable variables within the function definition.\n\nh₂((x,y)) = x*y # enclose the input arguments in parentheses to tell Julia to expect and unpack a tuple\n\nh₂ (generic function with 1 method)\n\n\n\nh₂((2, 3)) # this works perfectly, as we passed in a tuple\n\n6\n\n\n\nh₂(2, 3) # this gives an error, as h₂ expects a single tuple, not two different numeric values\n\n\nMethodError: no method matching h₂(::Int64, ::Int64)\nThe function `h₂` exists, but no method is defined for this combination of argument types.\n\nClosest candidates are:\n  h₂(::Any)\n   @ Main.Notebook ~/Teaching/BEE4850/spring2026/tutorials/julia-basics.qmd:372\n\nStacktrace:\n [1] top-level scope\n   @ ~/Teaching/BEE4850/spring2026/tutorials/julia-basics.qmd:380\n\n\n\n\nh₂([3, 10]) # this also works with arrays instead of tuples\n\n30\n\n\n\n\nVectorized operations\nJulia uses dot syntax to vectorize an operation and apply it element-wise across an array.\nFor example, to calculate the square root of 3:\n\nsqrt(3)\n\n1.7320508075688772\n\n\nTo calculate the square roots of every integer between 1 and 5:\n\nsqrt.([1, 2, 3, 4, 5])\n\n5-element Vector{Float64}:\n 1.0\n 1.4142135623730951\n 1.7320508075688772\n 2.0\n 2.23606797749979\n\n\nThe same dot syntax is used for arithmetic operations over arrays, since these operations are really functions.\n\n[1, 2, 3, 4] .* 2\n\n4-element Vector{Int64}:\n 2\n 4\n 6\n 8\n\n\nVectorization can be faster and is more concise to write and read than applying the same function to multiple variables or objects explicitly, so take advantage!\n\n\nReturning multiple values\nYou can return multiple values by separating them with a comma. This implicitly causes the function to return a tuple of values.\n\nfunction return_multiple_values(x, y)\n    return x + y, x * y\nend\nreturn_multiple_values(3, 5)\n\n(8, 15)\n\n\nThese values can be unpacked into multiple variables.\n\nn, ν = return_multiple_values(3, 5)\nn\n\n8\n\n\n\nν\n\n15\n\n\n\n\nReturning nothing\nSometimes you don’t want a function to return any values at all. For example, you might want a function that only prints a string to the console.\n\nfunction print_some_string(x)\n    println(\"x: $x\")\n    return nothing\nend\nprint_some_string(42)\n\nx: 42",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#printing-text-output",
    "href": "tutorials/julia-basics.html#printing-text-output",
    "title": "Tutorial: Julia Basics",
    "section": "Printing Text Output",
    "text": "Printing Text Output\nThe Text() function returns its argument as a plain text string. Notice how this is different from evaluating a string!\n\nText(\"I'm printing a string.\")\n\nI'm printing a string.\n\n\nText() is used in this tutorial as it returns the string passed to it. To print directly to the console, use println().\n\nprintln(\"I'm writing a string to the console.\")\n\nI'm writing a string to the console.\n\n\n\nPrinting Variables In a String\nWhat if we want to include the value of a variable inside of a string? We do this using string interpolation, using $variablename inside of the string.\n\nbar = 42\nText(\"Now I'm printing a variable: $bar\")\n\nNow I'm printing a variable: 42",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#control-flows",
    "href": "tutorials/julia-basics.html#control-flows",
    "title": "Tutorial: Julia Basics",
    "section": "Control Flows",
    "text": "Control Flows\nOne of the tricky things about learning a new programming language can be getting used to the specifics of control flow syntax. These types of flows include conditional if-then-else statements or loops.\n\nConditional Blocks\nConditional blocks allow different pieces of code to be evaluated depending on the value of a boolean expression or variable. For example, if we wanted to compute the absolute value of a number, rather than using abs():\n\nfunction our_abs(x)\n    if x &gt;= 0\n        return x\n    else\n        return -x\n    end\nend\n\nour_abs (generic function with 1 method)\n\n\n\nour_abs(4)\n\n4\n\n\n\nour_abs(-4)\n\n4\n\n\nTo nest conditional statements, use elseif.\n\nfunction test_sign(x)\n    if x &gt; 0\n        return Text(\"x is positive.\")\n    elseif x &lt; 0\n        return Text(\"x is negative.\")\n    else\n        return Text(\"x is zero.\")\n    end\nend\n\ntest_sign (generic function with 1 method)\n\n\n\ntest_sign(-5)\n\nx is negative.\n\n\n\ntest_sign(0)\n\nx is zero.\n\n\n\n\nLoops\nLoops allow expressions to be evaluated repeatedly until they are terminated. The two main types of loops are while loops and for loops.\n\nWhile loops\nwhile loops continue to evaluate an expression so long as a specified boolean condition is true. This is useful when you don’t know how many iterations it will take for the desired goal to be reached.\n\nfunction compute_factorial(x)\n    factorial = 1\n    while (x &gt; 1)\n        factorial *= x\n        x -= 1\n    end\n    return factorial\nend\ncompute_factorial(5)\n\n120\n\n\n\nWhile loops can easily turn into infinite loops if the condition is never meaningfully updated. Be careful, and look there if your programs are getting stuck. Also, If the expression in a while loop is false when the loop is reached, the loop will never be evaluated.\n\n\n\nFor loops\nfor loops run for a finite number of iterations, based on some defined index variable.\n\nfunction add_some_numbers(x)\n    total_sum = 0 # initialize at zero since we're adding\n    for i=1:x # the counter i is updated every iteration\n        total_sum += i\n    end\n    return total_sum\nend\nadd_some_numbers(4)\n\n10\n\n\nfor loops can also iterate over explicitly passed containers, rather than iterating over an incrementally-updated index sequence. Use the in keyword when defining the loop.\n\nfunction add_passed_numbers(set)\n    total_sum = 0\n    for i in set # this is the syntax we use when we want i to correspond to different container values\n        total_sum += i\n    end\n    return total_sum\nend\nadd_passed_numbers([1, 3, 5])\n\n9",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#linear-algebra",
    "href": "tutorials/julia-basics.html#linear-algebra",
    "title": "Tutorial: Julia Basics",
    "section": "Linear algebra",
    "text": "Linear algebra\nMatrices are defined in Julia as 2d arrays. Unlike basic arrays, matrices need to contain the same data type so Julia knows what operations are allowed. When defining a matrix, use semicolons to separate rows. Row elements should not be separated by commas.\n\ntest_matrix = [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nYou can also specify matrices using spaces and newlines.\n\ntest_matrix_2 = [1 2 3\n                 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nFinally, matrices can be created using comprehensions by separating the inputs by a comma.\n\n[i*j for i in 1:1:5, j in 1:1:5]\n\n5×5 Matrix{Int64}:\n 1   2   3   4   5\n 2   4   6   8  10\n 3   6   9  12  15\n 4   8  12  16  20\n 5  10  15  20  25\n\n\nVectors are treated as 1d matrices.\n\ntest_row_vector = [1 2 3]\n\n1×3 Matrix{Int64}:\n 1  2  3\n\n\n\ntest_col_vector = [1; 2; 3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\nMany linear algebra operations on vectors and matrices can be loaded using the LinearAlgebra package.",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#package-management",
    "href": "tutorials/julia-basics.html#package-management",
    "title": "Tutorial: Julia Basics",
    "section": "Package management",
    "text": "Package management\nSometimes you might need functionality that does not exist in base Julia. Julia handles packages using the Pkg package manager. After finding a package which has the functions that you need, you have two options: 1. Use the package management prompt in the Julia REPL (the standard Julia interface; what you get when you type julia in your terminal). Enter this by typing ] at the standard green Julia prompt julia&gt;. This will become a blue pkg&gt;. You can then download and install new packages using add packagename. 2. From the standard prompt, enter using Pkg; Pkg.add(packagename). The packagename package can then be used by adding using packagename to the start of the script.",
    "crumbs": [
      "Tutorial: Julia Basics"
    ]
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Term Project",
    "section": "",
    "text": "The term project gives you an opportunity to use and extend the methods we have learned in class to an environmental data set and/or model of your choosing. More details will be provided over the semester. We will discuss each component in class as well as providing relevant information on this page.",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "project/index.html#overview",
    "href": "project/index.html#overview",
    "title": "Term Project",
    "section": "",
    "text": "The term project gives you an opportunity to use and extend the methods we have learned in class to an environmental data set and/or model of your choosing. More details will be provided over the semester. We will discuss each component in class as well as providing relevant information on this page.",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "project/index.html#instructions",
    "href": "project/index.html#instructions",
    "title": "Term Project",
    "section": "Instructions",
    "text": "Instructions\n\nStudents can work individually or in groups of 2.\nIf the project is being done in a group, the proposal should also include a plan for the allocation of duties to each group member (not included in the 2 page limit).\nThe final project report should be no more than 5 pages (1-inch margins, 11 point Calibri font or equivalent), not including figures and references, but can include an appendix with additional details on methods.",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "project/index.html#schedule",
    "href": "project/index.html#schedule",
    "title": "Term Project",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nMilestone\nDue Date\n\n\n\n\nProposal and Exploratory Analysis\nFri, Feb 11\n\n\nCandidate Models\nFri, Mar 13\n\n\nSimulation Study\nFri, Apr 10\n\n\nFinal Report\nFri, May 15",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "project/index.html#milestones",
    "href": "project/index.html#milestones",
    "title": "Term Project",
    "section": "Milestones",
    "text": "Milestones\nEach of the intermediate reports should be no more than 3 pages long, not including figures or references.\n\nThe proposal should state the core science question at the center of the project and any relevant hypotheses that will be tested and discuss why the data set is relevant for addressing that question. It should also include visual and quantitative exploratory analyses to identify key features of the data.\nThe candidate models report should finalize the scientific hypotheses for the question and translate them into statistical models which will be used for the analysis. It should include any diagnostics used to specify the probability model(s), the results of fitting the model(s), and any associated analyses to assess the appropriateness of the model specification(s).\nThe simulation study should include the results of any Monte Carlo and/or bootstrap analyses as relevant for the models and science question.\n\nThe final report should synthesize all of these findings and include any relevant model comparisons or hypothesis tests as well as any further analyses conducted in support of addressing the research question.",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#complete-case-analysis",
    "href": "slides/lecture13-1-missing-example.html#complete-case-analysis",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Complete-Case Analysis",
    "text": "Complete-Case Analysis\nCommon approach to missing data:\nComplete-case Analysis: Only consider data for which all variables are available.\n\nCan result in bias if missing values have a systematic pattern.\nCould result in discarding a large amount of data."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#missingness-complete-at-random-mcar",
    "href": "slides/lecture13-1-missing-example.html#missingness-complete-at-random-mcar",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Missingness Complete At Random (MCAR)",
    "text": "Missingness Complete At Random (MCAR)\n\n\nMCAR: \\(M_Y\\) is independent of \\(X=x\\) and \\(Y=y\\).\nComplete cases are fully representative of the complete data:\n\\[\\mathbb{P}(Y=y) = P(Y=y | M_Y=0)\\]\n\n\n\nCode\nn = 50\nx = rand(Uniform(0, 100), n)\nlogit(x) = log(x / (1 - x))\ninvlogit(x) = exp(x) / (1 + exp(x))\nf(x) = invlogit(0.05 * (x - 50) + rand(Normal(0, 1)))\ny = f.(x)\n\nflin(x) = 0.25 * x + 2 + rand(Normal(0, 7))\ny = flin.(x)\nxpred = collect(0:0.1:100)\n\nlm_all = lm([ones(length(x)) x], y)\ny_lm_all = predict(lm_all, [ones(length(xpred)) xpred])\n\nmissing_y = Bool.(rand(Binomial.(1, 0.25), length(y)))\nxobs = x[.!(missing_y)]\nyobs = y[.!(missing_y)]\nlm_mcar = lm([ones(n - sum(missing_y)) xobs], yobs)\ny_lm_mcar = predict(lm_mcar, [ones(length(xpred)) xpred])\n\n\np1 = scatter(xobs, yobs, xlabel=L\"$x$\", ylabel=L\"$y$\", label=false, markersize=5, size=(600, 500), color=:blue)\nscatter!(x[missing_y], y[missing_y], alpha=0.9, color=:lightgrey, label=false, markersize=5)\nplot!(xpred, y_lm_all, color=:red, lw=3, label=\"Complete-Data Inference\", ribbon=GLM.dispersion(lm_all), fillalpha=0.2)\nplot!(xpred, y_lm_mcar, color=:blue, lw=3, linestyle=:dot, label=\"Observed-Data Inference\", ribbon=GLM.dispersion(lm_mcar), fillalpha=0.2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Illustration of MCAR Data"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#missingness-at-random-mar",
    "href": "slides/lecture13-1-missing-example.html#missingness-at-random-mar",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Missingness At Random (MAR)",
    "text": "Missingness At Random (MAR)\n\n\nMAR: \\(M_Y\\) is independent of \\(Y=y\\) conditional on \\(X=x\\).\nAlso called ignorable or uninformative missingness.\n\\[\n\\begin{align*}\n\\mathbb{P}&(Y=y | X=x) \\\\\n&= \\mathbb{P}(Y=y | X=x, M_Y=0)\n\\end{align*}\n\\]\n\n\n\nCode\nmissing_y = Bool.(rand.(Binomial.(1,  invlogit.(0.1 * (x .- 75)))))\nxobs = x[.!(missing_y)]\nyobs = y[.!(missing_y)]\nlm_mar = lm([ones(n - sum(missing_y)) xobs], yobs)\ny_lm_mar = predict(lm_mar, [ones(length(xpred)) xpred])\n\np2 = scatter(xobs, yobs, xlabel=L\"$x$\", ylabel=L\"$y$\", label=false, markersize=5, size=(600, 500), color=:blue)\nscatter!(x[missing_y], y[missing_y], alpha=0.9, color=:lightgrey, label=false, markersize=5)\nplot!(xpred, y_lm_all, color=:red, lw=3, label=\"Complete-Data Inference\", ribbon=GLM.dispersion(lm_all), fillalpha=0.2)\nplot!(xpred, y_lm_mar, color=:blue, lw=3, linestyle=:dot, label=\"Observed-Data Inference\", ribbon=GLM.dispersion(lm_mar), fillalpha=0.2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of MAR Data"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#missingness-not-at-random-mnar",
    "href": "slides/lecture13-1-missing-example.html#missingness-not-at-random-mnar",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Missingness Not-At-Random (MNAR)",
    "text": "Missingness Not-At-Random (MNAR)\n\n\nMNAR: \\(M_Y\\) is dependent on \\(Y=y\\) (and/or unmodeled variables).\nAlso called non-ignorable or informative missingness.\n\\[\n\\begin{align*}\n\\mathbb{P}&(Y=y | X=x) \\\\\n&\\neq \\mathbb{P}(Y=y | X=x, M_Y=0)\n\\end{align*}\n\\]\n\n\n\nCode\nmissing_y = Bool.(rand.(Binomial.(1,  invlogit.(0.9 * (y .- 15)))))\nxobs = x[.!(missing_y)]\nyobs = y[.!(missing_y)]\nlm_mnar = lm([ones(n - sum(missing_y)) xobs], yobs)\ny_lm_mnar = predict(lm_mnar, [ones(length(xpred)) xpred])\n\np2 = scatter(xobs, yobs, xlabel=L\"$x$\", ylabel=L\"$y$\", label=false, markersize=5, size=(600, 500), color=:blue)\nscatter!(x[missing_y], y[missing_y], alpha=0.9, color=:lightgrey, label=false, markersize=5)\nplot!(xpred, y_lm_all, color=:red, lw=3, label=\"Complete-Data Inference\", ribbon=GLM.dispersion(lm_all), fillalpha=0.2)\nplot!(xpred, y_lm_mnar, color=:blue, lw=3, linestyle=:dot, label=\"Observed-Data Inference\", ribbon=GLM.dispersion(lm_mnar), fillalpha=0.2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Illustration of MCAR Data"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#implications-of-missingness-mechanism",
    "href": "slides/lecture13-1-missing-example.html#implications-of-missingness-mechanism",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Implications of Missingness Mechanism",
    "text": "Implications of Missingness Mechanism\n\nMCAR: Strong, but generally implausible. Can only use complete cases as observed data is fully representative.\nMAR: More plausible than MCAR, can still justify complete-case analysis as conditional observed distributions are unbiased estimates of conditional complete distributions.\nMNAR: Deletion is a bad idea. The observed data does not follow the same conditional distribution. Missingness can be informative: try to model the missingness mechanism."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#methods-for-dealing-with-missing-data",
    "href": "slides/lecture13-1-missing-example.html#methods-for-dealing-with-missing-data",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Methods for Dealing with Missing Data",
    "text": "Methods for Dealing with Missing Data\n\nImputation: substitute values for missing data before analysis;\nAveraging: find expected values over all possible values of the missing variables."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#example-quality-data",
    "href": "slides/lecture13-1-missing-example.html#example-quality-data",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Example Quality Data",
    "text": "Example Quality Data\n\n\nCode\ndat = CSV.read(\"data/airquality/airquality.csv\", DataFrame)\nrename!(dat, :\"Solar.R\" =&gt; :Solar)\ndat.Miss_Ozone = ismissing.(dat.Ozone)\ndat.Miss_Solar = ismissing.(dat.Solar)\ndat[2:5, 1:7]\n\n\n\n\n4×7 DataFrame\n\n\n\nRow\nrownames\nOzone\nSolar\nWind\nTemp\nMonth\nDay\n\n\n\nInt64\nInt64?\nInt64?\nFloat64\nInt64\nInt64\nInt64\n\n\n\n\n1\n2\n36\n118\n8.0\n72\n5\n2\n\n\n2\n3\n12\n149\n12.6\n74\n5\n3\n\n\n3\n4\n18\n313\n11.5\n62\n5\n4\n\n\n4\n5\nmissing\nmissing\n14.3\n56\n5\n5\n\n\n\n\n\n\nFigure 4: Air quality dataset."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#assessing-missingness-ozone",
    "href": "slides/lecture13-1-missing-example.html#assessing-missingness-ozone",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Assessing Missingness (Ozone)",
    "text": "Assessing Missingness (Ozone)\n\nCode\ndat_ozone_complete = filter(:Ozone =&gt; x -&gt; !ismissing(x), dat)\ndat_ozone_missing = filter(:Ozone =&gt; x -&gt; ismissing(x), dat)\n\np1 = scatter(dat_ozone_complete.Temp, dat_ozone_complete.Wind, xlabel = \"Temperature (°C)\", ylabel=\"Wind (mph)\", markersize=5, color=:blue, label=\"Not Missing\")\nscatter!(dat_ozone_missing.Temp, dat_ozone_missing.Wind, markersize=5, color=:orange, label=\"Missing\")\nplot!(size=(600, 450))\n\np2 = scatter(dat_ozone_complete.Month, dat_ozone_complete.Day, xlabel = \"Month\", ylabel=\"Day\", markersize=5, color=:blue, label=\"Not Missing\")\nscatter!(dat_ozone_missing.Month, dat_ozone_missing.Day, markersize=5, color=:orange, label=\"Missing\")\nplot!(size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Air quality dataset.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#assessing-missingness-solar",
    "href": "slides/lecture13-1-missing-example.html#assessing-missingness-solar",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Assessing Missingness (Solar)",
    "text": "Assessing Missingness (Solar)\n\nCode\ndat_solar_complete = filter(:Solar =&gt; x -&gt; !ismissing(x), dat)\ndat_solar_missing = filter(:Solar =&gt; x -&gt; ismissing(x), dat)\n\np1 = scatter(dat_solar_complete.Temp, dat_solar_complete.Wind, xlabel = \"Temperature (°C)\", ylabel=\"Wind (mph)\", markersize=5, color=:blue, label=\"Not Missing\")\nscatter!(dat_solar_missing.Temp, dat_solar_missing.Wind, markersize=5, color=:orange, label=\"Missing\")\nplot!(size=(600, 450))\n\np2 = scatter(dat_solar_complete.Month, dat_solar_complete.Day, xlabel = \"Month\", ylabel=\"Day\", markersize=5, color=:blue, label=\"Not Missing\")\nscatter!(dat_solar_missing.Month, dat_solar_missing.Day, markersize=5, color=:orange, label=\"Missing\")\nplot!(size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Air quality dataset.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 6"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#prediction-with-noise",
    "href": "slides/lecture13-1-missing-example.html#prediction-with-noise",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Prediction with Noise",
    "text": "Prediction with Noise\n\nObtain bootstrap replicate of each imputation regression model.\nImpute by simulating from predictive distribution (including noise!).\nFit target regression model to imputed dataset.\nRepeat for number of imputations."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#airquality-imputation-prediction",
    "href": "slides/lecture13-1-missing-example.html#airquality-imputation-prediction",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Airquality Imputation (Prediction)",
    "text": "Airquality Imputation (Prediction)\nImpute using available data:\n\\[\n\\begin{align*}\n\\text{Ozone} &\\sim f(\\text{Wind}, \\text{Temp}, \\text{Month}, \\text{Day}) \\\\\n\\text{Solar.R} &\\sim g(\\text{Wind}, \\text{Temp}, \\text{Month}, \\text{Day}) \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#candidate-model-based-imputations",
    "href": "slides/lecture13-1-missing-example.html#candidate-model-based-imputations",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Candidate Model-Based Imputations",
    "text": "Candidate Model-Based Imputations\n\nCode\n# bootstrap linear models and get predictions\nnboot = 100\n## model for solar radiation\nfunction impute_bootstrap_model(dat_complete, dat_missing, model_formula)\n    idx = sample(1:nrow(dat_complete), nrow(dat_complete), replace=true)\n    dat_boot = dat_complete[idx, :]\n    mod = lm(model_formula, dat_boot)\n    return mod\nend\n\nfunction impute_predict_regression(dat_complete, dat_missing, nboot, model_formula)\n    impute_out = zeros(nrow(dat_missing), nboot)\n    for i = 1:nboot\n        mod = impute_bootstrap_model(dat_complete, dat_missing, model_formula)\n        impute_out[:, i] = predict(mod, dat_missing) .+ rand(Normal(0, GLM.dispersion(mod.model)), size(impute_out)[1])\n    end\n    return impute_out\nend\n\nimpute_solar = impute_predict_regression(dat_solar_complete, dat_solar_missing, nboot, @formula(Solar ~ Wind + Temp + Month + Day))\n\n## model for ozone\nimpute_ozone = impute_predict_regression(dat_ozone_complete, dat_ozone_missing, nboot, @formula(Ozone ~ Wind + Temp + Month + Day))\n\n# impute values into the complete-case dataset and plot\nfunction impute_variables(dat, impute_ozone, impute_solar)\n    impute = deepcopy(dat)\n    impute[ismissing.(impute.Ozone), :Ozone] = round.(impute_ozone[:, 1]; digits=0)\n    impute[ismissing.(impute.Solar), :Solar] = round.(impute_solar[:, 1]; digits=0)\n    return impute\nend\nimpute1 = impute_variables(dat, impute_ozone[:, 1], impute_solar[:, 1])\np1 = scatter(impute1.Solar[.!(impute1.Miss_Ozone) .& .!(impute1.Miss_Solar)], impute1.Ozone[.!(impute1.Miss_Ozone) .& .!(impute1.Miss_Solar)], color=:blue, markersize=5, xlabel=L\"Solar Radiation (W/m$^2$)\", ylabel=\"Ozone (ppb)\", label=\"Observed\")\nscatter!(impute1.Solar[impute1.Miss_Ozone .| impute1.Miss_Solar], impute1.Ozone[impute1.Miss_Ozone .| impute1.Miss_Solar], color=:orange, markersize=5, label=\"Imputed\")\nplot!(size=(600, 450))\n\nimpute2 = impute_variables(dat, impute_ozone[:, 2], impute_solar[:, 2])\np2 = scatter(impute2.Solar[.!(impute2.Miss_Ozone) .& .!(impute2.Miss_Solar)], impute2.Ozone[.!(impute2.Miss_Ozone) .& .!(impute2.Miss_Solar)], color=:blue, markersize=5, xlabel=L\"Solar Radiation (W/m$^2$)\", ylabel=\"Ozone (ppb)\", label=\"Observed\")\nscatter!(impute2.Solar[impute2.Miss_Ozone .| impute2.Miss_Solar], impute2.Ozone[impute2.Miss_Ozone .| impute2.Miss_Solar], color=:orange, markersize=5, label=\"Imputed\")\nplot!(size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Air quality dataset.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 7"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#model-imputed-time-series-ozone",
    "href": "slides/lecture13-1-missing-example.html#model-imputed-time-series-ozone",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Model Imputed Time Series (Ozone)",
    "text": "Model Imputed Time Series (Ozone)\n\n\nCode\np1 = plot(dat.rownames, dat.Ozone, lw=3, color=:blue, label=\"Observations\", xlabel=\"Day Number\", ylabel=\"Ozone (ppb)\")\nfor i = 1:nrow(dat_ozone_missing)\n    label = i == 1 ? \"Imputations\" : false\n    boxplot!(p1, [dat_ozone_missing[i, :rownames]], impute_ozone[i, :], color=:orange, label=label)\nend\nplot!(size=(1200, 500))\n\ndisplay(p1)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Air quality dataset."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#model-imputed-time-series-solar",
    "href": "slides/lecture13-1-missing-example.html#model-imputed-time-series-solar",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Model Imputed Time Series (Solar)",
    "text": "Model Imputed Time Series (Solar)\n\n\nCode\np2 = plot(dat.rownames, dat.Solar, lw=3, color=:blue, label=\"Observations\", xlabel=\"Day Number\", ylabel=L\"Solar (W/m$^2$)\")\nfor i = 1:nrow(dat_solar_missing)\n    label = i == 1 ? \"Imputations\" : false\n    boxplot!(p2, [dat_solar_missing[i, :rownames]], impute_solar[i, :], color=:orange, label=label)\nend\nplot!(size=(1200, 500))\n\ndisplay(p2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Air quality dataset."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#model-imputed-regression-coefficients",
    "href": "slides/lecture13-1-missing-example.html#model-imputed-regression-coefficients",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Model-Imputed Regression Coefficients",
    "text": "Model-Imputed Regression Coefficients\n\n\n\n\nCode\nβ_boot = zeros(nboot)\nσ_boot = zeros(nboot)\nfor i = 1:nboot\n    dat_boot = impute_variables(dat, impute_ozone[:, i], impute_solar[:, i])\n    model_boot = lm(@formula(Ozone ~ Solar), dat_boot)\n    β_boot[i] = coef(model_boot)[2]\n    σ_boot[i] = dispersion(model_boot.model)\nend\nβ_est = mean(β_boot)\nσ_est = sqrt(mean(σ_boot.^2) + (1 + 1/nboot) * var(β_boot))\n\n# also get complete case estimates for later\ncc_model = lm(@formula(Ozone ~ Solar), dropmissing(dat))\nβ_cc = coef(cc_model)[2]\nσ_cc = dispersion(cc_model.model)\nhistogram(β_boot, xlabel=L\"$\\beta$\", ylabel=\"Count\", label=false)\nvline!([β_cc], color=:red, label=\"Complete-Case\")\nplot!(size=(500, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Air quality dataset.\n\n\n\n\n\nImputed:\n\n\\(\\hat{\\beta} = 0\\.1\\)\n\\(\\hat{\\sigma} = 31\\.0\\)\n\n\nComplete Case:\n\n\\(\\hat{\\beta} = 0\\.13\\)\n\\(\\hat{\\sigma} = 31\\.0\\)"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#predictive-mean-matching",
    "href": "slides/lecture13-1-missing-example.html#predictive-mean-matching",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Predictive Mean Matching",
    "text": "Predictive Mean Matching\n\nObtain bootstrap replicate of each imputation regression model.\nGet predicted value of missing value \\(\\hat{y}_j\\).\nGenerate candidates by finding \\(k\\) nearest complete cases (minimize \\(|y - \\hat{y}_j|\\)) or use threshold \\(\\eta\\).\nSample from candidates to get imputed value.\nFit target regression model to imputed dataset.\nRepeat for number of imputations."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#predictive-mean-matching-imputations",
    "href": "slides/lecture13-1-missing-example.html#predictive-mean-matching-imputations",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Predictive Mean Matching Imputations",
    "text": "Predictive Mean Matching Imputations\n\nCode\n# bootstrap linear models and get predictions\nnboot = 100 # number of bootstrap samples for parameter variabiity\nk = 5 # number of nearest-neighbors to sample from\n\n## model for solar radiation\nfunction impute_pmm(dat_complete, dat_missing, nboot, nneighbors, model_formula, target_name)\n    impute = zeros(nrow(dat_missing), nboot)\n    candidates = zeros(nrow(dat_missing), nboot, nneighbors)\n    for i = 1:nboot\n        mod = impute_bootstrap_model(dat_complete, dat_missing, model_formula)\n        ŷ = predict(mod, dat_missing) # get predicted value for missing data\n        y = predict(mod, dat_complete)\n        for j = 1:nrow(dat_missing)\n            d = abs.(y .- ŷ[j])\n            sort_idx = sortperm(d)\n            candidates[j, i, 1:nneighbors] = dat_complete[sort_idx[1:nneighbors], target_name]\n            impute[j, i] = sample(candidates[j, i, :])\n        end\n    end\n    return impute\nend\n\nimpute_ozone_pmm = impute_pmm(dat_ozone_complete, dat_ozone_missing, 100, 5, @formula(Ozone ~ Wind + Temp + Month + Day), Symbol(\"Ozone\"))\nimpute_solar_pmm = impute_pmm(dat_solar_complete, dat_solar_missing, 100, 5, @formula(Solar ~ Wind + Temp + Month + Day), Symbol(\"Solar\"))\n\n\nfunction impute_variables(dat, impute_ozone, impute_solar)\n    impute = deepcopy(dat)\n    impute[ismissing.(impute.Ozone), :Ozone] = round.(impute_ozone[:, 1]; digits=0)\n    impute[ismissing.(impute.Solar), :Solar] = round.(impute_solar[:, 1]; digits=0)\n    return impute\nend\nimpute1 = impute_variables(dat, impute_ozone_pmm[:, 1], impute_solar_pmm[:, 1])\np1 = scatter(impute1.Solar[.!(impute1.Miss_Ozone) .& .!(impute1.Miss_Solar)], impute1.Ozone[.!(impute1.Miss_Ozone) .& .!(impute1.Miss_Solar)], color=:blue, markersize=5, xlabel=L\"Solar Radiation (W/m$^2$)\", ylabel=\"Ozone (ppb)\", label=\"Observed\")\nscatter!(impute1.Solar[impute1.Miss_Ozone .| impute1.Miss_Solar], impute1.Ozone[impute1.Miss_Ozone .| impute1.Miss_Solar], color=:orange, markersize=5, label=\"Imputed\")\nplot!(size=(600, 450))\n\nimpute2 = impute_variables(dat, impute_ozone_pmm[:, 2], impute_solar_pmm[:, 2])\np2 = scatter(impute2.Solar[.!(impute2.Miss_Ozone) .& .!(impute2.Miss_Solar)], impute2.Ozone[.!(impute2.Miss_Ozone) .& .!(impute2.Miss_Solar)], color=:blue, markersize=5, xlabel=L\"Solar Radiation (W/m$^2$)\", ylabel=\"Ozone (ppb)\", label=\"Observed\")\nscatter!(impute2.Solar[impute2.Miss_Ozone .| impute2.Miss_Solar], impute2.Ozone[impute2.Miss_Ozone .| impute2.Miss_Solar], color=:orange, markersize=5, label=\"Imputed\")\nplot!(size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Air quality dataset.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 11"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#pmm-imputed-time-series-ozone",
    "href": "slides/lecture13-1-missing-example.html#pmm-imputed-time-series-ozone",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "PMM Imputed Time Series (Ozone)",
    "text": "PMM Imputed Time Series (Ozone)\n\n\nCode\np1mm = plot(dat.rownames, dat.Ozone, lw=3, color=:blue, label=\"Observations\", xlabel=\"Day Number\", ylabel=\"Ozone (ppb)\")\nfor i = 1:nrow(dat_ozone_missing)\n    label = i == 1 ? \"Imputations\" : false\n    boxplot!(p1mm, [dat_ozone_missing[i, :rownames]], impute_ozone_pmm[i, :], color=:orange, label=label)\nend\nplot!(size=(1200, 500))\n\ndisplay(p1mm)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: Air quality dataset."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#pmm-imputed-time-series-solar",
    "href": "slides/lecture13-1-missing-example.html#pmm-imputed-time-series-solar",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "PMM Imputed Time Series (Solar)",
    "text": "PMM Imputed Time Series (Solar)\n\n\nCode\np2mm = plot(dat.rownames, dat.Solar, lw=3, color=:blue, label=\"Observations\", xlabel=\"Day Number\", ylabel=L\"Solar (W/m$^2$)\")\nfor i = 1:nrow(dat_solar_missing)\n    label = i == 1 ? \"Imputations\" : false\n    boxplot!(p2mm, [dat_solar_missing[i, :rownames]], impute_solar_pmm[i, :], color=:orange, label=label)\nend\nplot!(size=(1200, 500))\n\ndisplay(p2mm)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Air quality dataset."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#pmm-imputed-regression-coefficients",
    "href": "slides/lecture13-1-missing-example.html#pmm-imputed-regression-coefficients",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "PMM-Imputed Regression Coefficients",
    "text": "PMM-Imputed Regression Coefficients\n\n\n\n\nCode\nβ_boot = zeros(nboot)\nσ_boot = zeros(nboot)\nfor i = 1:nboot\n    dat_boot = impute_variables(dat, impute_ozone[:, i], impute_solar[:, i])\n    model_boot = lm(@formula(Ozone ~ Solar), dat_boot)\n    β_boot[i] = coef(model_boot)[2]\n    σ_boot[i] = dispersion(model_boot.model)\nend\nβ_est = mean(β_boot)\nσ_est = sqrt(mean(σ_boot.^2) + (1 + 1/nboot) * var(β_boot))\n\n# also get complete case estimates for later\ncc_model = lm(@formula(Ozone ~ Solar), dropmissing(dat))\nβ_cc = coef(cc_model)[2]\nσ_cc = dispersion(cc_model.model)\nhistogram(β_boot, xlabel=L\"$\\beta$\", ylabel=\"Count\", label=false)\nvline!([β_cc], color=:red, label=\"Complete-Case\")\nplot!(size=(500, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: Air quality dataset.\n\n\n\n\n\nImputed:\n\n\\(\\hat{\\beta} = 0\\.1\\)\n\\(\\hat{\\sigma} = 31\\.0\\)\n\n\nComplete Case:\n\n\\(\\hat{\\beta} = 0\\.13\\)\n\\(\\hat{\\sigma} = 31\\.0\\)"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#comparison-of-imputations-solar",
    "href": "slides/lecture13-1-missing-example.html#comparison-of-imputations-solar",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Comparison of Imputations (Solar)",
    "text": "Comparison of Imputations (Solar)\n\nCode\nimpute_model_df = DataFrame(impute_solar', :auto)\nimpute_model_df_stk = stack(impute_model_df)\nimpute_model_df_stk.Method .= \"Prediction\"\n\nimpute_pmm_df = DataFrame(impute_solar_pmm', :auto)\nimpute_pmm_df_stk = stack(impute_pmm_df)\nimpute_pmm_df_stk.Method .= \"PMM\"\n\nimpute_all_df = vcat(impute_model_df_stk, impute_pmm_df_stk)\n\n@df impute_all_df groupedboxplot(:variable, :value, group=:Method, xlabel=\"Imputed Case\", ylabel=L\"Solar Radiation (W/m$^2$)\")\nplot!(size=(1300, 500))\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: Air quality dataset."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#comparison-of-imputations-ozone",
    "href": "slides/lecture13-1-missing-example.html#comparison-of-imputations-ozone",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Comparison of Imputations (Ozone)",
    "text": "Comparison of Imputations (Ozone)\n\nCode\nimpute_model_df = DataFrame(impute_ozone', :auto)\nimpute_model_df_stk = stack(impute_model_df)\nimpute_model_df_stk.Method .= \"Prediction\"\n\nimpute_pmm_df = DataFrame(impute_ozone_pmm', :auto)\nimpute_pmm_df_stk = stack(impute_pmm_df)\nimpute_pmm_df_stk.Method .= \"PMM\"\n\nimpute_all_df = vcat(impute_model_df_stk, impute_pmm_df_stk)\n\n@df impute_all_df groupedboxplot(:variable, :value, group=:Method, xlabel=\"Imputed Case\", ylabel=\"Ozone (ppb)\")\nplot!(size=(1300, 500))\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16: Air quality dataset."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#key-points-1",
    "href": "slides/lecture13-1-missing-example.html#key-points-1",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Key Points",
    "text": "Key Points\n\nUse as much information as possible when conducting multiple imputation.\nIncorporate as much uncertainty as possible to avoid biasing downstream results: we don’t know what the missing data looks like!"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#inference-and-description-are-linked",
    "href": "slides/lecture13-1-missing-example.html#inference-and-description-are-linked",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Inference and Description Are Linked",
    "text": "Inference and Description Are Linked\n\n\n\nKnowing what is important to describe requires a model of data generation;\nDoing meaningful inference requires a model of data generation.\n\n\n\n\n\nSpidermen Meme"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#themes-of-this-class",
    "href": "slides/lecture13-1-missing-example.html#themes-of-this-class",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Themes of This Class",
    "text": "Themes of This Class\n\n\n\nProbability theory helps us deduce logical implications of theories conditional on our assumptions\nCannot use an “objective” procedure to avoid subjective responsibility\nVaguely motivated procedures give vague or misleading results\n\n\n\n\n\n\nBart Statistics Meme"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#data-generation-approximates-reality",
    "href": "slides/lecture13-1-missing-example.html#data-generation-approximates-reality",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Data Generation Approximates Reality",
    "text": "Data Generation Approximates Reality\n\n\n\n\n\nEstimand Estimator Cake\n\n\n\n\n\n\n\nEstimand Estimator Cake\n\n\n\n\n\n\n\n\nEstimate Cake\n\n\n\n\n\nSource: Richard McElreath"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#class-review-1",
    "href": "slides/lecture13-1-missing-example.html#class-review-1",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Class Review",
    "text": "Class Review\n\n\n\n\n\ntimeline\n      Introduction: Overview\n                  : Hypothesis Testing and Scientific Inference\n      Probability Fundamentals: Prob/Stats \"Review\"\n                              : Modeling Data-Generating Processes\n                              : Bayesian Statistics\n                              : Model-Data Discrepancy\n                              : Autocorrelated Residuals\n      Simulation Methods: Monte Carlo\n                        : Bootstrap\n                        : MCMC\n      Model Evaluation: Cross-Validation\n                      : Model Selection\n      Useful Extras: Extreme Values\n                    : Missing Data"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#workflow-covered-in-class",
    "href": "slides/lecture13-1-missing-example.html#workflow-covered-in-class",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Workflow Covered In Class",
    "text": "Workflow Covered In Class\n\n\n\nExploratory Analysis\nDevelop candidate model(s) and calibrate.\nSimulate from models to assess implications.\nCompare evidence for models with scoring rules/information criteria.\n\n\n\n\n\n\nRock Paper Scissors meme"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#what-might-come-next",
    "href": "slides/lecture13-1-missing-example.html#what-might-come-next",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "What Might Come Next",
    "text": "What Might Come Next\n\nMore advanced probability models (e.g. mixtures)\nSpatial models\nExperimental design (confounds)\n(Probabilistic) Machine learning"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#upcoming-schedule",
    "href": "slides/lecture13-1-missing-example.html#upcoming-schedule",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Upcoming Schedule",
    "text": "Upcoming Schedule\nWednesday: No Class\nNext Week + 5/5: Project Presentations"
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#assessments",
    "href": "slides/lecture13-1-missing-example.html#assessments",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "Assessments",
    "text": "Assessments\nHW5: due 5/2.\nLiterature Critique: Due 5/2."
  },
  {
    "objectID": "slides/lecture13-1-missing-example.html#references",
    "href": "slides/lecture13-1-missing-example.html#references",
    "title": "Multiple Imputation and Class Wrap-Up",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#last-class",
    "href": "slides/lecture04-1-diffeqmodels.html#last-class",
    "title": "Modeling Dynamical Systems",
    "section": "Last Class",
    "text": "Last Class\n\nGeneralized probability models (focused on generalized linear models)\nGoal: predict parameters/expectations of distributions (e.g. expectation of Poisson or probability of Binomial).\nOften requires use of a link function to convert linear regressions of predictors to appropriate parameter range."
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#one-note",
    "href": "slides/lecture04-1-diffeqmodels.html#one-note",
    "title": "Modeling Dynamical Systems",
    "section": "One Note",
    "text": "One Note\n\nMay need to standardize predictors when they are much larger than the response \\[\\hat{x} = \\frac{x - \\mu(x)}{\\sigma(x)}\\]\nThis doesn’t really matter for linear regression, but for non-linear models (GLMs or ML) can be important to detect the relevant effects."
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#historical-warming",
    "href": "slides/lecture04-1-diffeqmodels.html#historical-warming",
    "title": "Modeling Dynamical Systems",
    "section": "Historical Warming",
    "text": "Historical Warming\n\n\n\n\nCode\ntemps = CSV.read(\"data/climate/HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv\", DataFrame, delim=\",\")\n\ntime_obs = temps[:, 1]\ntemp_obs = temps[:, 2]\ntemp_lo = temps[:, 3]\ntemp_hi = temps[:, 4]\n\ntemp_lo = temp_lo .- mean(temp_obs[1:20])\ntemp_hi = temp_hi .- mean(temp_obs[1:20])\ntemp_obs = temp_obs .- mean(temp_obs[1:20]) # compute anomalies relative to first 20 years of data\ntemp_sd = (temp_hi - temp_lo) / 1.96 # estimate standard deviation using 95% CI\n\n# generate simulations\nhind_years = 1850:2022 # model years to simulate for fitting\nsim_years = 1850:2100 # model years for projections\nforcing_years = 1750:2500 # years for which we have forcing data/simulations\nhind_idx = indexin(hind_years, forcing_years) # find indices in t vector of simulation years\nsim_idx = indexin(sim_years, forcing_years)\n\nplot(time_obs, temp_obs, ribbon=(temp_obs-temp_lo,temp_hi-temp_obs), color=\"blue\", linewidth=2, fillalpha=0.2, legend=false, xlabel=\"Year\", ylabel=\"Temperature anomaly (°C)\", labelfontsize=18, tickfontsize=16, bottom_margin=10mm, left_margin=10mm)\nplot!(size=(550, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n1850\n\n\n1900\n\n\n1950\n\n\n2000\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−0.25\n\n\n0.00\n\n\n0.25\n\n\n0.50\n\n\n0.75\n\n\n1.00\n\n\n1.25\n\n\nTemperature anomaly (°C)\n\n\n\n\n\n\nFigure 1: Global temperature anomalies\n\n\n\n\n\nData Source: HadCRUT 5.0.1.0\n\n\n\n\n\nFrog in Pots Cartoon\n\n\n\nSource: Weekly Humorist"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#planetary-energy-balance",
    "href": "slides/lecture04-1-diffeqmodels.html#planetary-energy-balance",
    "title": "Modeling Dynamical Systems",
    "section": "Planetary Energy Balance",
    "text": "Planetary Energy Balance\n\nRepresentation of Planetary Energy Balance\nSource: Reprinted from A Climate Modeling Primer, A. Henderson-Sellers and K. McGuffie, Wiley, pg. 58, (1987) via https://www.e-education.psu.edu/meteo469/node/137."
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#radiative-forcing",
    "href": "slides/lecture04-1-diffeqmodels.html#radiative-forcing",
    "title": "Modeling Dynamical Systems",
    "section": "Radiative Forcing",
    "text": "Radiative Forcing\nClimate changes result from changes to the energy balance of the planet (or radiative forcings), due to e.g.:\n\ngreenhouse gas emissions (which trap radiation, warming the planet);\naerosol emissions from air pollution or volcanic eruptions (which block incoming radiation, cooling the planet);\nchanges to the solar cycle (which can increase or decrease the incoming solar radiation)."
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#historical-radiative-forcing",
    "href": "slides/lecture04-1-diffeqmodels.html#historical-radiative-forcing",
    "title": "Modeling Dynamical Systems",
    "section": "Historical Radiative Forcing",
    "text": "Historical Radiative Forcing\n\n\nCode\n# Dataset from https://zenodo.org/record/3973015\n# The CSV is read into a DataFrame object, and we specify that it is comma delimited\nforcings_all_85 = CSV.read(\"data/climate/ERF_ssp585_1750-2500.csv\", DataFrame, delim=\",\")\n\n# Separate out the individual components\nforcing_co2_85 = forcings_all_85[!,\"co2\"]\n# Get total aerosol forcings\nforcing_aerosol_rad_85 = forcings_all_85[!,\"aerosol-radiation_interactions\"]\nforcing_aerosol_cloud_85 = forcings_all_85[!,\"aerosol-cloud_interactions\"]\nforcing_aerosol_85 = forcing_aerosol_rad_85 + forcing_aerosol_cloud_85\nforcing_total_85 = forcings_all_85[!,\"total\"]\nforcing_non_aerosol_85 = forcing_total_85 - forcing_aerosol_85\nforcing_other_85 = forcing_total_85 - (forcing_co2_85 + forcing_aerosol_85)\n\nt = time_forcing = Int64.(forcings_all_85[!,\"year\"]) # Ensure that years are interpreted as integers\n\nplot(xlabel=\"Year\", ylabel=\"Radiative Forcing (W/m²)\", tickfontsize=16, guidefontsize=18, legendfontsize=16, leftmargin=10mm, bottommargin=5mm, right_margin=5mm)\nplot!(time_forcing, forcing_total_85, label=\"Total\", color=:black, linewidth=3)\nplot!(time_forcing, forcing_co2_85, label=\"CO₂\", color=:orange, linewidth=2)\nplot!(time_forcing, forcing_aerosol_85, label=\"Aerosol\", color=:blue, linewidth=2)\nplot!(time_forcing, forcing_other_85, label=\"Other\", color=:purple, linewidth=2)\nplot!(size=(800, 450))\nxlims!((1750, 2020))\nylims!(-4.5, 5)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1750\n\n\n1800\n\n\n1850\n\n\n1900\n\n\n1950\n\n\n2000\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−4\n\n\n−2\n\n\n0\n\n\n2\n\n\n4\n\n\nRadiative Forcing (W/m²)\n\n\n\n\n\n\n\n\n\nTotal\n\n\n\nCO₂\n\n\n\nAerosol\n\n\n\nOther\n\n\n\n\nFigure 2: Historical and projected radiative forcings.\n\n\n\n\n\nData Source: https://zenodo.org/records/3973015"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#what-are-some-sources-of-relevant-uncertainty-in-understanding-past-and-future-climate-changes-and-impacts",
    "href": "slides/lecture04-1-diffeqmodels.html#what-are-some-sources-of-relevant-uncertainty-in-understanding-past-and-future-climate-changes-and-impacts",
    "title": "Modeling Dynamical Systems",
    "section": "What Are Some Sources of Relevant Uncertainty in Understanding Past and Future Climate Changes and Impacts?",
    "text": "What Are Some Sources of Relevant Uncertainty in Understanding Past and Future Climate Changes and Impacts?\n\nOne key question: what is the sensitivity of warming to continued CO2 emissions?"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#the-energy-balance-model-ebm",
    "href": "slides/lecture04-1-diffeqmodels.html#the-energy-balance-model-ebm",
    "title": "Modeling Dynamical Systems",
    "section": "The Energy Balance Model (EBM)",
    "text": "The Energy Balance Model (EBM)\n\n\n\\[\\begin{align*}\n\\overbrace{\\frac{dH}{dt}}^{\\text{change in heat}} &= \\overbrace{F}^{\\text{RF}} - \\overbrace{\\lambda T}^{\\substack{\\text{change in} \\\\ \\text{temperature}}} \\\\[1em]\n\\Rightarrow C\\frac{dT}{dt} &= F - \\lambda T - \\gamma(T-T_D)\\\\\nC_D\\frac{dT_D}{dt} &= \\gamma(T-T_D)\n\\end{align*}\\]\n\n\n\n\nTwo Layer EBM Schematic\n\n\n\nSource: Palmer et al. (2018)"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#the-ebm-contd",
    "href": "slides/lecture04-1-diffeqmodels.html#the-ebm-contd",
    "title": "Modeling Dynamical Systems",
    "section": "The EBM (cont’d)",
    "text": "The EBM (cont’d)\n\n\\(c = 4.184\\times 10^6 \\\\ \\text{J/K/m}^2\\) is the specific heat of water per area.\nTotal RF: \\[F = F_\\text{non-aerosol} + \\alpha F_\\text{aerosol}.\\]\nThe climate feedback factor \\(\\lambda\\) controls how much the Earth warms in response to radiative forcing."
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#ebm-discretization",
    "href": "slides/lecture04-1-diffeqmodels.html#ebm-discretization",
    "title": "Modeling Dynamical Systems",
    "section": "EBM Discretization",
    "text": "EBM Discretization\nUse Euler discretization:\n\\[\\begin{align*}\nT(t+1) &= T(t) + \\frac{F(t) - \\lambda T(t) - \\gamma(T(t) - T_D(d))}{C} \\Delta t \\\\[0.5em]\nT_D(t+1) &= T_D(t) + \\frac{\\gamma (T(t) - T_D(t))}{C_D} \\Delta t\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#equilibrium-climate-sensitivity-ecs",
    "href": "slides/lecture04-1-diffeqmodels.html#equilibrium-climate-sensitivity-ecs",
    "title": "Modeling Dynamical Systems",
    "section": "Equilibrium Climate Sensitivity (ECS)",
    "text": "Equilibrium Climate Sensitivity (ECS)\nUnder steady-state conditions (constant \\(F\\) and \\(dT/dt = 0\\)), \\[T = \\frac{F}{\\lambda}.\\]\nWhen we double atmospheric CO2, we refer to the equilibrium temperature \\(S\\) as the equilibrium climate sensitivity:\n\\[S = \\underbrace{F_{2\\times \\text{CO}_2}}_{\\approx 4 \\text{W/m}^2}/\\lambda\\]"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#degree-of-freedom-free-parameters",
    "href": "slides/lecture04-1-diffeqmodels.html#degree-of-freedom-free-parameters",
    "title": "Modeling Dynamical Systems",
    "section": "Degree of Freedom / Free Parameters",
    "text": "Degree of Freedom / Free Parameters\nThere are a few uncertain parameters:\n\n\\(\\lambda\\) or \\(S\\)\n\\(\\gamma\\) (deep ocean temp diffusion)\n\\(\\alpha\\) (aerosol scaling factor)\n\\(d\\) (upper ocean mixing depth)\n\\(D\\) (deep ocean mixing depth)\n\\(T_0\\) (initial temperature in 1850)"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#programming-implementation",
    "href": "slides/lecture04-1-diffeqmodels.html#programming-implementation",
    "title": "Modeling Dynamical Systems",
    "section": "Programming Implementation",
    "text": "Programming Implementation\n\n\nCode\nfunction ebm(rf_nonaerosol, rf_aerosol; p=(3.2, 1.0, 1.3, 100.0, 800.0, -0.1))\n    # set up model parameters\n    S, γ, α, d, D, T₀ = p # this unpacks the parameter tuple into variables\n    F2xCO₂ = 4.0 # radiative forcing [W/m²] for a doubling of CO₂\n    λ = F2xCO₂ / S\n\n    c = 4.184e6 # heat capacity/area [J/K/m²]\n    C = c*d # heat capacity of mixed layer (per area)\n    CD = c*D # heat capacity of deep layer (per area)\n    F = rf_nonaerosol + α*rf_aerosol # radiative forcing\n    Δt = 31558152. # annual timestep [s]\n\n    T = zero(F)\n    T[1] = T₀\n    TD = zero(F)\n    for i in 1:length(F)-1\n        T[i+1] = T[i] + (F[i] - λ*T[i] - γ*(T[i]-TD[i]))/C * Δt\n        TD[i+1] = TD[i] + γ*(T[i]-TD[i])/CD * Δt\n    end\n    # return after normalizing to reference period\n    return T\nend\n\nebm_wrap(params) = ebm(forcing_non_aerosol_85[hind_idx], forcing_aerosol_85[hind_idx], p = params)"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#probability-models-for-simulations",
    "href": "slides/lecture04-1-diffeqmodels.html#probability-models-for-simulations",
    "title": "Modeling Dynamical Systems",
    "section": "Probability Models for Simulations",
    "text": "Probability Models for Simulations\nComputer Model: \\[\\eta(\\underbrace{\\theta}_{\\substack{\\text{calibration}\\\\\\text{variables}}}; \\underbrace{x}_{\\substack{\\text{control}\\\\\\text{variables}}})\\]\nObservations: \\[\\mathbf{y} \\sim p(\\underbrace{\\zeta(\\mathbf{x})}_{\\substack{\\text{expected}\\\\\\text{state}}})\\]"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#model-data-discrepancy",
    "href": "slides/lecture04-1-diffeqmodels.html#model-data-discrepancy",
    "title": "Modeling Dynamical Systems",
    "section": "Model-Data Discrepancy",
    "text": "Model-Data Discrepancy\nWrite \\[\\zeta(\\mathbf{x}) = \\delta(\\eta(\\theta; x))\\] where \\(\\delta\\) represents the discrepancy between the model output and the expected state.\nThen the probability model is: \\[\\mathbf{y} \\sim p(\\delta(\\eta(\\theta; x))).\\]"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#model-data-discrepancy-1",
    "href": "slides/lecture04-1-diffeqmodels.html#model-data-discrepancy-1",
    "title": "Modeling Dynamical Systems",
    "section": "Model-Data Discrepancy",
    "text": "Model-Data Discrepancy\nFor example, \\(\\delta\\) might capture:\n\nBias (e.g.: model consistently over/underpredicts);\nAccumulations of error (e.g.: persistent model underestimates);\nPartial observations (e.g.: do you count every animal?)"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#probability-models-for-simulation-models",
    "href": "slides/lecture04-1-diffeqmodels.html#probability-models-for-simulation-models",
    "title": "Modeling Dynamical Systems",
    "section": "Probability Models for Simulation Models",
    "text": "Probability Models for Simulation Models\nMost common setting (e.g. Brynjarsdóttir & O’Hagan (2014)):\n\\[\\mathbf{y} = \\underbrace{\\eta(\\mathbf{x}; \\theta)}_{\\text{model}} + \\underbrace{\\delta(x)}_{\\text{discrepancy}} + \\underbrace{\\varepsilon}_{\\text{error}}\\]"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#assuming-gaussian-discrepancy",
    "href": "slides/lecture04-1-diffeqmodels.html#assuming-gaussian-discrepancy",
    "title": "Modeling Dynamical Systems",
    "section": "Assuming Gaussian Discrepancy",
    "text": "Assuming Gaussian Discrepancy\n\\[\n\\begin{align*}\n\\mathbf{y} &= \\eta(\\mathbf{x}; \\theta) + \\delta(x) + \\varepsilon \\\\\n\\delta &\\sim N(0, \\sigma^2) \\\\\n\\varepsilon &\\sim N(0, \\omega^2)\n\\end{align*}\n\\]\nwhere \\(\\omega\\) is the standard error of the data."
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#mle-for-gaussian-discrepancy",
    "href": "slides/lecture04-1-diffeqmodels.html#mle-for-gaussian-discrepancy",
    "title": "Modeling Dynamical Systems",
    "section": "MLE for Gaussian Discrepancy",
    "text": "MLE for Gaussian Discrepancy\n\n\nCode\nfunction gaussian_iid_homosked(params, temp_dat, temp_err, m)\n    S, γ, α, d, D, T₀, σ = params \n    ebm_sim = m((S, γ, α, d, D, T₀))\n    ll = sum(logpdf.(Normal.(ebm_sim, sqrt.(σ^2 .+ temp_err.^2)), temp_dat))\n    return ll\nend\n\nlower = [1.0, 0.5, 0.0, 50.0, 200.0, temp_lo[1], 0.0]\nupper = [5.0, 1.5, 2.0, 200.0, 1000.0, temp_hi[1], 10.0]\np0 = [3.0, 1.0, 1.0, 100.0, 800.0,temp_obs[1], 5.0]\n\nresult = Optim.optimize(params -&gt; -gaussian_iid_homosked(params, temp_obs, temp_sd, ebm_wrap), lower, upper, p0)\nθ_iid = result.minimizer"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#mle-for-gaussian-discrepancy-1",
    "href": "slides/lecture04-1-diffeqmodels.html#mle-for-gaussian-discrepancy-1",
    "title": "Modeling Dynamical Systems",
    "section": "MLE for Gaussian Discrepancy",
    "text": "MLE for Gaussian Discrepancy\n\n\n\n\n\nParameters\nMLE\n\n\n\n\nS\n3.4\n\n\nγ\n1.5\n\n\nα\n0.9\n\n\nd\n77.1\n\n\nD\n627.6\n\n\nT₀\n-0.1\n\n\nσ\n0.1\n\n\n\n\n\n\nCode\nn_samples = 10_000\ntemp_iid = ebm_wrap(θ_iid) # simulate IID best fit\n# simulate projections with discrepancy and errors\ntemp_iid_proj = zeros(n_samples, length(temp_sd))\nfor i = 1:length(temp_sd)\n    temp_iid_err = rand(Normal(0, sqrt.(θ_iid[end]^2 .+ temp_sd[i]^2)), n_samples)\n    temp_iid_proj[:, i] = temp_iid[i] .+ temp_iid_err\nend\n# calculate quantiles\ntemp_iid_q = mapslices(col -&gt; quantile(col, [0.05, 0.5, 0.95]), temp_iid_proj; dims=1)\n\np = scatter(time_obs, temp_obs, color=:black, label=\"Observations\", ylabel=\"(°C)\", xlabel=\"Year\", title=\"Temperature Anomaly\")\nplot!(p, time_obs, temp_iid_q[2, :], ribbon=(temp_iid_q[2, :] - temp_iid_q[1, :], temp_iid_q[3, :] - temp_iid_q[2, :]), color=:red, fillalpha=0.3, label=\"Gaussian Discrepancy\")\nplot!(size=(600, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n1850\n\n\n1900\n\n\n1950\n\n\n2000\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−0.5\n\n\n0.0\n\n\n0.5\n\n\n1.0\n\n\n1.5\n\n\n(°C)\n\n\nTemperature Anomaly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nGaussian Discrepancy\n\n\n\n\nFigure 3: MLE Fit for Gaussian discrepancy"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#diagnosing-residuals",
    "href": "slides/lecture04-1-diffeqmodels.html#diagnosing-residuals",
    "title": "Modeling Dynamical Systems",
    "section": "Diagnosing Residuals",
    "text": "Diagnosing Residuals\n\n\nCode\nresids_homogauss = temp_obs - temp_iid\np1 = plot(time_obs, resids_homogauss, linewidth=3, label=false, size=(600, 500), xlabel=\"Year\", ylabel=\"Residual (°C)\")\np2 = histogram(resids_homogauss, size=(600, 500), xlabel=\"Residual (°C)\", ylabel=\"Count\")\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n1850\n\n\n1900\n\n\n1950\n\n\n2000\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−0.3\n\n\n−0.2\n\n\n−0.1\n\n\n0.0\n\n\n0.1\n\n\n0.2\n\n\nResidual (°C)\n\n\n\n\n\n(a) Residuals\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−0.3\n\n\n−0.2\n\n\n−0.1\n\n\n0.0\n\n\n0.1\n\n\n0.2\n\n\n0.3\n\n\nResidual (°C)\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n10\n\n\n20\n\n\n30\n\n\nCount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#analyzing-residual-assumptions",
    "href": "slides/lecture04-1-diffeqmodels.html#analyzing-residual-assumptions",
    "title": "Modeling Dynamical Systems",
    "section": "Analyzing Residual Assumptions",
    "text": "Analyzing Residual Assumptions\n\n\nCode\np1 = qqnorm(resids_homogauss, xlabel=\"Theoretical Values\", ylabel=\"Empirical Values\", title=\"Normal Q-Q Plot\", size=(600, 500))\npacf_homogauss = pacf(resids_homogauss, 1:5)\np2 = plot(1:5, pacf_homogauss, marker=:circle, line=:stem, linewidth=3, markersize=8, tickfontsize=16, guidefontsize=18, legend=false, ylabel=\"Partial Autocorrelation\", xlabel=\"Time Lag\", title=\"Partial Autocorrelation Plot\", size=(600, 500))\nhline!(p2, [0], color=:black, linestyle=:dash)\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n−0.4\n\n\n−0.2\n\n\n0.0\n\n\n0.2\n\n\nTheoretical Values\n\n\n\n\n\n\n\n\n\n\n\n\n−0.4\n\n\n−0.2\n\n\n0.0\n\n\n0.2\n\n\nEmpirical Values\n\n\nNormal Q-Q Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Residual diagnostics\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\nTime Lag\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.0\n\n\n0.1\n\n\n0.2\n\n\n0.3\n\n\n0.4\n\n\n0.5\n\n\nPartial Autocorrelation\n\n\nPartial Autocorrelation Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#key-points-1",
    "href": "slides/lecture04-1-diffeqmodels.html#key-points-1",
    "title": "Modeling Dynamical Systems",
    "section": "Key Points",
    "text": "Key Points\n\nProbability models for simulation models involve\n\nDiscrepancy\nObservation error\n\nMost common: additive\n“System state”: Model output + discrepancy\nCheck residual assumptions!"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#key-points-2",
    "href": "slides/lecture04-1-diffeqmodels.html#key-points-2",
    "title": "Modeling Dynamical Systems",
    "section": "Key Points",
    "text": "Key Points\n\nHindcast: compare observations with full output (discrepancy + error)\nProjections: no error, just discrepancy (state estimates)"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#questions-to-seed-discussion",
    "href": "slides/lecture04-1-diffeqmodels.html#questions-to-seed-discussion",
    "title": "Modeling Dynamical Systems",
    "section": "Questions To Seed Discussion",
    "text": "Questions To Seed Discussion\n\nWhat do you think are the differences between predictive and explanatory modeling?\nWhat can go wrong when we conflate the two?\nCan you think of approaches or workflows which bridge the two paradigms?"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#next-classes",
    "href": "slides/lecture04-1-diffeqmodels.html#next-classes",
    "title": "Modeling Dynamical Systems",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Correlates residuals and more general structures"
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#assessments",
    "href": "slides/lecture04-1-diffeqmodels.html#assessments",
    "title": "Modeling Dynamical Systems",
    "section": "Assessments",
    "text": "Assessments\nHomework 2 available; due next Friday (2/21)."
  },
  {
    "objectID": "slides/lecture04-1-diffeqmodels.html#references-scroll-for-full-list",
    "href": "slides/lecture04-1-diffeqmodels.html#references-scroll-for-full-list",
    "title": "Modeling Dynamical Systems",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nBrynjarsdóttir, J., & O’Hagan, A. (2014). Learning about physical parameters: the importance of model discrepancy. Inverse Problems, 30, 114007. https://doi.org/10.1088/0266-5611/30/11/114007\n\n\nPalmer, M. D., Harris, G. R., & Gregory, J. M. (2018). Extending CMIP5 projections of global mean temperature change and sea level rise due to thermal expansion using a physically-based emulator. Environ. Res. Lett., 13, 084003. https://doi.org/10.1088/1748-9326/aad2e4"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#markov-chains",
    "href": "slides/lecture09-1-mcmc.html#markov-chains",
    "title": "Markov Chain Monte Carlo",
    "section": "Markov Chains",
    "text": "Markov Chains\n\nStochastic process with transition matrix \\(P\\): \\(\\lambda_t = \\lambda_{t-1}P\\).\nMarkovian (memoryless) property: \\[\\mathbb{P}(X_t = x_i | X_{t-1} = x_{t-1}) = \\mathbb{P}(X_t = s_i | X_{t-1} = x_{t-1}, \\ldots, X_0 = x_0)\\]\nErgodic chains: dynamics converge to limiting distribution \\[\\pi_j = \\lim_{n \\to \\infty} \\mathbb{P}(X_n = s_j)\\] which is stationary, \\(\\pi = \\pi P\\)."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#markov-chain-monte-carlo",
    "href": "slides/lecture09-1-mcmc.html#markov-chain-monte-carlo",
    "title": "Markov Chain Monte Carlo",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\n\nFamily of methods for simulating from hard-to-sample from distributions \\(\\pi\\);\nRely on ergodic Markov chains for simulation;\nBy construction, chains converge to limiting distribution which is the target distribution \\(\\pi\\)."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#detailed-balance",
    "href": "slides/lecture09-1-mcmc.html#detailed-balance",
    "title": "Markov Chain Monte Carlo",
    "section": "Detailed Balance",
    "text": "Detailed Balance\nLet \\(\\{X_t\\}\\) be a Markov chain and let \\(\\pi\\) be a probability distribution over the states. Then the chain is in detailed balance with respect to \\(\\pi\\) if \\[\\pi_i P_{ij} = \\pi_j P_{ji}.\\]\nDetailed balance implies reversibility: the chain’s dynamics are the same when viewed forwards or backwards in time."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#detailed-balance-intuition",
    "href": "slides/lecture09-1-mcmc.html#detailed-balance-intuition",
    "title": "Markov Chain Monte Carlo",
    "section": "Detailed Balance Intuition",
    "text": "Detailed Balance Intuition\nA nice analogy (from Miranda Holmes-Cerfon) is traffic flow.\n\n\nConsider NYC and its surroundings: each borough/region can be thought of as a node, and population transitions occur across bridges/tunnels.\n\n\n\n\nNew York City Graph"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#detailed-balance-stationary-distributions",
    "href": "slides/lecture09-1-mcmc.html#detailed-balance-stationary-distributions",
    "title": "Markov Chain Monte Carlo",
    "section": "Detailed Balance: Stationary Distributions",
    "text": "Detailed Balance: Stationary Distributions\nDetailed balance is a sufficient but not necessary condition for the existence of a stationary distribution (namely \\(\\pi\\)):\n\\[\\begin{align*}\n(\\pi P)_i &= \\sum_j \\pi_j P_{ji} \\\\\n&= \\sum_j \\pi_i P_{ij} \\\\\n&= \\pi_i \\sum_j P_{ij} = \\pi_i\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#idea-of-sampling-algorithm",
    "href": "slides/lecture09-1-mcmc.html#idea-of-sampling-algorithm",
    "title": "Markov Chain Monte Carlo",
    "section": "Idea of Sampling Algorithm",
    "text": "Idea of Sampling Algorithm\nThe idea of our sampling algorithm is to construct an ergodic Markov chain from the detailed balance equation for the target distribution.\n\nDetailed balance implies that the target distribution is the stationary distribution.\nErgodicity implies that this distribution is unique and can be obtained as the limiting distribution of the chain’s dynamics."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#idea-of-sampling-algorithm-1",
    "href": "slides/lecture09-1-mcmc.html#idea-of-sampling-algorithm-1",
    "title": "Markov Chain Monte Carlo",
    "section": "Idea of Sampling Algorithm",
    "text": "Idea of Sampling Algorithm\nIn other words:\n\nGenerate an appropriate Markov chain so that its stationary distribution of the target distribution \\(\\pi\\);\nRun its dynamics long enough to converge to the stationary distribution;\nUse the resulting ensemble of states as Monte Carlo samples from \\(\\pi\\) ."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#sampling-algorithm",
    "href": "slides/lecture09-1-mcmc.html#sampling-algorithm",
    "title": "Markov Chain Monte Carlo",
    "section": "Sampling Algorithm",
    "text": "Sampling Algorithm\nAny algorithm which follows this procedure is a Markov chain Monte Carlo algorithm.\nGood news: These algorithms are designed to work quite generally, without (usually) having to worry about technical details like detailed balance and ergodicity.\nBad news: They can involve quite a bit of tuning for computational efficiency. Some algorithms or implementations are faster/adaptive to reduce this need."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#sampling-algorithm-1",
    "href": "slides/lecture09-1-mcmc.html#sampling-algorithm-1",
    "title": "Markov Chain Monte Carlo",
    "section": "Sampling Algorithm",
    "text": "Sampling Algorithm\nAnnoying news:\n\nConvergence to the stationary distribution is only guaranteed asymptotically; evaluating if the chain has been run long enough requires lots of heuristics.\nDue to Markovian property, samples are autocorrelated, so smaller “effective sample size” than the raw number of samples."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#what-is-the-upshot",
    "href": "slides/lecture09-1-mcmc.html#what-is-the-upshot",
    "title": "Markov Chain Monte Carlo",
    "section": "What Is The Upshot?",
    "text": "What Is The Upshot?\nGiven a Markov chain \\(\\{X_t\\}_{t=1, \\ldots, T}\\) returned from MCMC, sampling from distribution \\(\\pi\\):\n\n\\(\\mathbb{P}(X_t = y) \\to \\pi(y)\\) as \\(t \\to \\infty\\)\nThis means the chain can be considered a dependent sample approximately distributed from \\(\\pi\\).\nThe first values (the transient portion) of the chain are highly dependent on the initial value but do not affect asymptotic convergence."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#history-of-mcmc",
    "href": "slides/lecture09-1-mcmc.html#history-of-mcmc",
    "title": "Markov Chain Monte Carlo",
    "section": "History of MCMC",
    "text": "History of MCMC\n\nMetropolis et al\n\n“…instead of choosing configurations randomly, then weighting them with \\(\\exp(- E/ kT)\\), we choose configurations with a probability \\(\\exp (- E/ kT)\\) and weight them evenly.”"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#example-metropolis-hastings",
    "href": "slides/lecture09-1-mcmc.html#example-metropolis-hastings",
    "title": "Markov Chain Monte Carlo",
    "section": "Example: Metropolis-Hastings",
    "text": "Example: Metropolis-Hastings\nGiven \\(X_t = x_t\\):\n\nGenerate \\(Y_t \\sim q(y | x_t)\\);\nSet \\(X_{t+1} = Y_t\\) with probability \\(\\rho(x_t, Y_t)\\), where \\[\\rho(x, y) = \\min \\left\\{\\frac{\\pi(y)}{\\pi(x)}\\frac{q(x | y)}{q(y | x)}, 1\\right\\},\\] else set \\(X_{t+1} = x_t\\)."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#m-h-algorithm-illustration",
    "href": "slides/lecture09-1-mcmc.html#m-h-algorithm-illustration",
    "title": "Markov Chain Monte Carlo",
    "section": "M-H Algorithm Illustration",
    "text": "M-H Algorithm Illustration\n\n\n\n\n\nFirst Example of M-H Algorithm\n\n\n\n\n\n\nSecond Example of M-H Algorithm"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#how-simple-is-that",
    "href": "slides/lecture09-1-mcmc.html#how-simple-is-that",
    "title": "Markov Chain Monte Carlo",
    "section": "How Simple Is That?",
    "text": "How Simple Is That?\nThe devil is in the details: performance and efficiency are highly dependent on the choice of \\(q\\).\n\nKey: There is a tradeoff between exploration and acceptance.\n\nWide proposal: Can make bigger jumps, may be more likely to reject proposals.\nNarrow proposal: More likely to accept proposals, may not “mix” efficiently."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#more-modern-mcmc-algorithms",
    "href": "slides/lecture09-1-mcmc.html#more-modern-mcmc-algorithms",
    "title": "Markov Chain Monte Carlo",
    "section": "More Modern MCMC Algorithms",
    "text": "More Modern MCMC Algorithms\nMany innovations in the last decade: best methods use gradients and don’t require much tuning.\nThese days, no real reason to not use Hamiltonian Monte Carlo (default in pyMC3, Turing, Stan, other probabilistic programming languages) unless you can’t write your code in a PPL."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#san-francisco-tide-gauge-data",
    "href": "slides/lecture09-1-mcmc.html#san-francisco-tide-gauge-data",
    "title": "Markov Chain Monte Carlo",
    "section": "San Francisco Tide Gauge Data",
    "text": "San Francisco Tide Gauge Data\n\n\nCode\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(DataFrames.transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18\n)\np2 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(1, 1.7), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#probability-model",
    "href": "slides/lecture09-1-mcmc.html#probability-model",
    "title": "Markov Chain Monte Carlo",
    "section": "Probability Model",
    "text": "Probability Model\n\\[\n\\begin{align*}\n& y \\sim LogNormal(\\mu, \\sigma) \\tag{likelihood}\\\\\n& \\left. \\begin{aligned}\n& \\mu \\sim Normal(0, 0.5) \\\\\n& \\sigma \\sim HalfNormal(0, 0.1)\n\\end{aligned} \\right\\} \\tag{priors}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#specifying-extreme-example-with-turing.jl",
    "href": "slides/lecture09-1-mcmc.html#specifying-extreme-example-with-turing.jl",
    "title": "Markov Chain Monte Carlo",
    "section": "Specifying Extreme Example with Turing.jl",
    "text": "Specifying Extreme Example with Turing.jl\n\n\nCode\nusing Turing\n## y: observed data\n## can also specify covariates or auxiliary data in the function if used\n@model function tide_model(y)\n    # specify priors\n    μ ~ Normal(0, 0.5)\n    σ ~ truncated(Normal(0, 0.1), 0, Inf)\n    # specify likelihood\n    y ~ LogNormal(μ, σ)\n    # returning y allows us (later) to generate predictive simulations\n    return y \nend"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#sampling-from-posterior",
    "href": "slides/lecture09-1-mcmc.html#sampling-from-posterior",
    "title": "Markov Chain Monte Carlo",
    "section": "Sampling from Posterior",
    "text": "Sampling from Posterior\n\n\nCode\nm = tide_model(dat_annmax.residual)\n# draw 10_000 samples using NUTS() sampler, with 4 chains (using MCMCThreads() for serial sampling)\nsurge_chain = sample(m, NUTS(), MCMCThreads(), 10_000, 4, progress=false)\n\n\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/kwj9g/src/sample.jl:384\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.05\n\n\n\n\nChains MCMC chain (10000×14×4 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 4\nSamples per chain = 10000\nWall duration     = 6.29 seconds\nCompute duration  = 5.06 seconds\nparameters        = μ, σ\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nUse `describe(chains)` for summary statistics and quantiles."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#what-does-this-output-mean",
    "href": "slides/lecture09-1-mcmc.html#what-does-this-output-mean",
    "title": "Markov Chain Monte Carlo",
    "section": "What Does This Output Mean?",
    "text": "What Does This Output Mean?\n\nMCSE: Monte Carlo Standard Error for mean.\nESS (Effective Sample Size): Accounts for autocorrelation \\(\\rho_t\\) across samples \\[N_\\text{eff} = \\frac{N}{1+2\\sum_{t=1}^\\infty \\rho_t}\\]\nRhat: Convergence metric (Gelman & Rubin (1992)) based on multiple chains."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#visualizing-the-sampler",
    "href": "slides/lecture09-1-mcmc.html#visualizing-the-sampler",
    "title": "Markov Chain Monte Carlo",
    "section": "Visualizing the Sampler",
    "text": "Visualizing the Sampler\n\n\nCode\nplot(surge_chain, size=(1200, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Posterior samples from Turing.jl"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#what-can-go-wrong",
    "href": "slides/lecture09-1-mcmc.html#what-can-go-wrong",
    "title": "Markov Chain Monte Carlo",
    "section": "What Can Go Wrong?",
    "text": "What Can Go Wrong?\n\n\n\n\nMCMC Sampling for Various Proposals"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#autocorrelation-of-chains",
    "href": "slides/lecture09-1-mcmc.html#autocorrelation-of-chains",
    "title": "Markov Chain Monte Carlo",
    "section": "Autocorrelation of Chains",
    "text": "Autocorrelation of Chains\n\n\n\n\nMCMC Sampling for Various Proposals"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#how-to-identify-convergence",
    "href": "slides/lecture09-1-mcmc.html#how-to-identify-convergence",
    "title": "Markov Chain Monte Carlo",
    "section": "How To Identify Convergence?",
    "text": "How To Identify Convergence?\nShort answer: There is no guarantee! Judgement based on an accumulation of evidence from various heuristics.\n\nThe good news — getting the precise “right” end of the transient chain doesn’t matter.\nIf a few transient iterations remain, the effect will be washed out with a large enough post-convergence chain."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#heuristics-for-convergence",
    "href": "slides/lecture09-1-mcmc.html#heuristics-for-convergence",
    "title": "Markov Chain Monte Carlo",
    "section": "Heuristics for Convergence",
    "text": "Heuristics for Convergence\nCompare distribution (histogram/kernel density plot) after half of the chain to full chain.\n\n\n\n\n\n\n\n\n2000 Iterations\n\n\n\n\n\n\n\n10000 Iterations\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#gelman-rubin-diagnostic",
    "href": "slides/lecture09-1-mcmc.html#gelman-rubin-diagnostic",
    "title": "Markov Chain Monte Carlo",
    "section": "Gelman-Rubin Diagnostic",
    "text": "Gelman-Rubin Diagnostic\nGelman & Rubin (1992)\n\nRun multiple chains from “overdispersed” starting points\nCompare intra-chain and inter-chain variances\nSummarized as \\(\\hat{R}\\) statistic: closer to 1 implies better convergence."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#on-multiple-chains",
    "href": "slides/lecture09-1-mcmc.html#on-multiple-chains",
    "title": "Markov Chain Monte Carlo",
    "section": "On Multiple Chains",
    "text": "On Multiple Chains\nUnless a specific parallelized scheme (called sequential Monte Carlo) is used, cannot run multiple shorter chains in lieu of one longer chain since each chain needs to individually converge.\nThis means multiple chains are more useful for diagnostics. But you can sample from each once they’ve converged."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#heuristics-for-convergence-1",
    "href": "slides/lecture09-1-mcmc.html#heuristics-for-convergence-1",
    "title": "Markov Chain Monte Carlo",
    "section": "Heuristics for Convergence",
    "text": "Heuristics for Convergence\n\nIf you’re more interested in the mean estimate, can also look at the its stability by iteration or the Monte Carlo standard error.\nLook at traceplots; do you see sudden “jumps”?\nWhen in doubt, run the chain longer."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#transient-chain-portion",
    "href": "slides/lecture09-1-mcmc.html#transient-chain-portion",
    "title": "Markov Chain Monte Carlo",
    "section": "Transient Chain Portion",
    "text": "Transient Chain Portion\nWhat do we do with the transient portion of the chain?\n\n\nDiscard as burn-in (might be done automatically by a PPL);\nJust run the chain longer."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#what-to-do-after",
    "href": "slides/lecture09-1-mcmc.html#what-to-do-after",
    "title": "Markov Chain Monte Carlo",
    "section": "What To Do After?",
    "text": "What To Do After?\n\nCan compute expectations using the full chain; MCSE is more complicated but is reported from most PPL outputs (otherwise platform specific implementations).\nCan subsample from or thin chain if computationally convenient (but pay attention to \\(ESS\\))."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#posterior-visualization",
    "href": "slides/lecture09-1-mcmc.html#posterior-visualization",
    "title": "Markov Chain Monte Carlo",
    "section": "Posterior Visualization",
    "text": "Posterior Visualization\n\n\nCode\nchn_1 = surge_chain[:, :, 1]\np1 = histogram(chn_1[:μ], label=\"Samples\", normalize=:pdf, legend=:topleft, xlabel=L\"μ\", ylabel=L\"p(μ|y)\",)\np2 = histogram(chn_1[:σ], label=\"Samples\", normalize=:pdf, legend=:topleft, xlabel=L\"σ\", ylabel=L\"p(σ|y)\")\np = plot(p1, p2, layout = @layout [a b])\nvline!(p, mean(chn_1)[:, 2]', color=:purple, linewidth=3, label=\"Posterior Mean\")\nplot!(p, size=(1200, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Posterior visualization for surge chain"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#correlations",
    "href": "slides/lecture09-1-mcmc.html#correlations",
    "title": "Markov Chain Monte Carlo",
    "section": "Correlations",
    "text": "Correlations\n\n\nCode\np1 = histogram2d(chn_1[:μ], chn_1[:σ], normalize=:pdf, legend=false, xlabel=L\"$\\mu$\", ylabel=L\"\\sigma\")\nplot!(p1, size=(1200, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Posterior correlations"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#monte-carlo-posterior-predictive-checks",
    "href": "slides/lecture09-1-mcmc.html#monte-carlo-posterior-predictive-checks",
    "title": "Markov Chain Monte Carlo",
    "section": "Monte Carlo: Posterior Predictive Checks",
    "text": "Monte Carlo: Posterior Predictive Checks\n\\[p(\\hat{y} | y) = \\int p(\\hat{y} | \\theta) p(\\theta | y) d\\theta\\]\n\nSimulate \\(\\theta_i\\) from posterior (from chain);\nSimulate statistic or predicted values \\(\\hat{y} | \\theta_i\\).\nCompute expectations/credible intervals from ensemble."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#posterior-predictive-check-example",
    "href": "slides/lecture09-1-mcmc.html#posterior-predictive-check-example",
    "title": "Markov Chain Monte Carlo",
    "section": "Posterior Predictive Check Example",
    "text": "Posterior Predictive Check Example\n\n\nCode\nreturn_periods = 2:100\nreturn_levels = zeros(10_000, length(return_periods))\nplt_rt = plot(; ylabel=\"Return Level (m)\", xlabel=\"Return Period (yrs)\", legend=:bottomright)\nfor idx in 1:10_000\n    μ = chn_1[:μ][idx]\n    σ = chn_1[:σ][idx]\n    return_levels[idx, :] = quantile.(LogNormal(μ, σ), 1 .- (1 ./ return_periods))\n    label = idx == 1 ? \"Posterior\" : false\n    plot!(plt_rt, return_periods, return_levels[idx, :]; color=:black, alpha=0.05, label=label, linewidth=0.5)\nend\n# plot return level quantiles\nrl_q = mapslices(col -&gt; quantile(col, [0.025, 0.5, 0.975]), return_levels, dims=1)\nplot!(plt_rt, return_periods, rl_q[[1,3], :]', color=:green, linewidth=3, label=\"95% CI\")\nplot!(plt_rt, return_periods, rl_q[2, :], color=:red, linewidth=3, label=\"Posterior Median\")\n# plot data\nscatter!(plt_rt, return_periods, quantile(dat_annmax.residual, 1 .- (1 ./ return_periods)), label=\"Data\", color=:blue)\nplot!(plt_rt, size=(1200, 500))\nplt_rt\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Posterior predictive checks"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#key-points-mcmc-convergence",
    "href": "slides/lecture09-1-mcmc.html#key-points-mcmc-convergence",
    "title": "Markov Chain Monte Carlo",
    "section": "Key Points (MCMC Convergence)",
    "text": "Key Points (MCMC Convergence)\n\nMust rely on “accumulation of evidence” from heuristics for determination about convergence to stationary distribution.\nTransient portion of chain: Meh. Some people worry about this too much. Discard or run the chain longer.\nParallelizing solves few problems, but running multiple chains can be useful for diagnostics."
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#next-classes",
    "href": "slides/lecture09-1-mcmc.html#next-classes",
    "title": "Markov Chain Monte Carlo",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Cross-Validation and Model Skill\nNext Week: Entropy and Information Criteria"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#assessments",
    "href": "slides/lecture09-1-mcmc.html#assessments",
    "title": "Markov Chain Monte Carlo",
    "section": "Assessments",
    "text": "Assessments\n\nHomework 3: Due Friday (3/14)\nProject Proposal: Due 3/21"
  },
  {
    "objectID": "slides/lecture09-1-mcmc.html#references-scroll-for-full-list",
    "href": "slides/lecture09-1-mcmc.html#references-scroll-for-full-list",
    "title": "Markov Chain Monte Carlo",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nGelman, A., & Rubin, D. B. (1992). Inference from Iterative Simulation Using Multiple Simulations. Stat. Sci., 7, 457–511. https://doi.org/10.1214/ss/1177011136"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#last-classes",
    "href": "slides/lecture05-1-bayes.html#last-classes",
    "title": "Bayesian Statistics",
    "section": "Last Class(es)",
    "text": "Last Class(es)\n\nProbability models for dynamical systems/simulation models\nGenerative model: can include discrepancy and/or observational errors\n\nModel data, not expected value (regression)\n\nMaximize likelihood over model and statistical parameters."
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#non-uniqueness-of-mle",
    "href": "slides/lecture05-1-bayes.html#non-uniqueness-of-mle",
    "title": "Bayesian Statistics",
    "section": "Non-Uniqueness of MLE",
    "text": "Non-Uniqueness of MLE\n\nMany models do not have well-defined maximum likelihoods."
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#non-identifiability",
    "href": "slides/lecture05-1-bayes.html#non-identifiability",
    "title": "Bayesian Statistics",
    "section": "Non-Identifiability",
    "text": "Non-Identifiability\n\\[\\underbrace{h_t}_{\\substack{\\text{hare} \\\\ \\text{pelts}}} \\sim \\text{LogNormal}(\\log(\\underbrace{p_H}_{\\substack{\\text{trap} \\\\ \\text{rate}}} H_T), \\sigma_H)\\] \\[l_t \\sim \\text{LogNormal}(\\log(p_L L_T), \\sigma_L)\\]\n\n\n\\[\n\\begin{align*}\n\\frac{dH}{dt} &= H_t b_H - H_t (L_t m_H) \\\\\nH_T &= H_1 + \\int_1^T \\frac{dH}{dt}dt\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n\\frac{dL}{dt} &= L_t (H_t b_L) - L_t m_L \\\\\nL_T &= L_1 + \\int_1^T \\frac{dL}{dt}dt\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#non-uniqueness-of-mle-1",
    "href": "slides/lecture05-1-bayes.html#non-uniqueness-of-mle-1",
    "title": "Bayesian Statistics",
    "section": "Non-Uniqueness of MLE",
    "text": "Non-Uniqueness of MLE\n\nMany models do not have well-defined maximum likelihoods.\nCan be due to multi-modality or “ridges”.\nSometimes also referred to as equifinality.\nPoses problems for MLE."
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#prior-information",
    "href": "slides/lecture05-1-bayes.html#prior-information",
    "title": "Bayesian Statistics",
    "section": "Prior Information",
    "text": "Prior Information\nSo far: no way to use prior information about parameters (other than bounds on MLE optimization).\nFor example: what “trap rates” are more plausible?"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#bayes-rule",
    "href": "slides/lecture05-1-bayes.html#bayes-rule",
    "title": "Bayesian Statistics",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\nOriginal version (Bayes, 1763):\n\\[P(A | B) = \\frac{P(B | A) \\times P(A)}{P(B)} \\quad \\text{if} \\quad P(B) \\neq 0.\\]"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#bayes-rule-1",
    "href": "slides/lecture05-1-bayes.html#bayes-rule-1",
    "title": "Bayesian Statistics",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n“Modern” version (Laplace, 1774):\n\\[\\underbrace{{p(\\theta | y)}}_{\\text{posterior}} = \\frac{\\overbrace{p(y | \\theta)}^{\\text{likelihood}}}{\\underbrace{p(y)}_\\text{normalization}} \\overbrace{p(\\theta)}^\\text{prior}\\]"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#bayes-rule-ignoring-normalizing-constants",
    "href": "slides/lecture05-1-bayes.html#bayes-rule-ignoring-normalizing-constants",
    "title": "Bayesian Statistics",
    "section": "Bayes’ Rule (Ignoring Normalizing Constants)",
    "text": "Bayes’ Rule (Ignoring Normalizing Constants)\nThe version of Bayes’ rule which matters the most for 95% (approximate) of Bayesian statistics:\n\\[p(\\theta | y) \\propto p(y | \\theta) \\times p(\\theta)\\]\n\n“The posterior is the prior times the likelihood…”"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#credible-intervals",
    "href": "slides/lecture05-1-bayes.html#credible-intervals",
    "title": "Bayesian Statistics",
    "section": "Credible Intervals",
    "text": "Credible Intervals\nBayesian credible intervals are straightforward to interpret: \\(\\theta\\) is in \\(I\\) with probability \\(\\alpha\\).\nChoose \\(I\\) such that \\[p(\\theta \\in I | \\mathbf{y}) = \\alpha.\\]"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#bayesian-model-components",
    "href": "slides/lecture05-1-bayes.html#bayesian-model-components",
    "title": "Bayesian Statistics",
    "section": "Bayesian Model Components",
    "text": "Bayesian Model Components\nA fully specified Bayesian model includes:\n\nPrior distributions over the parameters, \\(p(\\theta)\\)\nProbability model for the data given the parameters (the likelihood), \\(p(y | \\theta)\\)t\n\nThink: Prior provides proposed explanations, likelihood re-weights based on ability to produce the data."
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#generative-modeling",
    "href": "slides/lecture05-1-bayes.html#generative-modeling",
    "title": "Bayesian Statistics",
    "section": "Generative Modeling",
    "text": "Generative Modeling\nBayesian models lend themselves towards generative simulation by generating new data \\(\\tilde{y}\\) through the posterior predictive distribution:\n\\[p(\\tilde{y} | \\mathbf{y}) = \\int_{\\Theta} p(\\tilde{y} | \\theta) p(\\theta | \\mathbf{y}) d\\theta\\]"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#how-to-choose-a-prior",
    "href": "slides/lecture05-1-bayes.html#how-to-choose-a-prior",
    "title": "Bayesian Statistics",
    "section": "How To Choose A Prior?",
    "text": "How To Choose A Prior?\nOne perspective: Priors should reflect “actual knowledge” independent of the analysis (Jaynes, 2003)\nAnother: Priors are part of the probability model, and can be specified/changed accordingly based on predictive skill (Gelman et al., 2017; Gelman & Shalizi, 2013)"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#what-makes-a-good-prior",
    "href": "slides/lecture05-1-bayes.html#what-makes-a-good-prior",
    "title": "Bayesian Statistics",
    "section": "What Makes A Good Prior?",
    "text": "What Makes A Good Prior?\n\nReflects level of understanding (informative vs. weakly informative vs. non-informative).\nDoes not zero out probability of plausible values.\nRegularization (extreme values should be less probable)"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#what-makes-a-bad-prior",
    "href": "slides/lecture05-1-bayes.html#what-makes-a-bad-prior",
    "title": "Bayesian Statistics",
    "section": "What Makes A Bad Prior?",
    "text": "What Makes A Bad Prior?\n\nAssigns probability zero to plausible values;\nWeights implausible values equally as more plausible ones;\nDouble counts information (e.g. fitting a prior to data which is also used in the likelihood)\nChosen based on vibes.\nPersonal opinion: Uniform distributions"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#a-coin-flipping-example",
    "href": "slides/lecture05-1-bayes.html#a-coin-flipping-example",
    "title": "Bayesian Statistics",
    "section": "A Coin Flipping Example",
    "text": "A Coin Flipping Example\nWe would like to understand if a coin-flipping game is fair. We’ve observed the following sequence of flips:\n\n\nCode\nflips = [\"H\", \"H\", \"H\", \"T\", \"H\", \"H\", \"H\", \"H\", \"H\"]\n\n\n9-element Vector{String}:\n \"H\"\n \"H\"\n \"H\"\n \"T\"\n \"H\"\n \"H\"\n \"H\"\n \"H\"\n \"H\""
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#coin-flipping-likelihood",
    "href": "slides/lecture05-1-bayes.html#coin-flipping-likelihood",
    "title": "Bayesian Statistics",
    "section": "Coin Flipping Likelihood",
    "text": "Coin Flipping Likelihood\nThe data-generating process here is straightforward: we can represent a coin flip with a heads-probability of \\(\\theta\\) as a sample from a Bernoulli distribution,\n\\[y_i \\sim \\text{Bernoulli}(\\theta).\\]\n\n\nCode\nflip_ll(θ) = sum(logpdf(Bernoulli(θ), flips .== \"H\"))\nθ_mle = Optim.optimize(θ -&gt; -flip_ll(θ), 0, 1).minimizer\nround(θ_mle, digits=2)\n\n\n0.89"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#coin-flipping-prior",
    "href": "slides/lecture05-1-bayes.html#coin-flipping-prior",
    "title": "Bayesian Statistics",
    "section": "Coin Flipping Prior",
    "text": "Coin Flipping Prior\nSuppose that we spoke to a friend who knows something about coins, and she tells us that it is extremely difficult to make a passable weighted coin which comes up heads more than 75% of the time."
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#coin-flipping-prior-1",
    "href": "slides/lecture05-1-bayes.html#coin-flipping-prior-1",
    "title": "Bayesian Statistics",
    "section": "Coin Flipping Prior",
    "text": "Coin Flipping Prior\n\n\nSince \\(\\theta\\) is bounded between 0 and 1, we’ll use a Beta distribution for our prior, specifically \\(\\text{Beta}(5,5)\\).\n\n\n\nCode\nprior_dist = Beta(5, 5)\nplot(prior_dist; label=false, xlabel=L\"$θ$\", ylabel=L\"$p(θ)$\", linewidth=3, tickfontsize=16, guidefontsize=18)\nplot!(size=(500, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Beta prior for coin flipping example"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#maximum-a-posteriori-estimate",
    "href": "slides/lecture05-1-bayes.html#maximum-a-posteriori-estimate",
    "title": "Bayesian Statistics",
    "section": "Maximum A Posteriori Estimate",
    "text": "Maximum A Posteriori Estimate\nCombining using Bayes’ rule lets us calculate the maximum a posteriori (MAP) estimate:\n\n\nCode\nflip_ll(θ) = sum(logpdf(Bernoulli(θ), flips .== \"H\"))\nflip_lprior(θ) = logpdf(Beta(5, 5), θ)\nflip_lposterior(θ) = flip_ll(θ) + flip_lprior(θ)\nθ_map = Optim.optimize(θ -&gt; -(flip_lposterior(θ)), 0, 1).minimizer\nround(θ_map, digits=2)\n\n\n0.71"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#coin-flipping-posterior-distribution",
    "href": "slides/lecture05-1-bayes.html#coin-flipping-posterior-distribution",
    "title": "Bayesian Statistics",
    "section": "Coin Flipping Posterior Distribution",
    "text": "Coin Flipping Posterior Distribution\n\n\nCode\nθ_range = 0:0.01:1\nplot(θ_range, flip_lposterior.(θ_range), color=:black, label=\"Posterior\", linewidth=3)\nplot!(θ_range, flip_ll.(θ_range), color=:black, label=\"Likelihood\", linewidth=3, linestyle=:dash)\nplot!(θ_range, flip_lprior.(θ_range), color=:black, label=\"Prior\", linewidth=3, linestyle=:dot)\nvline!([θ_map], color=:red, label=\"MAP\", linewidth=2)\nvline!([θ_mle], color=:blue, label=\"MLE\", linewidth=2)\nxlabel!(L\"$\\theta$\")\nylabel!(\"Log-Density\")\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Posterior distribution for the coin-flipping example"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#bayes-and-parametric-uncertainty",
    "href": "slides/lecture05-1-bayes.html#bayes-and-parametric-uncertainty",
    "title": "Bayesian Statistics",
    "section": "Bayes and Parametric Uncertainty",
    "text": "Bayes and Parametric Uncertainty\nFrequentist: Parametric uncertainty is purely the result of sampling variability\nBayesian: Parameters have probabilities based on consistency with data and priors.\nThink: how “likely” is a set of parameters to have produced the data given the specified data generating process?"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#bayesian-updating",
    "href": "slides/lecture05-1-bayes.html#bayesian-updating",
    "title": "Bayesian Statistics",
    "section": "Bayesian Updating",
    "text": "Bayesian Updating\n\nThe posterior is a “compromise” between the prior and the data.\nThe posterior mean is a weighted combination of the data and the prior mean.\nThe weights depend on the prior and the likelihood variances.\nMore data usually makes the posterior more confident."
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#key-points-1",
    "href": "slides/lecture05-1-bayes.html#key-points-1",
    "title": "Bayesian Statistics",
    "section": "Key Points",
    "text": "Key Points\n\nBayesian probability: parameters have probabilities conditional on data\nNeed to specify prior distribution (think generatively!).\nPosterior distribution reflects compromise between prior and likelihood.\nMaximum a posteriori gives “most probable” parameter values"
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#key-points-priors",
    "href": "slides/lecture05-1-bayes.html#key-points-priors",
    "title": "Bayesian Statistics",
    "section": "Key Points: Priors",
    "text": "Key Points: Priors\n\nUse prior predictive simulations to refine priors.\nBe transparent and principled about prior choices (sensitivity analyses?).\nDon’t choose priors based on convenience.\nWill talk more about general sampling later."
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#next-classes",
    "href": "slides/lecture05-1-bayes.html#next-classes",
    "title": "Bayesian Statistics",
    "section": "Next Classes",
    "text": "Next Classes\nNext Week: Sampling! Specifically, Monte Carlo."
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#assessments",
    "href": "slides/lecture05-1-bayes.html#assessments",
    "title": "Bayesian Statistics",
    "section": "Assessments",
    "text": "Assessments\nHomework 2 due Friday (2/21).\nQuiz: Due Monday (all on today’s lecture).\nProject: Will discuss Monday, start thinking about possible topics."
  },
  {
    "objectID": "slides/lecture05-1-bayes.html#references-scroll-for-full-list",
    "href": "slides/lecture05-1-bayes.html#references-scroll-for-full-list",
    "title": "Bayesian Statistics",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nBayes, T. (1763). An Essay towards Solving a Problem in the Doctrine of Chance. Philosophical Transactions of the Royal Society of London, 53, 370–418.\n\n\nGelman, A., & Shalizi, C. R. (2013). Philosophy and the practice of Bayesian statistics. Br. J. Math. Stat. Psychol., 66, 8–38. https://doi.org/10.1111/j.2044-8317.2011.02037.x\n\n\nGelman, A., Simpson, D., & Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood. Entropy, 19, 555. https://doi.org/10.3390/e19100555\n\n\nJaynes, E. T. (2003). Probability theory: the Logic of Science. (G. L. Bretthorst, Ed.). Cambridge, UK ; New York, NY: Cambridge University Press. Retrieved from https://market.android.com/details?id=book-tTN4HuUNXjgC\n\n\nLaplace, P. S. (1774). Mémoire sur la Probabilité des Causes par les évènemens. In Mémoires de Mathematique et de Physique, Presentés à l’Académie Royale des Sciences, Par Divers Savans & Lus Dans ses Assemblées (pp. 621–656)."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#what-makes-a-good-prediction",
    "href": "slides/lecture10-2-information-models.html#what-makes-a-good-prediction",
    "title": "Information and Entropy",
    "section": "What Makes A Good Prediction?",
    "text": "What Makes A Good Prediction?\nWhat do we want to see in a probabilistic projection \\(F\\)?\n\n\nCalibration: Does the predicted CDF \\(F(y)\\) align with the “true” distribution of observations \\(y\\)? \\[\\mathbb{P}(y \\leq F^{-1}(\\tau)) = \\tau \\qquad \\forall \\tau \\in [0, 1]\\]\nDispersion: Is the concentration (variance) of \\(F\\) aligned with the concentration of observations?\nSharpness: How concentrated are the forecasts \\(F\\)?"
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#probability-integral-transform-pit",
    "href": "slides/lecture10-2-information-models.html#probability-integral-transform-pit",
    "title": "Information and Entropy",
    "section": "Probability Integral Transform (PIT)",
    "text": "Probability Integral Transform (PIT)\nCommon to use the PIT to make these more concrete: \\(Z_F = F(y)\\).\nThe forecast is probabilistically calibrated if \\(Z_F \\sim Uniform(0, 1)\\).\nThe forecast is properly dispersed if \\(\\text{Var}(Z_F) = 1/12\\).\nSharpness can be measured by the width of a particular prediction interval. A good forecast is a sharp as possible subject to calibration (Gneiting et al., 2007)."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#scoring-rules",
    "href": "slides/lecture10-2-information-models.html#scoring-rules",
    "title": "Information and Entropy",
    "section": "Scoring Rules",
    "text": "Scoring Rules\nA scoring rule \\(S(F, y)\\) measures the “loss” of a predicted probability distribution \\(F\\) once an observation \\(y\\) is obtained.\nProper scoring rules are minimized when the forecasted distribution matches the observed distribution:\n\\[\\mathbb{E}_Y(S(G, G)) \\leq \\mathbb{E}_Y(S(F, G)) \\qquad \\forall F.\\]"
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#k-fold-cross-validation",
    "href": "slides/lecture10-2-information-models.html#k-fold-cross-validation",
    "title": "Information and Entropy",
    "section": "\\(k\\)-Fold Cross-Validation",
    "text": "\\(k\\)-Fold Cross-Validation\nWhat if we repeated this procedure for multiple held-out sets?\n\nRandomly split data into \\(k = n / m\\) equally-sized subsets.\nFor each \\(i = 1, \\ldots, k\\), fit model to \\(y_{-i}\\) and test on \\(y_i\\).\n\nIf data are large, this is a good approximation."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#loo-cv-algorithm",
    "href": "slides/lecture10-2-information-models.html#loo-cv-algorithm",
    "title": "Information and Entropy",
    "section": "LOO-CV Algorithm",
    "text": "LOO-CV Algorithm\n\nDrop one value \\(y_i\\).\nRefit model on rest of data \\(y_{-i}\\).\nPredict dropped point \\(p(\\hat{y}_i | y_{-i})\\).\nEvaluate score on dropped point (\\(\\log p(y_i | y_{-i})\\)).\nRepeat on rest of data set."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#interpreting-scores",
    "href": "slides/lecture10-2-information-models.html#interpreting-scores",
    "title": "Information and Entropy",
    "section": "Interpreting Scores",
    "text": "Interpreting Scores\nWhen directly comparing models, this can be straightforward: lower (usually) score =&gt; better.\nBut this doesn’t tell us anything about whether a particular score is good or even acceptable: How do we quantify the “distance” from “perfect” prediction?"
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#uncertainty-and-information",
    "href": "slides/lecture10-2-information-models.html#uncertainty-and-information",
    "title": "Information and Entropy",
    "section": "Uncertainty and Information",
    "text": "Uncertainty and Information\nMore uncertainty ⇒ predictions are more difficult.\nOne approach: quantify information as the reduction in uncertainty conditional on a prediction or projection.\nExample: Perfect prediction ⇒ complete reduction in uncertainty (observation will always match prediction)."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#quantifying-uncertainty",
    "href": "slides/lecture10-2-information-models.html#quantifying-uncertainty",
    "title": "Information and Entropy",
    "section": "Quantifying Uncertainty",
    "text": "Quantifying Uncertainty\nWhat properties should a measure of uncertainty possess?\n\nShould be continuous wrt probabilities;\nShould increase with number of possible events;\nShould be additive."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#entropy",
    "href": "slides/lecture10-2-information-models.html#entropy",
    "title": "Information and Entropy",
    "section": "Entropy",
    "text": "Entropy\nIt turns out there is only one function which satisfies these conditions: Information Entropy (Shannon, 1948)\n\\[H(p) = -\\mathbb{E}(\\log(p_i)) = - \\sum_{i=1}^n p_i \\log p_i\\]\nThe entropy (or uncertainty) of a probability distribution is the average log-probability of an event."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#entropy-example",
    "href": "slides/lecture10-2-information-models.html#entropy-example",
    "title": "Information and Entropy",
    "section": "Entropy Example",
    "text": "Entropy Example\nSuppose in Ithaca we have “true” probabilities of rain, sunshine, and fog which are 0.55, 0.2, and 0.25.\n\\[H(p) = -(0.55 \\log(0.55) + 0.2 \\log(0.2) + 0.25\\log(0.25) \\approx 1.0\\]"
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#entropy-example-1",
    "href": "slides/lecture10-2-information-models.html#entropy-example-1",
    "title": "Information and Entropy",
    "section": "Entropy Example",
    "text": "Entropy Example\nNow suppose in Dubai these probabilities are 0.01, 0.95, and 0.04, respectively.\n\\[H(p) = -(0.01 \\log(0.01) + 0.95 \\log(0.95) + 0.04\\log(0.04) \\approx 0.22\\]\n\\(H\\) is lower in Dubai than in Ithaca because there is less uncertainty about the outcome on a given day."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#maximum-entropy",
    "href": "slides/lecture10-2-information-models.html#maximum-entropy",
    "title": "Information and Entropy",
    "section": "Maximum Entropy",
    "text": "Maximum Entropy\nAs an aside, what distribution maximizes entropy (has the most uncertainty subject to its constraints)?\nThis is equivalent to a distribution being the “least informative” given a set of constraints.\nCan think of this as being the distribution which can emerge through the greatest combination of data-generating events."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#maximum-entropy-examples",
    "href": "slides/lecture10-2-information-models.html#maximum-entropy-examples",
    "title": "Information and Entropy",
    "section": "Maximum Entropy Examples",
    "text": "Maximum Entropy Examples\n\nFrom the Central Limit Theorem, Gaussians emerge as the limit of sums of arbitrary random variables with finite mean and variance.\nThis is equivalent to Gaussians as the entropy-maximizing distribution for a given (finite) mean and variance.\nBinomial distributions maximize entropy under the assumptions of two outcomes with constant probabilities.\n\nThis is where generalized linear models come from!"
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#from-entropy-to-accuracy",
    "href": "slides/lecture10-2-information-models.html#from-entropy-to-accuracy",
    "title": "Information and Entropy",
    "section": "From Entropy to Accuracy",
    "text": "From Entropy to Accuracy\nEntropy: Measure of uncertainty across a distribution \\(p\\).\nDivergence: Uncertainty induced by using probabilities from one distribution \\(q\\) to describe outcomes from another “true” distribution \\(p\\).\nThe lower the divergence between a predictive distribution \\(q\\) and a “true” distribution \\(p\\), the more “skill” \\(q\\) has."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#kullback-leibler-divergence",
    "href": "slides/lecture10-2-information-models.html#kullback-leibler-divergence",
    "title": "Information and Entropy",
    "section": "Kullback-Leibler Divergence",
    "text": "Kullback-Leibler Divergence\nOne way to formalize divergence: how much additional entropy (uncertainty) is introduced by using a model \\(q\\) instead of the true target \\(p\\)?\n\\[D_{KL}(p, q) = \\sum_i p_i (\\log(p_i) - \\log(q_i)) = \\sum_i p_i \\log\\left(\\frac{p_i}{q_i}\\right)\\]\nThus the “divergence” (intuitively: distance) between two distributions is the average difference in log-probabilities between the target \\(p\\) and the model \\(q\\)."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#k-l-divergence-example",
    "href": "slides/lecture10-2-information-models.html#k-l-divergence-example",
    "title": "Information and Entropy",
    "section": "K-L Divergence Example",
    "text": "K-L Divergence Example\n\n\nSuppose the “true” probability of rain is \\(p(\\text{Rain}) = 0.65\\) and the “true” probability of \\(p(\\text{Sunshine})=0.35\\).\nWhat happens as we change \\(q(\\text{Rain})\\)?\n\n\n\nCode\np_true = [0.65, 0.35]\nq_rain = 0.01:0.01:0.99\n\nfunction kl_divergence_2outcome(p, q)\n    div_diff = log.(p) .- log.(q)\n    return sum(p .* div_diff)\nend\n\nkl_out(q) = kl_divergence_2outcome(p_true, [q, 1 - q])\nplot(q_rain, kl_out.(q_rain), lw=3, label=false, ylabel=\"Kullback-Leibler Divergence\", xlabel=L\"$q(\\textrm{Rain})$\")\nvline!([p_true[1]], lw=3, color=:red, linestyle=:dash, label=\"True Probability\")\nplot!(size=(600, 550))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Example of K-L Divergence"
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#k-l-divergence-is-not-symmetric",
    "href": "slides/lecture10-2-information-models.html#k-l-divergence-is-not-symmetric",
    "title": "Information and Entropy",
    "section": "K-L Divergence Is Not Symmetric",
    "text": "K-L Divergence Is Not Symmetric\n\n\n\n\nCode\np = MixtureModel(Normal, [(-1, 0.25), (1, 0.25)], [0.5, 0.5])\npsamp = rand(p, 100_000)\nq = Normal(0, 1)\nqsamp = rand(q, 100_000)\n\n# Monte Carlo estimation of K-L Divergence\nkl_pq = mean(logpdf.(p, psamp) .- logpdf.(q, psamp))\nkl_qp = mean(logpdf.(q, qsamp) .- logpdf.(p, qsamp))\np1 = density(psamp, lw=3, label=L\"$p$\", color=:blue, xlabel=L\"$x$\", ylabel=\"Density\")\ndensity!(p1, qsamp, lw=3, label=L\"$q$\", color=:black, linestyle=:dash)\nplot!(p1, size=(500, 300))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Lack of symmetry of the K-L Divergence\n\n\n\n\n\\(D_{KL}(p, q) =\\) 0.72\n\\(D_{KL}(q, p) =\\) 2.02\n\n\n\nCode\np = MixtureModel(Normal, [(-1, 0.25), (1, 0.25)], [0.5, 0.5])\npsamp = rand(p, 100_000)\nq = Normal(-1, 0.25)\nqsamp = rand(q, 100_000)\n\n# Monte Carlo estimation of K-L Divergence\nkl_pq = mean(logpdf.(p, psamp) .- logpdf.(q, psamp))\nkl_qp = mean(logpdf.(q, qsamp) .- logpdf.(p, qsamp))\np2 = density(psamp, lw=3, label=L\"$p$\", color=:blue, xlabel=L\"$x$\", ylabel=\"Density\")\ndensity!(p2, qsamp, lw=3, label=L\"$q$\", color=:black, linestyle=:dash)\nplot!(p2, size=(500, 300))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Lack of symmetry of the K-L Divergence\n\n\n\n\n\\(D_{KL}(p, q) =\\) 15.26\n\\(D_{KL}(q, p) =\\) 0.69"
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#divergence-for-model-comparison",
    "href": "slides/lecture10-2-information-models.html#divergence-for-model-comparison",
    "title": "Information and Entropy",
    "section": "Divergence for Model Comparison",
    "text": "Divergence for Model Comparison\nSo Far: Divergence as measure of lack of accuracy.\nBut we never know the “true” distribution of outcomes.\nIt turns out we rarely need to do this: no model will be “right,” so we’re often interested in comparing two candidate models \\(q\\) and \\(r\\)."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#divergence-for-model-comparison-1",
    "href": "slides/lecture10-2-information-models.html#divergence-for-model-comparison-1",
    "title": "Information and Entropy",
    "section": "Divergence for Model Comparison",
    "text": "Divergence for Model Comparison\nWhat is the difference between \\(D_{KL}(p, q)\\) and \\(D_{KL}(p, r)\\)?\n\\[\nD_{KL}(p, q) - D_{KL}(p, r) = \\sum_i p_i (\\log(r_i) - \\log(q_i))\n\\]\nWe don’t know \\(p_i\\), but comparing the logarithmic scores \\(S(r) = \\sum_i \\log(r_i)\\) and \\(S(q) = \\sum_i \\log(q_i)\\) gives us an approximation."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#deviance-scale",
    "href": "slides/lecture10-2-information-models.html#deviance-scale",
    "title": "Information and Entropy",
    "section": "Deviance Scale",
    "text": "Deviance Scale\nThis log-predictive-density score is better when larger.\nCommon to see this converted to deviance, which is \\(-2\\text{lppd}\\):\n\n\\(-1\\) reorients so smaller is better;\nMultiplied by \\(2\\) for historical reasons (cancels out the -1/2 in the Gaussian likelihood)."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#importance-of-holding-out-data",
    "href": "slides/lecture10-2-information-models.html#importance-of-holding-out-data",
    "title": "Information and Entropy",
    "section": "Importance of Holding Out Data",
    "text": "Importance of Holding Out Data\n\nTraining data contains information about those predictions (reducing cross-entropy).\nTesting model on training data therefore isn’t a test of “pure” prediction."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#deviance-and-degrees-of-freedom",
    "href": "slides/lecture10-2-information-models.html#deviance-and-degrees-of-freedom",
    "title": "Information and Entropy",
    "section": "Deviance and Degrees of Freedom",
    "text": "Deviance and Degrees of Freedom\n\nCode\nfunction calculate_deviance(y_gen, x_gen, y_oos, x_oos; degree=2, reg=Inf)\n    # fit model\n    lb = [-5 .+ zeros(degree); 0.01]\n    ub = [5 .+ zeros(degree); 10.0]\n    p0 = [zeros(degree); 5.0]\n\n    function model_loglik(p, y, x, reg)\n        if mean(p[1:end-1]) &lt;= reg\n            ll = 0\n            for i = 1:length(y)\n                μ = sum(x[i, 1:degree] .* p[1:end-1])\n                ll += logpdf(Normal(μ, p[end]), y[i])\n            end\n        else\n            ll = -Inf\n        end\n        return ll\n    end\n\n    result = optimize(p -&gt; -model_loglik(p, y_gen, x_gen, reg), lb, ub, p0)\n    θ = result.minimizer\n\n    function deviance(p, y_pred, x_pred)\n        dev = 0\n        for i = 1:length(y_pred)\n            μ = sum(x_pred[i, 1:degree] .* p[1:end-1])\n            dev += -2 * logpdf(Normal(μ, p[end]), y_pred[i])\n        end\n        return dev\n    end\n\n    dev_is = deviance(θ, y_gen, x_gen)\n    dev_oos = deviance(θ, y_oos, x_oos)\n    return (dev_is, dev_oos)\nend\n\n\nN_sim = 10_000\n\nfunction simulate_deviance(N_sim, N_data; reg=Inf)\n    d_is = zeros(5, 4)\n    for d = 1:5\n        dev_out = zeros(N_sim, 2)\n        for n = 1:N_sim\n            x_gen = rand(Uniform(-2, 2), (N_data, 5))\n            μ_gen = [0.1 * x_gen[i, 1] - 0.3 * x_gen[i, 2] for i in 1:N_data]\n            y_gen = rand.(Normal.(μ_gen, 1))\n\n            x_oos = rand(Uniform(-2, 2), (N_data, 5))\n            μ_oos = [0.1 * x_oos[i, 1] - 0.3 * x_oos[i, 2] for i in 1:N_data]\n            y_oos = rand.(Normal.(μ_oos, 1))\n\n            dev_out[n, :] .= calculate_deviance(y_gen, x_gen, y_oos, x_oos, degree=d, reg=reg)\n        end\n        d_is[d, 1] = mean(dev_out[:, 1])\n        d_is[d, 2] = std(dev_out[:, 1])\n        d_is[d, 3] = mean(dev_out[:, 2])\n        d_is[d, 4] = std(dev_out[:, 2])\n    end\n    return d_is\nend\n\nd_20 = simulate_deviance(N_sim, 20)\np1 = scatter(collect(1:5) .- 0.1, d_20[:, 1], yerr=d_20[:, 2], color=:blue, linecolor=:blue, lw=2, markersize=5, label=\"In Sample\", xlabel=\"Number of Predictors\", ylabel=\"Deviance\", title=L\"$N = 20$\")\nscatter!(p1, collect(1:5) .+ 0.1, d_20[:, 3], yerr=d_20[:, 4], lw=2, markersize=5, color=:black, linecolor=:black, label=\"Out of Sample\")\nplot!(p1, size=(600, 550))\n\nd_100 = simulate_deviance(N_sim, 100)\np2 = scatter(collect(1:5) .- 0.1, d_100[:, 1], yerr=d_100[:, 2], color=:blue, linecolor=:blue, lw=2, markersize=5, label=\"In Sample\", xlabel=\"Number of Predictors\", ylabel=\"Deviance\", title=L\"$N = 100$\")\nscatter!(p2, collect(1:5) .+ 0.1, d_100[:, 3], yerr=d_100[:, 4], lw=2, markersize=5, color=:black, linecolor=:black, label=\"Out of Sample\")\nplot!(p2, size=(600, 550))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Simulation of in vs. out of sample deviance calculations with increasing number of simulations\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#key-points",
    "href": "slides/lecture10-2-information-models.html#key-points",
    "title": "Information and Entropy",
    "section": "Key Points",
    "text": "Key Points\n\nInformation entropy as measure of uncertainty.\nKullback-Leiber Divergence: measure of “distance” between two distributions.\nDifference between K-L divergences lets us compare predictive skill of two models even without knowing “true” probabilities of events.\nDeviance: Multiply log-predictive score by -2."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#next-classes",
    "href": "slides/lecture10-2-information-models.html#next-classes",
    "title": "Information and Entropy",
    "section": "Next Classes",
    "text": "Next Classes\nNext Week: Spring Break\nWeek After: Information Criteria for Model Comparison\nRest of Semester: Specific topics useful for environmental data analysis (extremes, missing data, etc)."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#assessments",
    "href": "slides/lecture10-2-information-models.html#assessments",
    "title": "Information and Entropy",
    "section": "Assessments",
    "text": "Assessments\nHW4: Due on 4/11 at 9pm."
  },
  {
    "objectID": "slides/lecture10-2-information-models.html#references-scroll-for-full-list",
    "href": "slides/lecture10-2-information-models.html#references-scroll-for-full-list",
    "title": "Information and Entropy",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nGneiting, T., Fadoua Balabdaoui, & Raftery, A. E. (2007). Probabilistic Forecasts, Calibration and Sharpness. J. R. Stat. Soc. Series B Stat. Methodol., 69, 243–268. Retrieved from http://www.jstor.org/stable/4623266\n\n\nShannon, C. E. (1948). A mathematical theory of communication. Bell Syst. Tech. J., 27, 379–423. Retrieved from https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#bayesian-statistics",
    "href": "slides/lecture06-2-random-simulation.html#bayesian-statistics",
    "title": "Simulating Random Variables",
    "section": "Bayesian Statistics",
    "text": "Bayesian Statistics\n\nFocused on conditional probability (conditioned on the data)\nBayes’ Theorem to update priors using likelihood.\nPriors can be finicky: use predictive simulations.\nOutstanding question: how do we sample from the posterior (versus just taking the MAP?)"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#why-simulate",
    "href": "slides/lecture06-2-random-simulation.html#why-simulate",
    "title": "Simulating Random Variables",
    "section": "Why Simulate?",
    "text": "Why Simulate?\n\nWe want to see implications of a probability model.\nWe want to test statistical procedures (synthetic data simulation).\nEasier than computing integrals (Monte Carlo).\nComputational efficiency (e.g. stochastic gradient descent).\n\nGenerally: Turns calculus/analytical problems into data summary problems."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#example-posterior-sampling",
    "href": "slides/lecture06-2-random-simulation.html#example-posterior-sampling",
    "title": "Simulating Random Variables",
    "section": "Example: Posterior Sampling",
    "text": "Example: Posterior Sampling\n\n\n\\(p(\\theta)\\) and \\(p(y | \\theta)\\) are often “nice” distributions, but nothing says \\[p(\\theta | y) \\propto p(\\theta) \\times p(y | \\theta)\\]\nhas to be!\n\n\n\nCode\nnormal_samples = rand(Normal(2, 0.5), 10)\n\nlik(σ) = prod(pdf(Normal.(2, σ), normal_samples))\nprior(σ) = pdf(LogNormal(log(0.25), 0.25), σ)\nposterior(σ) = lik(σ) * prior(σ)\n\nσ_range = 0.1:0.01:1\nplot(σ_range, posterior.(σ_range) / maximum(posterior.(σ_range)), color=:black, label=\"Posterior\", linewidth=3)\nplot!(σ_range, lik.(σ_range) / maximum(lik.(σ_range)), color=:blue, label=\"Likelihood\", linewidth=3, linestyle=:dash)\nplot!(σ_range, prior.(σ_range) / maximum(prior.(σ_range)), color=:orange, label=\"Prior\", linewidth=3, linestyle=:dot)\nxlabel!(L\"$\\sigma$\")\nylabel!(\"Scaled Log-Density\")\nplot!(size=(550, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Comparison of prior, posterior, and likelihood."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#built-in-sampling-functions",
    "href": "slides/lecture06-2-random-simulation.html#built-in-sampling-functions",
    "title": "Simulating Random Variables",
    "section": "Built-In Sampling Functions",
    "text": "Built-In Sampling Functions\nR: sample, rnorm, rbinom, etc.\nJulia: rand, Distributions.rand\nPython: numpy.random.rand, scipy.stats.xx.rand"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#what-are-these-functions-doing",
    "href": "slides/lecture06-2-random-simulation.html#what-are-these-functions-doing",
    "title": "Simulating Random Variables",
    "section": "What Are These Functions Doing?",
    "text": "What Are These Functions Doing?\nThink of a biased coin with probability of heads \\(\\theta\\).\nWant to obtain a Bernoulli random variable.\nWhat can we do without using a built-in Bernoulli function?"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#coin-flip-simulation",
    "href": "slides/lecture06-2-random-simulation.html#coin-flip-simulation",
    "title": "Simulating Random Variables",
    "section": "Coin Flip Simulation",
    "text": "Coin Flip Simulation\nGiven heads probability \\(\\theta\\):\n\nDraw \\(u \\sim Uniform(0, 1)\\).\nIf \\(u &lt; \\theta\\), return H.\nElse return T."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#what-about-discrete-variables",
    "href": "slides/lecture06-2-random-simulation.html#what-about-discrete-variables",
    "title": "Simulating Random Variables",
    "section": "What About Discrete Variables?",
    "text": "What About Discrete Variables?\nHow can we generalize this strategy for discrete distributions with category probabilities \\(\\theta_1, \\theta_2, \\ldots, \\theta_n\\)?"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#discrete-variable-simulation",
    "href": "slides/lecture06-2-random-simulation.html#discrete-variable-simulation",
    "title": "Simulating Random Variables",
    "section": "Discrete Variable Simulation",
    "text": "Discrete Variable Simulation\nGiven category probabilities \\(\\theta_1, \\theta_2, \\ldots, \\theta_n\\):\n\nDraw \\(u \\sim Uniform(0, 1)\\).\nIf \\(u &lt; \\theta_1\\), return 1.\nIf \\(u &lt; \\theta_1 + \\theta_2\\), return 2,\n\\(\\vdots\\)\nElse return \\(n\\)."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#generalization-quantile-transform-method",
    "href": "slides/lecture06-2-random-simulation.html#generalization-quantile-transform-method",
    "title": "Simulating Random Variables",
    "section": "Generalization: Quantile Transform Method",
    "text": "Generalization: Quantile Transform Method\nGiven \\(U \\sim Uniform(0, 1)\\), target CDF \\(F\\), \\(X = F^{-1}(U)\\) has CDF \\(F\\).\nWhy?\n\\[\\mathbb{P}(X \\leq a) = \\mathbb{P}(F^{-1}(U) \\leq a) = \\mathbb{P}(U \\leq F(a)) = F(a)\\]\n\nIn other words, if we can generate uniform variables and calculate quantiles, can generate non-uniform variables."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#problem-solvedright",
    "href": "slides/lecture06-2-random-simulation.html#problem-solvedright",
    "title": "Simulating Random Variables",
    "section": "Problem Solved…Right?",
    "text": "Problem Solved…Right?\n\nOften don’t have quantile functions in closed form.\nThey also often don’t have nice numerical solutions."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#using-the-pdf",
    "href": "slides/lecture06-2-random-simulation.html#using-the-pdf",
    "title": "Simulating Random Variables",
    "section": "Using the PDF",
    "text": "Using the PDF\n\n\nSuppose the PDF \\(f\\) has support on \\([a, b]\\) and \\(f(x) \\leq M\\).\nWhat could we do to sample \\(X \\sim f(\\cdot)\\)?\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Beta Distribution"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#rejection-sampling-algorithm",
    "href": "slides/lecture06-2-random-simulation.html#rejection-sampling-algorithm",
    "title": "Simulating Random Variables",
    "section": "Rejection Sampling Algorithm",
    "text": "Rejection Sampling Algorithm\nGiven pdf \\(f\\) for \\(X\\), upper bound \\(M\\) on \\(f\\) in \\([a, b]\\):\n\nSimulate uniformly: \\(y \\sim [a, b]\\), \\(u \\sim [0, 1]\\).\nIf \\(Mu &lt; f(y)\\), keep \\(y\\) as a sample of \\(X\\).\nOtherwise reject."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#rejection-sampling-visualization",
    "href": "slides/lecture06-2-random-simulation.html#rejection-sampling-visualization",
    "title": "Simulating Random Variables",
    "section": "Rejection Sampling Visualization",
    "text": "Rejection Sampling Visualization\n\n\nFigure 3: Example of rejection sampling for a Beta distribution"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#rejection-sampling-efficiency",
    "href": "slides/lecture06-2-random-simulation.html#rejection-sampling-efficiency",
    "title": "Simulating Random Variables",
    "section": "Rejection Sampling Efficiency",
    "text": "Rejection Sampling Efficiency\nImportant: Only kept 27% of proposals.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Results from uniform rejection sampling\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#more-general-rejection-sampling",
    "href": "slides/lecture06-2-random-simulation.html#more-general-rejection-sampling",
    "title": "Simulating Random Variables",
    "section": "More General Rejection Sampling",
    "text": "More General Rejection Sampling\n\n\nUse a proposal density \\(g(\\cdot)\\) which “covers” target \\(f(\\cdot)\\) and is easy to sample from.\nSample from \\(g\\), reject based on \\(f\\).\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Using a t distribution as proposal density for normal."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#general-rejection-sampling-algorithm",
    "href": "slides/lecture06-2-random-simulation.html#general-rejection-sampling-algorithm",
    "title": "Simulating Random Variables",
    "section": "General Rejection Sampling Algorithm",
    "text": "General Rejection Sampling Algorithm\nSuppose \\(f(x) \\leq M g(x)\\) for some \\(1 &lt; M &lt; \\infty\\).\n\nSimulate a proposal \\(y \\sim g(x)\\) (e.g. by quantile method).\nSimulate \\(u \\sim \\text{Unif}(0, 1)\\)\nIf \\[u &lt; \\frac{f(y)}{M g(y)},\\] accept \\(y\\); otherwise reject."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#rejection-sampling-example",
    "href": "slides/lecture06-2-random-simulation.html#rejection-sampling-example",
    "title": "Simulating Random Variables",
    "section": "Rejection Sampling Example",
    "text": "Rejection Sampling Example\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Example of rejection sampling for a Normal distribution\n\n\n\n\n\nThis time we kept 35% of the proposals."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#rejection-sampling-efficiency-1",
    "href": "slides/lecture06-2-random-simulation.html#rejection-sampling-efficiency-1",
    "title": "Simulating Random Variables",
    "section": "Rejection Sampling Efficiency",
    "text": "Rejection Sampling Efficiency\n\nProbability of accepting a sample is \\(1/M\\), so the “tighter” the proposal distribution coverage the more efficient the sampler.\nNeed to be able to compute \\(M\\) and sample from the proposal.\n\nFinding a good proposal and computing \\(M\\) may not be easy (or possible) for complex distributions!"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#bimodal-rejection-sampling-example",
    "href": "slides/lecture06-2-random-simulation.html#bimodal-rejection-sampling-example",
    "title": "Simulating Random Variables",
    "section": "Bimodal Rejection Sampling Example",
    "text": "Bimodal Rejection Sampling Example\n\nCode\n# specify target distribution\nmixmod = MixtureModel(Normal, [(-1, 0.75), (1, 0.4)], [0.5, 0.5])\nx = -5:0.01:5\np1 = plot(x, pdf.(mixmod, x), lw=3, color=:red, xlabel=L\"$x$\", ylabel=\"Density\", label=\"Target\")\nplot!(p1, x, 2.5 * pdf.(Normal(0, 1.5), x), lw=3, color=:blue, label=\"Proposal (M=2.5)\")\nplot!(p1, size=(550, 500))\n\nnsamp = 10_000\nM = 2.5\nu = rand(Uniform(0, 1), nsamp)\ny = rand(Normal(0, 1.5), nsamp)\ng = pdf.(Normal(0, 1.5), y)\nf = pdf.(mixmod, y)\nkeep_samp = u .&lt; f ./ (M * g)\np2 = histogram(y[keep_samp], normalize=:pdf, xlabel=L\"$x$\", ylabel=\"Density\", label=\"Kept Samples\", legend=:topleft)\nplot!(p2, x, pdf.(mixmod, x), linewidth=3, color=:black, label=\"True Target\")\ndensity!(y[keep_samp], label=\"Sampled Density\", color=:red)\nplot!(p2, size=(550, 500))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) rejection sampling for a mixture model\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 7"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#where-do-random-samples-come-from",
    "href": "slides/lecture06-2-random-simulation.html#where-do-random-samples-come-from",
    "title": "Simulating Random Variables",
    "section": "Where Do “Random” Samples Come From?",
    "text": "Where Do “Random” Samples Come From?\n\n\nThere’s no such thing as a truly random number generator!\nNeed to generate samples in a deterministic way that have random-like properties.\n\n\n\n\nXKCD Cartoon 221\n\n\n\nSource: XKCD 221"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#pseudorandom-number-generators",
    "href": "slides/lecture06-2-random-simulation.html#pseudorandom-number-generators",
    "title": "Simulating Random Variables",
    "section": "Pseudorandom Number Generators",
    "text": "Pseudorandom Number Generators\nWe want:\n\nNumber of \\(U_i \\in [a, b] \\subset [0, 1]\\) is \\(\\propto b-a\\)\nNo correlation between successive \\(U_i\\).\nNo detectable dependencies in longer series.\n\nThere are several of these implemented in modern programming languages: typical default is the Mersenne Twister."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#example-rotations",
    "href": "slides/lecture06-2-random-simulation.html#example-rotations",
    "title": "Simulating Random Variables",
    "section": "Example: Rotations",
    "text": "Example: Rotations\n\\[U_{t+1} = U_t + \\alpha \\mod 1\\]\n\nIf \\(\\alpha \\neq k/n\\) is irrational, this never repeats (no \\(m\\) such that \\(m\\alpha = 1\\).)\nIf \\(\\alpha = k/n\\) is rational, this repeats, but with a long period for large \\(n\\)."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#prngs-and-chaos",
    "href": "slides/lecture06-2-random-simulation.html#prngs-and-chaos",
    "title": "Simulating Random Variables",
    "section": "(P)RNGs and Chaos",
    "text": "(P)RNGs and Chaos\nWe can get similar dynamics from area-preserving chaotic dynamical systems:\n\nLong periods with dense orbits (well-mixing);\nArea-perserving (uniformly distributed);\nRapid divergence of nearby points (sensitivity to initial conditions);"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#example-arnold-cat-map",
    "href": "slides/lecture06-2-random-simulation.html#example-arnold-cat-map",
    "title": "Simulating Random Variables",
    "section": "Example: Arnold Cat Map",
    "text": "Example: Arnold Cat Map\n\n\n\\[\n\\begin{align*}\n\\phi_{t+1} &= 2U_t + \\phi_t \\mod  1 \\\\\nU_{t+1} &= \\phi_t + U_t \\mod  1\n\\end{align*}\n\\]\nReport only \\((U_t)\\): get hard to predict uniformly distributed data.\n\n\n\n\nArnold Cat Map\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#cat-map-vs-mersenne-twister",
    "href": "slides/lecture06-2-random-simulation.html#cat-map-vs-mersenne-twister",
    "title": "Simulating Random Variables",
    "section": "Cat Map vs Mersenne Twister",
    "text": "Cat Map vs Mersenne Twister\n\nCode\nfunction cat_map(N)\n    U = zeros(N)\n    ϕ = zeros(N)\n    U[1], ϕ[1] = rand(Uniform(0, 1), 2)\n    for i = 2:N\n        ϕ[i] = rem(U[i-1] + 2 * ϕ[i-1], 1.0)\n        U[i] = rem(U[i-1] + ϕ[i-1], 1.0)\n    end\n    return U\nend\n\nN = 1_000\nZ = cat_map(N)\nU = rand(MersenneTwister(1), N) # Mersenne Twister is not the default in Julia, but is in other languages, so using it here by explicitly setting it as the generator\n\np1 = scatter(Z[1:end-1], Z[2:end], xlabel=L\"$U_t$\", ylabel=L\"$U_{t+1}$\", title=\"Cat Map\", label=false, size=(500, 500))\np2 = scatter(U[1:end-1], U[2:end], xlabel=L\"$U_t$\", ylabel=L\"$U_{t+1}$\", title=\"Mersenne Twister\", label=false, size=(500, 500))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Comparison of the Arnold cat map and output from the Mersenne Twister\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 8"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#random-seeds",
    "href": "slides/lecture06-2-random-simulation.html#random-seeds",
    "title": "Simulating Random Variables",
    "section": "Random Seeds",
    "text": "Random Seeds\nPseudo-random numbers are deterministic, so can be repeated. The sequence depends on a seed.\n\nIf you don’t set a seed explicitly, the computer will choose one (usually based on the date/time stamp when you execute the script or program).\nSetting a seed resets the sequence.\n\nSet seeds to ensure reproducibility."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#butbe-careful-with-seeds",
    "href": "slides/lecture06-2-random-simulation.html#butbe-careful-with-seeds",
    "title": "Simulating Random Variables",
    "section": "But…Be Careful With Seeds",
    "text": "But…Be Careful With Seeds\n\nPossible for a statistical procedure to appear to work with just one seed; test across several.\nSeeds may match, but RNG algorithm may be different across different languages/versions."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#key-points-bayesian-statistics",
    "href": "slides/lecture06-2-random-simulation.html#key-points-bayesian-statistics",
    "title": "Simulating Random Variables",
    "section": "Key Points: Bayesian Statistics",
    "text": "Key Points: Bayesian Statistics\n\nUse prior predictive simulations to refine priors.\nPriors matter less when likelihood is highly informative."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#key-points-random-numbers",
    "href": "slides/lecture06-2-random-simulation.html#key-points-random-numbers",
    "title": "Simulating Random Variables",
    "section": "Key Points: Random Numbers",
    "text": "Key Points: Random Numbers\n\nCan generate uniform distributions using pseudorandom number generators (unstable dynamical systems).\nTransform into other distributions using:\n\nQuantile method;\nRejection method.\n\nDefault functions work well; only really have to worry about any of this when sampling from “non-nice” distributions"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#next-classes",
    "href": "slides/lecture06-2-random-simulation.html#next-classes",
    "title": "Simulating Random Variables",
    "section": "Next Classes",
    "text": "Next Classes\nNext Week: Monte Carlo Simulation and the Bootstrap"
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#term-project",
    "href": "slides/lecture06-2-random-simulation.html#term-project",
    "title": "Simulating Random Variables",
    "section": "Term Project",
    "text": "Term Project\n\nCan work in groups of 1-2\nProposal due 3/21.\nMax three pages, should include background and problem statement, data overview, proposed probability models, and research plan.\nDeliverables include presentations (in class, last 2-3 sessions) and written report (during finals week)."
  },
  {
    "objectID": "slides/lecture06-2-random-simulation.html#references-scroll-for-full-list",
    "href": "slides/lecture06-2-random-simulation.html#references-scroll-for-full-list",
    "title": "Simulating Random Variables",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#monte-carlo-simulation",
    "href": "slides/lecture07-2-bootstrap.html#monte-carlo-simulation",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\n\nMonte Carlo Principle: Approximate \\[\\mathbb{E}_f[h(X)] = \\int h(x) f(x) dx \\approx \\frac{1}{n} \\sum_{i=1}^n h(X_i)\\]\nMonte Carlo is an unbiased estimator, but beware (and always report) the standard error \\(\\sigma_n = \\sigma_Y / \\sqrt{n}\\) or confidence intervals.\nMore advanced methods can reduce variance, but may be difficult to implement in practice."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#uses-of-monte-carlo",
    "href": "slides/lecture07-2-bootstrap.html#uses-of-monte-carlo",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Uses of Monte Carlo",
    "text": "Uses of Monte Carlo\n\nEstimate expectations / quantiles;\nCalculate deterministic quantities (framed as stochastic expectations);\nOptimization (problem 3 on HW3)."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#so-far",
    "href": "slides/lecture07-2-bootstrap.html#so-far",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "So Far…",
    "text": "So Far…\nWe’re 7 weeks in and haven’t said anything about how to quantify uncertainties.\nLet’s talk about that."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#sampling-distributions-1",
    "href": "slides/lecture07-2-bootstrap.html#sampling-distributions-1",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\n\nThe sampling distribution of a statistic captures the uncertainty associated with random samples.\n\n\n\n\nSampling Distribution"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#estimating-sampling-distributions",
    "href": "slides/lecture07-2-bootstrap.html#estimating-sampling-distributions",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Estimating Sampling Distributions",
    "text": "Estimating Sampling Distributions\n\nSpecial Cases: can derive closed-form representations of sampling distributions (think statistical tests)\nAsymptotics: Central Limit Theorem or Fisher Information"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#fisher-information",
    "href": "slides/lecture07-2-bootstrap.html#fisher-information",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Fisher Information",
    "text": "Fisher Information\n\n\nFisher Information: \\[\\mathcal{I}_x(\\theta) = -\\mathbb{E}\\left[\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log \\mathcal{L}(\\theta | x)\\right]\\]\nObserved Fisher Information (uses observed data and calculated at the MLE): \\[\\mathcal{I}_\\tilde{x}(\\hat{\\theta})\\]\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Examples of Fisher Information"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#fisher-information-and-standard-errors",
    "href": "slides/lecture07-2-bootstrap.html#fisher-information-and-standard-errors",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Fisher Information and Standard Errors",
    "text": "Fisher Information and Standard Errors\nAsymptotic result: \\[\\sqrt{n}(\\theta_\\text{MLE} - \\theta^*) \\to N(0, \\left(\\mathcal{I}_x(\\hat{\\theta^*})\\right)^{-1}\\]\nSampling distribution based on observed data: \\[\\theta \\sim N\\left(\\hat{\\theta}_\\text{MLE}, \\left(n\\mathcal{I}_\\tilde{x}(\\theta_\\text{MLE})\\right)^{-1}\\right)\\]"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#estimating-fisher-information",
    "href": "slides/lecture07-2-bootstrap.html#estimating-fisher-information",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Estimating Fisher Information",
    "text": "Estimating Fisher Information\n\nCan be done with automatic differentiation or (sometimes hard) calculations;\nMay be singular (no inverse ; undefined standard errors) for complex models;\nMay not be a good approximation of the variance for finite samples!"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#the-bootstrap-principle",
    "href": "slides/lecture07-2-bootstrap.html#the-bootstrap-principle",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "The Bootstrap Principle",
    "text": "The Bootstrap Principle\n\n\nEfron (1979) suggested combining estimation with simulation: the bootstrap.\nKey idea: use the data to simulate a data-generating mechanism.\n\n\n\n\n\nXxibit Bootstrap Meme"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#bootstrap-principle",
    "href": "slides/lecture07-2-bootstrap.html#bootstrap-principle",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Bootstrap Principle",
    "text": "Bootstrap Principle\n\n\n\nAssume the existing data is representative of the “true” population,\nSimulate based on properties of the data itself\nRe-estimate statistics from re-samples.\n\n\n\n\n\nBootstrap Sampling Distribution"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#why-does-the-bootstrap-work",
    "href": "slides/lecture07-2-bootstrap.html#why-does-the-bootstrap-work",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Why Does The Bootstrap Work?",
    "text": "Why Does The Bootstrap Work?\nEfron’s key insight: due to the Central Limit Theorem, the bootstrap distribution \\(\\mathcal{D}(\\tilde{t}_i)\\) has the same relationship to the observed estimate \\(\\hat{t}\\) as the sampling distribution \\(\\mathcal{D}(\\hat{t})\\) has to the “true” value \\(t_0\\):\n\\[\\mathcal{D}(\\tilde{t} - \\hat{t}) \\approx \\mathcal{D}(\\hat{t} - t_0)\\]\nwhere \\(t_0\\) the “true” value of a statistic, \\(\\hat{t}\\) the sample estimate, and \\((\\tilde{t}_i)\\) the bootstrap estimates."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#what-can-we-do-with-the-bootstrap",
    "href": "slides/lecture07-2-bootstrap.html#what-can-we-do-with-the-bootstrap",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "What Can We Do With The Bootstrap?",
    "text": "What Can We Do With The Bootstrap?\nLet \\(t_0\\) the “true” value of a statistic, \\(\\hat{t}\\) the estimate of the statistic from the sample, and \\((\\tilde{t}_i)\\) the bootstrap estimates.\n\nEstimate Variance: \\(\\text{Var}[\\hat{t}] \\approx \\text{Var}[\\tilde{t}]\\)\nBias Correction: \\(\\mathbb{E}[\\hat{t}] - t_0 \\approx \\mathbb{E}[\\tilde{t}] - \\hat{t}\\)\n\n\nNotice that bias correction “shifts” away from the bootstrapped samples."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#simple-bootstrap-confidence-intervals",
    "href": "slides/lecture07-2-bootstrap.html#simple-bootstrap-confidence-intervals",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "“Simple” Bootstrap Confidence Intervals",
    "text": "“Simple” Bootstrap Confidence Intervals\n\nBasic Bootstrap CIs (based on CLT for error distribution \\(\\tilde{t} - \\hat{t}\\)): \\[\\left(\\hat{t} - (Q_{\\tilde{t}}(1-\\alpha/2) - \\hat{t}), \\hat{t} - (Q_{\\tilde{t}}(\\alpha/2) - \\hat{t})\\right)\\]\nPercentile Bootstrap CIs (simplest, often wrong coverage): \\[(Q_{\\tilde{t}}(1-\\alpha/2), Q_{\\tilde{t}}(1-\\alpha/2))\\]"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#the-non-parametric-bootstrap-1",
    "href": "slides/lecture07-2-bootstrap.html#the-non-parametric-bootstrap-1",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "The Non-Parametric Bootstrap",
    "text": "The Non-Parametric Bootstrap\n\n\nThe non-parametric bootstrap is the most “naive” approach to the bootstrap: resample-then-estimate.\n\n\n\n\nNon-Parametric Bootstrap"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#sources-of-non-parametric-bootstrap-error",
    "href": "slides/lecture07-2-bootstrap.html#sources-of-non-parametric-bootstrap-error",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Sources of Non-Parametric Bootstrap Error",
    "text": "Sources of Non-Parametric Bootstrap Error\n\nSampling error: error from using finitely many replications\nStatistical error: error in the bootstrap sampling distribution approximation"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#simple-example-is-a-coin-fair",
    "href": "slides/lecture07-2-bootstrap.html#simple-example-is-a-coin-fair",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Simple Example: Is A Coin Fair?",
    "text": "Simple Example: Is A Coin Fair?\nSuppose we have observed twenty flips with a coin, and want to know if it is weighted.\n\n\nCode\n# define coin-flip model\np_true = 0.6\nn_flips = 20\ncoin_dist = Bernoulli(p_true)\n# generate data set\ndat = rand(coin_dist, n_flips)\nfreq_dat = sum(dat) / length(dat)\ndat'\n\n\n1×20 adjoint(::Vector{Bool}) with eltype Bool:\n 1  1  0  0  0  1  0  0  0  1  1  1  1  1  1  0  1  1  1  1\n\n\nThe frequency of heads is 0.65."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#is-the-coin-fair",
    "href": "slides/lecture07-2-bootstrap.html#is-the-coin-fair",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Is The Coin Fair?",
    "text": "Is The Coin Fair?\n\n\nCode\n# bootstrap: draw new samples\nfunction coin_boot_sample(dat)\n    boot_sample = sample(dat, length(dat); replace=true)\n    return boot_sample\nend\n\nfunction coin_boot_freq(dat, nsamp)\n    boot_freq = [sum(coin_boot_sample(dat)) for _ in 1:nsamp]\n    return boot_freq / length(dat)\nend\n\nboot_out = coin_boot_freq(dat, 1000)\nq_boot = 2 * freq_dat .- quantile(boot_out, [0.975, 0.025])\n\np = histogram(boot_out, xlabel=\"Heads Frequency\", ylabel=\"Count\", title=\"1000 Bootstrap Samples\", label=false, right_margin=5mm)\nvline!(p, [p_true], linewidth=3, color=:orange, linestyle=:dash, label=\"True Probability\")\nvline!(p, [mean(boot_out) ], linewidth=3, color=:red, linestyle=:dash, label=\"Bootstrap Mean\")\nvline!(p, [freq_dat], linewidth=3, color=:purple, linestyle=:dot, label=\"Observed Frequency\")\nvspan!(p, q_boot, linecolor=:grey, fillcolor=:grey, alpha=0.3, fillalpha=0.3, label=\"95% CI\")\nplot!(p, size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Bootstrap heads frequencies for 20 resamples."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#larger-sample-example",
    "href": "slides/lecture07-2-bootstrap.html#larger-sample-example",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Larger Sample Example",
    "text": "Larger Sample Example\n\n\nCode\nn_flips = 50\ndat = rand(coin_dist, n_flips)\nfreq_dat = sum(dat) / length(dat)\n\nboot_out = coin_boot_freq(dat, 1000)\nq_boot = 2 * freq_dat .- quantile(boot_out, [0.975, 0.025])\n\np = histogram(boot_out, xlabel=\"Heads Frequency\", ylabel=\"Count\", title=\"1000 Bootstrap Samples\", titlefontsize=20, guidefontsize=18, tickfontsize=16, legendfontsize=16, label=false, bottom_margin=7mm, left_margin=5mm, right_margin=5mm)\nvline!(p, [p_true], linewidth=3, color=:orange, linestyle=:dash, label=\"True Probability\")\nvline!(p, [mean(boot_out) ], linewidth=3, color=:red, linestyle=:dash, label=\"Bootstrap Mean\")\nvline!(p, [freq_dat], linewidth=3, color=:purple, linestyle=:dot, label=\"Observed Frequency\")\nvspan!(p, q_boot, linecolor=:grey, fillcolor=:grey, alpha=0.3, fillalpha=0.3, label=\"95% CI\")\nplot!(p, size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Bootstrap heads frequencies for 1000 resamples."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#why-use-the-non-parametric-bootstrap",
    "href": "slides/lecture07-2-bootstrap.html#why-use-the-non-parametric-bootstrap",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Why Use The Non-Parametric Bootstrap?",
    "text": "Why Use The Non-Parametric Bootstrap?\n\nDo not need to rely on variance asymptotics;\nCan obtain non-symmetric CIs.\nEmbarrassingly parallel to simulate new replicates and generate statistics."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#when-cant-you-use-the-non-parametric-bootstrap",
    "href": "slides/lecture07-2-bootstrap.html#when-cant-you-use-the-non-parametric-bootstrap",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "When Can’t You Use The Non-Parametric Bootstrap",
    "text": "When Can’t You Use The Non-Parametric Bootstrap\n\nMaxima/minima\nVery extreme values.\n\nGenerally, anything very sensitive to outliers which might not be re-sampled."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#bootstrapping-with-structured-data",
    "href": "slides/lecture07-2-bootstrap.html#bootstrapping-with-structured-data",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Bootstrapping with Structured Data",
    "text": "Bootstrapping with Structured Data\nThe naive non-parametric bootstrap that we just saw doesn’t work if data has structure, e.g. spatial or temporal dependence."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#simple-bootstrapping-fails-with-structured-data",
    "href": "slides/lecture07-2-bootstrap.html#simple-bootstrapping-fails-with-structured-data",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Simple Bootstrapping Fails with Structured Data",
    "text": "Simple Bootstrapping Fails with Structured Data\n\nCode\ntide_dat = CSV.read(joinpath(\"data\", \"surge\", \"norfolk-hourly-surge-2015.csv\"), DataFrame)\nsurge_resids = tide_dat[:, 5] - tide_dat[:, 3]\n\np1 = plot(surge_resids, xlabel=\"Hour\", ylabel=\"(m)\", title=\"Tide Gauge Residuals\", label=:false, linewidth=3)\nplot!(p1, size=(600, 450))\n\nresample_index = sample(1:length(surge_resids), length(surge_resids); replace=true)\np2 = plot(surge_resids[resample_index], xlabel=\"Hour\", ylabel=\"(m)\", title=\"Tide Gauge Resample\", label=:false, linewidth=3)\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Simple bootstrap with time series data.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#block-bootstraps",
    "href": "slides/lecture07-2-bootstrap.html#block-bootstraps",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Block Bootstraps",
    "text": "Block Bootstraps\nClever idea from Kunsch (1989): Divide time series \\(y_{1:T}\\) into overlapping blocks of length \\(k\\).\n\\[\\{y_{1:k}, y_{2:k+1}, \\ldots y_{n-k+1:n}\\}\\]\nThen draw \\(m = n / k\\) of these blocks with replacement and construct replicate time series:\n\\[\\hat{y}_{1:n} = (y_{b_1}, \\ldots, y_{b_m}) \\]\nNote: Your series must not have a trend!"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#block-bootstrap-example",
    "href": "slides/lecture07-2-bootstrap.html#block-bootstrap-example",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Block Bootstrap Example",
    "text": "Block Bootstrap Example\n\n\n\n\nCode\nk = 20\nn_blocks = length(surge_resids) - k + 1\nblocks = zeros(Float64, (k, n_blocks))\nfor i = 1:n_blocks\n    blocks[:, i] = surge_resids[i:(k+i-1)]\nend\nblocks[:, 1:5]\n\n\n20×5 Matrix{Float64}:\n  0.136   0.107   0.097   0.093   0.093\n  0.107   0.097   0.093   0.093   0.08\n  0.097   0.093   0.093   0.08    0.068\n  0.093   0.093   0.08    0.068   0.031\n  0.093   0.08    0.068   0.031   0.005\n  0.08    0.068   0.031   0.005   0.011\n  0.068   0.031   0.005   0.011   0.012\n  0.031   0.005   0.011   0.012   0.001\n  0.005   0.011   0.012   0.001  -0.002\n  0.011   0.012   0.001  -0.002  -0.014\n  0.012   0.001  -0.002  -0.014  -0.013\n  0.001  -0.002  -0.014  -0.013  -0.015\n -0.002  -0.014  -0.013  -0.015  -0.02\n -0.014  -0.013  -0.015  -0.02   -0.019\n -0.013  -0.015  -0.02   -0.019  -0.031\n -0.015  -0.02   -0.019  -0.031  -0.071\n -0.02   -0.019  -0.031  -0.071  -0.067\n -0.019  -0.031  -0.071  -0.067  -0.084\n -0.031  -0.071  -0.067  -0.084  -0.086\n -0.071  -0.067  -0.084  -0.086  -0.092\n\n\n\n\n\nCode\nm = Int64(ceil(length(surge_resids) / k))\nn_boot = 1_000\nsurge_bootstrap = zeros(length(surge_resids), n_boot)\nfor i = 1:n_boot\n    block_sample_idx = sample(1:n_blocks, m; replace=true)\n    surge_bootstrap[:, i] = reduce(vcat, blocks[:, block_sample_idx])\nend\n\np = plot(surge_resids, color=:black, lw=3, label=\"Data\", xlabel=\"Hour\", ylabel=\"(m)\", title=\"Tide Gauge Residuals\", alpha=0.5)\nplot!(p, surge_bootstrap[:, 1], color=:blue, lw=3, label=\"Replicate\", alpha=0.5)\nplot!(p, size=(600, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Comparison of original data with block bootstrap replicate."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#block-bootstrap-replicates",
    "href": "slides/lecture07-2-bootstrap.html#block-bootstrap-replicates",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Block Bootstrap Replicates",
    "text": "Block Bootstrap Replicates\n\n\nCode\np = plot(xlabel=\"Hour\", ylabel=\"(m)\", title=\"Tide Gauge Residuals\")\nfor i = 1:10\n    label = i == 1 ? \"Replicate\" : false\n    plot!(p, surge_bootstrap[:, i], label=label, color=:gray, alpha=0.2, lw=2)\nend\nplot!(p, surge_resids, label=\"Data\", color=:black, lw=3)\nplot!(p, size=(1200, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Block bootstrap with time series data."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#generalizing-the-block-bootstrap",
    "href": "slides/lecture07-2-bootstrap.html#generalizing-the-block-bootstrap",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Generalizing the Block Bootstrap",
    "text": "Generalizing the Block Bootstrap\n\nCircular Bootstrap: “Wrap” the time series in a circle, \\(y_1, y_2, \\ldots, y_n, y_1, y_2, \\ldots\\) then divide into blocks and resample.\nStationary Bootstrap: Use random block lengths to avoid systematic boundary transitions.\nBlock of Blocks Bootstrap: Divide series into blocks of length \\(k_2\\), then subdivide into blocks of length \\(k_1\\). Sample blocks with replacement then sample sub-blocks within each block with replacement."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#key-points",
    "href": "slides/lecture07-2-bootstrap.html#key-points",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Key Points",
    "text": "Key Points\n\nBootstrap: Approximate sampling distribution by re-simulating data\nNon-Parametric Bootstrap: Treat data as representative of population and re-sample.\nMore complicated for structured data: block bootstrap for time series, analogues for spatial data."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#sources-of-non-parametric-bootstrap-error-1",
    "href": "slides/lecture07-2-bootstrap.html#sources-of-non-parametric-bootstrap-error-1",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Sources of Non-Parametric Bootstrap Error",
    "text": "Sources of Non-Parametric Bootstrap Error\n\nSampling error: error from using finitely many replications\nStatistical error: error in the bootstrap sampling distribution approximation"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#when-to-use-the-non-parametric-bootstrap",
    "href": "slides/lecture07-2-bootstrap.html#when-to-use-the-non-parametric-bootstrap",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "When To Use The Non-Parametric Bootstrap",
    "text": "When To Use The Non-Parametric Bootstrap\n\nSample is representative of the sampling distribution\nDoesn’t work well for extreme values!\nDoes not work at all for max/mins (or any other case where the CLT fails)."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#next-classes",
    "href": "slides/lecture07-2-bootstrap.html#next-classes",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Next Classes",
    "text": "Next Classes\nMonday: The Parametric Bootstrap\nWednesday: What is a Markov Chain?"
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#assessments",
    "href": "slides/lecture07-2-bootstrap.html#assessments",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "Assessments",
    "text": "Assessments\nHomework 3: Due next Friday (3/14)\nProject Proposals: Due 3/21."
  },
  {
    "objectID": "slides/lecture07-2-bootstrap.html#references-scroll-for-full-list",
    "href": "slides/lecture07-2-bootstrap.html#references-scroll-for-full-list",
    "title": "Uncertainty Quantification and The Bootstrap",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nEfron, B. (1979). Bootstrap methods: Another look at the jackknife. Ann. Stat., 7, 1–26. https://doi.org/10.1214/aos/1176344552\n\n\nKunsch, H. R. (1989). The jackknife and the bootstrap for general stationary observations. Ann. Stat., 17, 1217–1241. https://doi.org/10.1214/aos/1176347265"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#about-me",
    "href": "slides/lecture01-1-intro.html#about-me",
    "title": "Welcome to BEE 4850/5850!",
    "section": "About Me",
    "text": "About Me\nInstructor: Prof. Vivek Srikrishnan, viveks at cornell dot edu\nResearch Interests:\n\nBridging Earth science, data science, and decision science to improve climate risk management;\nUnintended consequences which result from neglecting uncertainty or system dynamics."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#meet-my-supervisors",
    "href": "slides/lecture01-1-intro.html#meet-my-supervisors",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Meet My Supervisors",
    "text": "Meet My Supervisors\n\n\n\n\nMy Supervisors"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#ta",
    "href": "slides/lecture01-1-intro.html#ta",
    "title": "Welcome to BEE 4850/5850!",
    "section": "TA",
    "text": "TA\nTA: Shikhar Prakash (sp868 at cornell dot edu)\nResearch Interests: Electric power systems\nOther Interests: Hiking in national parks, scuba diving, reading non fiction and sci-fi, learning local cuisines and history while traveling, and cooking"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#what-do-you-hope-to-get-out-of-this-course",
    "href": "slides/lecture01-1-intro.html#what-do-you-hope-to-get-out-of-this-course",
    "title": "Welcome to BEE 4850/5850!",
    "section": "What Do You Hope To Get Out Of This Course?",
    "text": "What Do You Hope To Get Out Of This Course?\nTake a moment, write it down, and we’ll share!"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#why-does-data-analysis-matter",
    "href": "slides/lecture01-1-intro.html#why-does-data-analysis-matter",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Why Does Data Analysis Matter?",
    "text": "Why Does Data Analysis Matter?\n\nScientific insight;\nDecision-making;\nUnderstanding uncertainty"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#examples-of-data-related-questions",
    "href": "slides/lecture01-1-intro.html#examples-of-data-related-questions",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Examples of Data-Related Questions",
    "text": "Examples of Data-Related Questions\n\nHow often will a particular (usually extreme) outcome occur?\nWhat causes differences between environmental conditions at different times/locations?\nWhat might the impacts be of environmental change?\nDoes a new (treatment/management) method have a reliable impact?"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#the-ideal",
    "href": "slides/lecture01-1-intro.html#the-ideal",
    "title": "Welcome to BEE 4850/5850!",
    "section": "The Ideal",
    "text": "The Ideal\n\n\nGoal: Obtain data that is so self-evident that you don’t need to do statistics!\n\n\n\n\nXKCD 2400\n\n\n\nSource: XKCD 2400"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#but-in-actuality",
    "href": "slides/lecture01-1-intro.html#but-in-actuality",
    "title": "Welcome to BEE 4850/5850!",
    "section": "But in Actuality",
    "text": "But in Actuality\n\nExcept in the most controlled experiments, data are noisy;\nThe causes of the data cannot be extracted from the data alone;\nThe reasons for statistical analyses are not found in the data themselves."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#description-and-inference-are-connected",
    "href": "slides/lecture01-1-intro.html#description-and-inference-are-connected",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Description and Inference are Connected",
    "text": "Description and Inference are Connected\n\n\nExcept for superficial tasks:\n\nKnowing what is important to describe requires a model;\nUnderstanding how the data might differ from the population requires a model.\n\n\n\n\n\n\nSpidermen Meme"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#some-problems-with-the-standard-data-analysis-toolkit",
    "href": "slides/lecture01-1-intro.html#some-problems-with-the-standard-data-analysis-toolkit",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Some Problems With The “Standard” Data Analysis Toolkit",
    "text": "Some Problems With The “Standard” Data Analysis Toolkit\n\nStatistical assumptions may not be valid;\n“Null” vs “Alternative” hypotheses and tests may be chosen for computational convenience, not scientific relevance.\nMany different substantive models can imply the same statistical model.\n\n\nImportant: “Big” data doesn’t solve the problem!"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#model-based-data-analysis",
    "href": "slides/lecture01-1-intro.html#model-based-data-analysis",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Model-Based Data Analysis",
    "text": "Model-Based Data Analysis\n\n\nModels are how we assess evidence for theories.\nModels are encodings of hypothesized data-generating processes.\n\n\n\n\n\nRock Paper Scissors meme"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#what-can-we-do-with-models",
    "href": "slides/lecture01-1-intro.html#what-can-we-do-with-models",
    "title": "Welcome to BEE 4850/5850!",
    "section": "What Can We Do With Models?",
    "text": "What Can We Do With Models?\n\nInference: what are parameter values which explain data?\nSimulation: what are alternative counterfactual observations?\nProjection: what future data might be observed assuming this data-generating process?\nTesting: which data-generating processes are most consistent with the data?"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#my-philosophical-position",
    "href": "slides/lecture01-1-intro.html#my-philosophical-position",
    "title": "Welcome to BEE 4850/5850!",
    "section": "My Philosophical Position",
    "text": "My Philosophical Position\n\n\n\nStatistics is a rhetorical tool: we use stats to tell a quantitative story about data.\nProbability theory helps us deduce logical implications of theories conditional on our assumptions\nCannot use an “objective” procedure to avoid subjective responsibility\n\n\n\n\n\n\nBart Statistics Meme"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#model-based-data-analysis-1",
    "href": "slides/lecture01-1-intro.html#model-based-data-analysis-1",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Model-Based Data Analysis",
    "text": "Model-Based Data Analysis\nWe can (transparently):\n\nExamine logical implications of model assumptions (including interventions/out-of-sample generation).\nAssess evidence for multiple hypotheses by generating simulated data.\nIdentify opportunities to design future experiments or observations to distinguish between competing hypotheses."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#model-based-data-analysis-2",
    "href": "slides/lecture01-1-intro.html#model-based-data-analysis-2",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Model-Based Data Analysis",
    "text": "Model-Based Data Analysis\n\n\nModels are how we assess evidence for theories.\n\n\n\n\n\nRock Paper Scissors meme"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#what-we-will-cover",
    "href": "slides/lecture01-1-intro.html#what-we-will-cover",
    "title": "Welcome to BEE 4850/5850!",
    "section": "What We Will Cover",
    "text": "What We Will Cover\n\nExploratory Analysis\nLinear Models and GLMs\nModel Calibration, Evaluation, and Comparison\nSimulation (Monte Carlo, Bootstrap, Imputation)\nExtreme Value Analysis"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#background-knowledge-computing",
    "href": "slides/lecture01-1-intro.html#background-knowledge-computing",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Background Knowledge: Computing",
    "text": "Background Knowledge: Computing\n\nBasics (at the level of CS 111x)\nNo specific language requirement.\nSome extra work/effort may be needed if you haven’t coded in a while.\nMay need some additional familiarity with statistical packages (and “light” optimization)"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#background-knowledge-probabilitystatistics",
    "href": "slides/lecture01-1-intro.html#background-knowledge-probabilitystatistics",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Background Knowledge: Probability/Statistics",
    "text": "Background Knowledge: Probability/Statistics\n\nENGRD 2700/CEE 3040\nSummary statistics of data\nProbability distributions\nBasic visualizations\nMonte Carlo basics"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#grades",
    "href": "slides/lecture01-1-intro.html#grades",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Grades",
    "text": "Grades\n\n\n\nAssessment\nWeight\n\n\n\n\nExercises\n5%\n\n\nQuizzes\n25%\n\n\nReadings\n10%\n\n\nHomework Assignments\n30%\n\n\nTerm Project\n30%"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#overall-guidelines",
    "href": "slides/lecture01-1-intro.html#overall-guidelines",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Overall Guidelines",
    "text": "Overall Guidelines\n\nCollaboration highly encouraged, but all work must reflect your own understanding\nSubmit PDFs on Gradescope\n50% penalty for late submission (up to 24 hours)\nStandard rubric available on website\nAlways cite external references"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#exercises",
    "href": "slides/lecture01-1-intro.html#exercises",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Exercises",
    "text": "Exercises\n\nOn Gradescope\nSimple, mostly multiple choice problems, autograded\nCan submit as often as you want, intended to reinforce concepts."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#quizzes",
    "href": "slides/lecture01-1-intro.html#quizzes",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Quizzes",
    "text": "Quizzes\n\n5 throughout the semester\nBeginning of a Friday class, end of class will motivate/introduce new material.\nClosed book, closed note, no calculators.\nWill be scanned into Gradescope for grading/returns."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#readings",
    "href": "slides/lecture01-1-intro.html#readings",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Readings",
    "text": "Readings\n\nSeveral readings assigned for discussion throughout the semester.\nAnnotation assignments on Canvas through Perusall: by start of next Monday’s lecture.\nAnnotations can discuss aspects of the reading you think are interesting, had questions on, or can engage with other students’ annotations.\nWill be graded on level of engagement, not quantity of annotations."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#literature-critique",
    "href": "slides/lecture01-1-intro.html#literature-critique",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Literature Critique",
    "text": "Literature Critique\n\nFor 5850 students\nSelect a paper which involves some type of statistical or data analysis\nCritique choices: do they support the scientific conclusions?\nSubmit a 2-3 page writeup at the end of the semester\nIf you’re unsure where to look for a paper, talk to Prof. Srikrishnan"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#homework-assignments",
    "href": "slides/lecture01-1-intro.html#homework-assignments",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Homework Assignments",
    "text": "Homework Assignments\n\nMore in-depth problems, mostly computational\nRoughly 2 weeks to complete\nRegrade requests must be made within one week"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#term-project",
    "href": "slides/lecture01-1-intro.html#term-project",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Term Project",
    "text": "Term Project\n\nAnalyze a question of interest using a data set of your choosing\nCan work individually or groups of 2\nProposal due in March, interim update in April, final report during finals week."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#attendance",
    "href": "slides/lecture01-1-intro.html#attendance",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Attendance",
    "text": "Attendance\nNot required, but students tend to do better when they’re actively engaged in class."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#office-hours",
    "href": "slides/lecture01-1-intro.html#office-hours",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Office Hours",
    "text": "Office Hours\n\nInstructor: MWTh 10-11 AM, 318 Riley-Robb\nTA: TBD\nAlmost impossible to find a time that works for all (or even most); please feel free to make appointments as/if needed. Usually also have time after class (perhaps not all Mondays)."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#accomodations",
    "href": "slides/lecture01-1-intro.html#accomodations",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Accomodations",
    "text": "Accomodations\nIf you have any access barriers in this class, please seek out any helpful accomodations.\n\nGet an SDS letter.\nIf you need an accomodation before you have an official letter, please reach out to me ASAP!"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#academic-integrity",
    "href": "slides/lecture01-1-intro.html#academic-integrity",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nHopefully not a concern…\n\nCollaboration is great and is encouraged!\nKnowing how to find and use helpful resources is a skill we want to develop.\nDon’t just copy…learn from others and give credit.\nSubmit your own original work."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#academic-integrity-1",
    "href": "slides/lecture01-1-intro.html#academic-integrity-1",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nObviously, just copying down answers from Chegg or ChatGPT and passing them off as your own is not ok."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#llms-bullshit-generators",
    "href": "slides/lecture01-1-intro.html#llms-bullshit-generators",
    "title": "Welcome to BEE 4850/5850!",
    "section": "LLMs: Bullshit Generators",
    "text": "LLMs: Bullshit Generators\nThink about ChatGPT as a drunk who tells stories for drinks.\nIt will give you plausible-looking text or code on any topic, but it doesn’t know anything beyond what it “overheard.”\nChatGPT can be useful for certain tasks (e.g. understanding code errors), but may neglect context for why/when certain information or solutions work."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#chatgpt-the-stochastic-parrot",
    "href": "slides/lecture01-1-intro.html#chatgpt-the-stochastic-parrot",
    "title": "Welcome to BEE 4850/5850!",
    "section": "ChatGPT: The Stochastic Parrot",
    "text": "ChatGPT: The Stochastic Parrot\nMust specifically call out where you used ChatGPT in your work (beyond simple referencing; see syllabus for details)."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#communications",
    "href": "slides/lecture01-1-intro.html#communications",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Communications",
    "text": "Communications\nUse Ed Discussion for questions and discussions about class, homework assignments, etc.\n\nTry to use public posts so others can benefit from questions and can weigh in.\nI will make announcements through Ed."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#email",
    "href": "slides/lecture01-1-intro.html#email",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Email",
    "text": "Email\nWhen urgency or privacy is required, email is ok.\n\n\n\n\n\n\nImportant\n\n\nPlease include BEE4850 in your email subject line! This will ensure it doesn’t get lost in the shuffle.\nBetter: Use Ed Discussion and reserve email for matters that are particular urgent and/or require privacy."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#course-website",
    "href": "slides/lecture01-1-intro.html#course-website",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Course Website",
    "text": "Course Website\nhttps://envdata.viveks.me/spring2026/\n\nCentral hub for information, schedule, and policies\nWill add link and some information to Canvas (assignment due dates, etc)"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#computing-tools",
    "href": "slides/lecture01-1-intro.html#computing-tools",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Computing Tools",
    "text": "Computing Tools\n\nCourse is programming language-agnostic.\nAssignments will have notebooks set up for Julia (environments, etc) on GitHub."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#some-tips-for-success",
    "href": "slides/lecture01-1-intro.html#some-tips-for-success",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Some Tips For Success",
    "text": "Some Tips For Success\n\nStart the homeworks early; this gives time to sort out conceptual problems and debug.\nAsk questions (in class and online) and try to help each other.\nGive me feedback!"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#next-classes",
    "href": "slides/lecture01-1-intro.html#next-classes",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Next Classes",
    "text": "Next Classes\nFriday: Probability Review\nNext Week: Exploratory analysis and visualization."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#assessments",
    "href": "slides/lecture01-1-intro.html#assessments",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 available; due Friday, 2/6."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#references-scroll-for-full-list",
    "href": "slides/lecture01-1-intro.html#references-scroll-for-full-list",
    "title": "Welcome to BEE 4850/5850!",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#markov-chain-monte-carlo",
    "href": "slides/lecture09-2-model-skill.html#markov-chain-monte-carlo",
    "title": "Predictive Model Assessments",
    "section": "Markov Chain Monte Carlo",
    "text": "Markov Chain Monte Carlo\n\nFamily of methods for simulating from hard-to-sample from distributions \\(\\pi\\);\nRely on ergodic Markov chains for simulation;\nBy construction, chains converge to limiting distribution which is the target distribution \\(\\pi\\)."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#markov-chain-monte-carlo-convergence",
    "href": "slides/lecture09-2-model-skill.html#markov-chain-monte-carlo-convergence",
    "title": "Predictive Model Assessments",
    "section": "Markov Chain Monte Carlo (Convergence)",
    "text": "Markov Chain Monte Carlo (Convergence)\n\nAssessment of convergence relies on heuristics.\nExamples: Stabilility of distribution, \\(\\hat{R}\\).\nPoor mixing can result in slow convergence; look at ESS.\nFolk Theorem of Statistical Computing (Gelman): Poor performance likely means something is wrong with your model."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#what-are-models-for",
    "href": "slides/lecture09-2-model-skill.html#what-are-models-for",
    "title": "Predictive Model Assessments",
    "section": "What Are Models For?",
    "text": "What Are Models For?\n\n\n\nData summaries;\nPredictors of new data;\nSimulators of counterfactuals (causal analysis).\n\n\n\n\n\n\nCausality Correlation Metric\n\n\n\nSource: Richard McElreath"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#impact-of-increasing-model-complexity",
    "href": "slides/lecture09-2-model-skill.html#impact-of-increasing-model-complexity",
    "title": "Predictive Model Assessments",
    "section": "Impact of Increasing Model Complexity",
    "text": "Impact of Increasing Model Complexity\n\n\nCode\nntrain = 20\nx = rand(Uniform(-2, 2), ntrain)\nf(x) = x.^3 .- 5x.^2 .+ 1\ny = f(x) + rand(Normal(0, 2), length(x))\np0 = scatter(x, y, label=\"Data\", markersize=5)\nxrange = -2:0.01:2\nplot!(p0, xrange, f.(xrange), lw=3, color=:gray, label=\"True Curve\")\nplot!(p0, size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: True data and the data-generating curve."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#impact-of-increasing-model-complexity-1",
    "href": "slides/lecture09-2-model-skill.html#impact-of-increasing-model-complexity-1",
    "title": "Predictive Model Assessments",
    "section": "Impact of Increasing Model Complexity",
    "text": "Impact of Increasing Model Complexity\n\nCode\nfunction polyfit(d, x, y)\n    function m(d, θ, x)\n        mout = zeros(length(x), d + 1)\n        for j in eachindex(x)\n            for i = 0:d\n                mout[j, i + 1] = θ[i + 1] * x[j]^i\n            end\n        end\n        return sum(mout; dims=2)\n    end\n    θ₀ = [zeros(d+1); 1.0]\n    lb = [-10.0 .+ zeros(d+1); 0.01]\n    ub = [10.0 .+ zeros(d+1); 20.0]\n    optim_out = optimize(θ -&gt; -sum(logpdf.(Normal.(m(d, θ[1:end-1], x), θ[end]), y)), lb, ub, θ₀)\n    θmin = optim_out.minimizer\n    mfit(x) = sum([θmin[i + 1] * x^i for i in 0:d])\n    return (mfit, θmin[end])\nend\n\nfunction plot_polyfit(d, x, y)\n    m, σ = polyfit(d, x, y)\n    p = scatter(x, y, label=\"Data\", markersize=5, ylabel=L\"$y$\", xlabel=L\"$x$\", title=\"Degree $d\")\n    plot!(p, xrange, m.(xrange), ribbon = 1.96 * σ, fillalpha=0.2, lw=3, label=\"Fit\")\n    ylims!(p, (-30, 15))\n    plot!(p, size=(600, 450))\n    return p\nend\n\np1 = plot_polyfit(1, x, y)\np2 = plot_polyfit(2, x, y)\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Impact of increasing model complexity on model fits\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#impact-of-increasing-model-complexity-2",
    "href": "slides/lecture09-2-model-skill.html#impact-of-increasing-model-complexity-2",
    "title": "Predictive Model Assessments",
    "section": "Impact of Increasing Model Complexity",
    "text": "Impact of Increasing Model Complexity\n\nCode\np3 = plot_polyfit(3, x, y)\np4 = plot_polyfit(4, x, y)\n\ndisplay(p3)\ndisplay(p4)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Impact of increasing model complexity on model fits\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#impact-of-increasing-model-complexity-3",
    "href": "slides/lecture09-2-model-skill.html#impact-of-increasing-model-complexity-3",
    "title": "Predictive Model Assessments",
    "section": "Impact of Increasing Model Complexity",
    "text": "Impact of Increasing Model Complexity\n\nCode\np5 = plot_polyfit(6, x, y)\np6 = plot_polyfit(10, x, y)\n\ndisplay(p5)\ndisplay(p6)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Impact of increasing model complexity on model fits\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#what-is-happening",
    "href": "slides/lecture09-2-model-skill.html#what-is-happening",
    "title": "Predictive Model Assessments",
    "section": "What Is Happening?",
    "text": "What Is Happening?\nWe can think of a model as a form of data compression.\nInstead of storing coordinates of individual points, project onto parameters of functional form.\nThe degree to which we can “tune” the model by adjusting parameters are called the model degrees of freedom (DOF), which is one measure of model complexity."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#implications-of-model-dof",
    "href": "slides/lecture09-2-model-skill.html#implications-of-model-dof",
    "title": "Predictive Model Assessments",
    "section": "Implications of Model DOF",
    "text": "Implications of Model DOF\nHigher DOF ⇒ more ability to represent complex patterns.\n\n\n\nIf DOF is too low, the model can’t capture meaningful data-generating signals (underfitting).\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Impact of increasing model complexity on model fits"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#implications-of-model-dof-1",
    "href": "slides/lecture09-2-model-skill.html#implications-of-model-dof-1",
    "title": "Predictive Model Assessments",
    "section": "Implications of Model DOF",
    "text": "Implications of Model DOF\nHigher DOF ⇒ more ability to represent complex patterns.\n\n\nBut if DOF is too high, the model will “learn” the noise rather than the signal, resulting in poor generalization (overfitting).\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Impact of increasing model complexity on model fits"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#in--vs.-out-of-sample-error",
    "href": "slides/lecture09-2-model-skill.html#in--vs.-out-of-sample-error",
    "title": "Predictive Model Assessments",
    "section": "In- Vs. Out-Of-Sample Error",
    "text": "In- Vs. Out-Of-Sample Error\n\n\nCode\nntest = 20\nxtest = rand(Uniform(-2, 2), ntest)\nytest = f(xtest) + rand(Normal(0, 2), length(xtest))\n\nin_error = zeros(11)\nout_error = zeros(11)\nfor d = 0:10\n    m, σ = polyfit(d, x, y)\n    in_error[d+1] = mean((m.(x) .- y).^2)\n    out_error[d+1] = mean((m.(xtest) .- ytest).^2)\nend\n\nplot(0:10, in_error, markersize=5, color=:blue, lw=3, label=\"In-Sample Error\", xlabel=\"Polynomial Degree\", ylabel=\"Mean Squared Error\", legend=:topleft)\nplot!(0:10, out_error, markersize=5, color=:red, lw=3, label=\"Out-of-Sample Error\")\nplot!(yaxis=:log)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Impact of increasing model complexity on in and out of sample error"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#why-is-overfitting-a-problem",
    "href": "slides/lecture09-2-model-skill.html#why-is-overfitting-a-problem",
    "title": "Predictive Model Assessments",
    "section": "Why Is Overfitting A Problem?",
    "text": "Why Is Overfitting A Problem?\nExample from The Signal and the Noise by Nate Silver:\n\nEngineers at Fukushima used a non-linear regression on historical earthquake data (theory suggests a linear model).\nThis model predicted a &gt;9 Richter earthquake would only happen once every 13,000 years; engineers therefore designed nuclear plant to withstand an 8.6 Richter earthquake.\nTheoretical linear relationship suggests that a &gt;9 Richter earthquake would happen every 300 years."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#bias-vs.-variance",
    "href": "slides/lecture09-2-model-skill.html#bias-vs.-variance",
    "title": "Predictive Model Assessments",
    "section": "Bias vs. Variance",
    "text": "Bias vs. Variance\nThe difference between low and high DOFs can be formalized using bias and variance.\nSuppose we have a data-generating model \\[y = f(x) + \\varepsilon, \\varepsilon \\sim N(0, \\sigma).\\] We want to fit a model \\(\\hat{y} \\approx \\hat{f}(x)\\)."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#bias",
    "href": "slides/lecture09-2-model-skill.html#bias",
    "title": "Predictive Model Assessments",
    "section": "Bias",
    "text": "Bias\nBias is error from mismatches between the model predictions and the data (\\(\\text{Bias}[\\hat{f}] = \\mathbb{E}[\\hat{f}] - y\\)).\nBias comes from under-fitting meaningful relationships between inputs and outputs:\n\ntoo few degrees of freedom (“too simple”)\nneglected processes."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#variance",
    "href": "slides/lecture09-2-model-skill.html#variance",
    "title": "Predictive Model Assessments",
    "section": "Variance",
    "text": "Variance\nVariance is error from over-sensitivity to small fluctuations in training inputs \\(D\\) (\\(\\text{Variance} = \\text{Var}_D(\\hat{f}(x; D)\\)).\nVariance can come from over-fitting noise in the data:\n\ntoo many degrees of freedom (“too complex”)\npoor identifiability"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#bias-variance-tradeoff",
    "href": "slides/lecture09-2-model-skill.html#bias-variance-tradeoff",
    "title": "Predictive Model Assessments",
    "section": "“Bias-Variance Tradeoff”",
    "text": "“Bias-Variance Tradeoff”\nCan decompose MSE into bias and variance terms:\n\\[\n\\begin{align*}\n\\text{MSE} &= \\mathbb{E}[y - \\hat{f}^2] \\\\\n&= \\mathbb{E}[y^2 - 2y\\hat{f}(x) + \\hat{f}^2] \\\\\n&= \\mathbb{E}[y^2] - 2\\mathbb{E}[y\\hat{f}] + E[\\hat{f}^2] \\\\\n&= \\mathbb{E}[(f + \\varepsilon)^2] - \\mathbb{E}[(f + \\varepsilon)\\hat{f}] + E[\\hat{f}^2] \\\\\n&= \\vdots \\\\\n&= \\text{Bias}(\\hat{f})^2 + \\text{Var}(\\hat{f}) + \\sigma^2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#bias-variance-tradeoff-1",
    "href": "slides/lecture09-2-model-skill.html#bias-variance-tradeoff-1",
    "title": "Predictive Model Assessments",
    "section": "“Bias-Variance Tradeoff”",
    "text": "“Bias-Variance Tradeoff”\nThis means that for a fixed error level, you can reduce bias (increasing model complexity) or decrease variance (simplifying model) but one comes at the cost of the other.\nThis is the so-called “bias-variance tradeoff.”"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#more-general-b-v-tradeoff",
    "href": "slides/lecture09-2-model-skill.html#more-general-b-v-tradeoff",
    "title": "Predictive Model Assessments",
    "section": "More General B-V Tradeoff",
    "text": "More General B-V Tradeoff\nThis decomposition is for MSE, but the principle holds more generally.\n\nModels which perform better “on average” over the training data (low bias) are more likely to overfit (high variance);\nModels which have less uncertainty for training data (low variance) are more likely to do worse “on average” (high bias)."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#common-story-complexity-and-bias-variance",
    "href": "slides/lecture09-2-model-skill.html#common-story-complexity-and-bias-variance",
    "title": "Predictive Model Assessments",
    "section": "Common Story: Complexity and Bias-Variance",
    "text": "Common Story: Complexity and Bias-Variance\n\n\n\n\nBias-Variance Tradeoff\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#is-bias-variance-tradeoff-useful",
    "href": "slides/lecture09-2-model-skill.html#is-bias-variance-tradeoff-useful",
    "title": "Predictive Model Assessments",
    "section": "Is “Bias-Variance Tradeoff” Useful?",
    "text": "Is “Bias-Variance Tradeoff” Useful?\n\nTautology: total error is the sum of bias, variance, and irreducible error.\nBut error isn’t actually “conserved”: changing models changes all three terms.\nBias and variance do not directly tell us anything about generalizability (prediction error is not necessarily monotonic; see e.g. Belkin et al. (2019), Mei & Montanari (2019)).\nInstead, think about approximation vs. estimation error."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#r2-for-point-predictions",
    "href": "slides/lecture09-2-model-skill.html#r2-for-point-predictions",
    "title": "Predictive Model Assessments",
    "section": "\\(R^2\\) for Point Predictions",
    "text": "\\(R^2\\) for Point Predictions\n\\[R^2 = (\\sigma^2_\\text{model}/\\sigma^2_\\text{data})\\]\n\nProbably most common, also only meaningful for linear Gaussian models \\(y \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\).\nIncreasing predictors always increases \\(R^2\\), making it useless for model selection.\nCan adjust for increasing complexity, substitute \\[R^2_\\text{adj} = 1 - ((n / (n-2)) \\sigma^2_\\text{residuals} / \\sigma^2_\\text{data})\\]"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#myths-and-problems-with-r2",
    "href": "slides/lecture09-2-model-skill.html#myths-and-problems-with-r2",
    "title": "Predictive Model Assessments",
    "section": "Myths and Problems With \\(R^2\\)",
    "text": "Myths and Problems With \\(R^2\\)\n\nIt (and its adjusted value) doesn’t measure goodness of fit! If we knew the “true” regression slope \\(\\beta_1\\), not hard to derive \\[R^2 = \\frac{\\beta_1^2 \\text{Var}(x)}{\\beta_1^2 \\text{Var}(x) + \\sigma^2}.\\] This can be made arbitrarily small if \\(\\text{Var}(x)\\) is small or \\(\\sigma^2\\) is large even if the model is true. And when the model is wrong, \\(R^2\\) can be made arbitrarily large."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#myths-and-problems-with-r2-1",
    "href": "slides/lecture09-2-model-skill.html#myths-and-problems-with-r2-1",
    "title": "Predictive Model Assessments",
    "section": "Myths and Problems With \\(R^2\\)",
    "text": "Myths and Problems With \\(R^2\\)\n\nIt says nothing about prediction error.\nCannot be compared across different datasets (since it’s impacted by \\(\\text{Var}(x)\\)).\nIs not preserved under transformations.\nIt doesn’t explain anything about variance: \\(x\\) regressed on \\(y\\) gives the same \\(R^2\\) as \\(y\\) regressed on \\(x\\) (measure of correlation)."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#gaming-statistics-with-outliers",
    "href": "slides/lecture09-2-model-skill.html#gaming-statistics-with-outliers",
    "title": "Predictive Model Assessments",
    "section": "Gaming Statistics with Outliers",
    "text": "Gaming Statistics with Outliers\n\nSMBC: Take It Off\nSource: Saturday Morning Breakfast Cereal"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#anscombes-quartet-illustrates-problems-with-r2",
    "href": "slides/lecture09-2-model-skill.html#anscombes-quartet-illustrates-problems-with-r2",
    "title": "Predictive Model Assessments",
    "section": "Anscombe’s Quartet Illustrates Problems With \\(R^2\\)",
    "text": "Anscombe’s Quartet Illustrates Problems With \\(R^2\\)\n\n\nAnscombe’s Quartet (Anscombe, 1973) consists of datasets which have the same summary statistics (including \\(R^2\\)) but very different graphs.\n\n\n\n\nAnscombe’s Quartet\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#alternatives-to-r2",
    "href": "slides/lecture09-2-model-skill.html#alternatives-to-r2",
    "title": "Predictive Model Assessments",
    "section": "Alternatives to \\(R^2\\)",
    "text": "Alternatives to \\(R^2\\)\nIn pretty much every case where \\(R^2\\) might be useful, (root) mean squared error ((R)MSE) is better.\nMore generally, we want to think about measures which capture the skill of a probabilistic prediction.\nThese are commonly called scoring rules (Gneiting & Katzfuss, 2014)."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#key-points-bias",
    "href": "slides/lecture09-2-model-skill.html#key-points-bias",
    "title": "Predictive Model Assessments",
    "section": "Key Points (Bias)",
    "text": "Key Points (Bias)\n\nBias: Discrepancy between expected model output and observations.\nRelated to approximation error and underfitting.\n“Simpler” models tend to have higher bias.\nHigh bias suggests less sensitivity to specifics of dataset."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#key-points-variance",
    "href": "slides/lecture09-2-model-skill.html#key-points-variance",
    "title": "Predictive Model Assessments",
    "section": "Key Points (Variance)",
    "text": "Key Points (Variance)\n\nVariance: How much the model predictions vary around the mean.\nRelated to estimation error and overfitting.\n“More complex” models tend to have higher variance.\nHigh variance suggests high sensitivity to specifics of dataset."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#key-points-bias-variance-tradeoff",
    "href": "slides/lecture09-2-model-skill.html#key-points-bias-variance-tradeoff",
    "title": "Predictive Model Assessments",
    "section": "Key Points (Bias-Variance Tradeoff)",
    "text": "Key Points (Bias-Variance Tradeoff)\n\nError can be decomposed into bias, variance, and irreducible error.\n“Tradeoff” reflects this decomposition (but it’s not really a tradeoff).\nUseful conceptual to think about the balance between not picking up on meaningful signals (underfitting) and modeling noise (overfitting)."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#next-classes",
    "href": "slides/lecture09-2-model-skill.html#next-classes",
    "title": "Predictive Model Assessments",
    "section": "Next Classes",
    "text": "Next Classes\nNext Week: Model Comparison Methods: Cross-Validation and Information Criteria"
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#assessments",
    "href": "slides/lecture09-2-model-skill.html#assessments",
    "title": "Predictive Model Assessments",
    "section": "Assessments",
    "text": "Assessments\nProject Proposal: Due Friday.\nHW4: Will release this week, due 4/11 (after break).\nLiterature Critique: Talk to Prof. Srikrishnan if you want help finding a paper.\nNo quiz this week so you can focus on your project proposal."
  },
  {
    "objectID": "slides/lecture09-2-model-skill.html#refernences-scroll-for-full-list",
    "href": "slides/lecture09-2-model-skill.html#refernences-scroll-for-full-list",
    "title": "Predictive Model Assessments",
    "section": "Refernences (Scroll for Full List)",
    "text": "Refernences (Scroll for Full List)\n\n\n\n\nAnscombe, F. J. (1973). Graphs in Statistical Analysis. Am. Stat., 27, 17. https://doi.org/10.2307/2682899\n\n\nBelkin, M., Hsu, D., & Xu, J. (2019). Two models of double descent for weak features. arXiv [Cs.LG]. Retrieved from http://arxiv.org/abs/1903.07571\n\n\nGneiting, T., & Katzfuss, M. (2014). Probabilistic Forecasting. Annual Review of Statistics and Its Application, 1, 125–151. https://doi.org/10.1146/annurev-statistics-062713-085831\n\n\nMei, S., & Montanari, A. (2019). The generalization error of random features regression: Precise asymptotics and double descent curve. arXiv [Math.ST]. Retrieved from http://arxiv.org/abs/1908.05355"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#statistics-as-decision-making-under-uncertainty",
    "href": "slides/lecture03-1-probmodels.html#statistics-as-decision-making-under-uncertainty",
    "title": "Probability Models and EDA",
    "section": "Statistics as Decision-Making Under Uncertainty",
    "text": "Statistics as Decision-Making Under Uncertainty\n\nWent over “standard” null hypothesis significance approach.\nNull vs. alternative hypotheses\n\\(p\\)-values: (continuous) assessment of probability of seeing test statistic under null hypothesis."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#why-do-we-need-models-for-data",
    "href": "slides/lecture03-1-probmodels.html#why-do-we-need-models-for-data",
    "title": "Probability Models and EDA",
    "section": "Why Do We Need Models For Data?",
    "text": "Why Do We Need Models For Data?\n\nData are imperfect: Data \\(X\\) are only one realization of the data that could have been observed and/or we can only measure indirect proxies of what we care about.\nOver time, statisticians learned to treat these imperfections as the results of random processes (for some of this history, see Hacking (1990)), requiring probability models."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#predicting-random-variables",
    "href": "slides/lecture03-1-probmodels.html#predicting-random-variables",
    "title": "Probability Models and EDA",
    "section": "Predicting Random Variables",
    "text": "Predicting Random Variables\nLet’s say that we want to predict the value of a variable \\(y \\sim Y\\). We need a criteria to define the “best” point prediction.\nReasonable starting point:\n\\[\\text{MSE}(m) = \\mathbb{E}\\left[(Y - m)^2\\right]\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#mse-as-squared-bias-variance",
    "href": "slides/lecture03-1-probmodels.html#mse-as-squared-bias-variance",
    "title": "Probability Models and EDA",
    "section": "MSE As (Squared) Bias + Variance",
    "text": "MSE As (Squared) Bias + Variance\n\\[\\begin{aligned}\n\\mathbb{V}(Y) &= \\mathbb{E}\\left[(Y - \\mathbb{E}[Y])^2\\right] \\\\\n&= \\mathbb{E}\\left[Y^2 - 2Y\\mathbb{E}[Y] + \\mathbb{E}[Y]^2\\right] \\\\\n&= \\mathbb{E}[Y^2] - 2\\mathbb{E}[Y]^2 + \\mathbb{E}[Y]^2 = \\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2 .\n\\end{aligned}\\]\n\n\\[\\Rightarrow \\text{MSE}(m) = \\mathbb{E}\\left[(Y - m)^2\\right] = \\mathbb{E}\\left[(Y-m)\\right]^2 + \\mathbb{V}(Y - m).\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#bias-variance-decomposition-of-mse",
    "href": "slides/lecture03-1-probmodels.html#bias-variance-decomposition-of-mse",
    "title": "Probability Models and EDA",
    "section": "Bias-Variance Decomposition of MSE",
    "text": "Bias-Variance Decomposition of MSE\nThen:\n\\[\\begin{aligned}\n\\text{MSE}(m) &= \\mathbb{E}\\left[(Y-m)\\right]^2 + \\mathbb{V}(Y - m)  \\\\\n&= \\mathbb{E}\\left[(Y-m)\\right]^2 + \\mathbb{V}(Y) \\\\\n&= \\left(\\mathbb{E}[Y] - m\\right)^2 + \\mathbb{V}(Y).\n\\end{aligned}\\]\nThis is the source of the so-called “bias-variance tradeoff” (more on this later)."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#optimizing",
    "href": "slides/lecture03-1-probmodels.html#optimizing",
    "title": "Probability Models and EDA",
    "section": "Optimizing…",
    "text": "Optimizing…\nWe want to find the minimum value of \\(\\text{MSE}(m)\\) (denote the optimal prediction by \\(\\mu\\)):\n\n\\[\n\\begin{aligned}\n\\frac{d\\text{MSE}}{dm} &= -2(\\mathbb{E}[Y] - m) + 0 \\\\\n0 = \\left.\\frac{d\\text{MSE}}{dm}\\right|_{m = \\mu} &= -2(\\mathbb{E}[Y] - \\mu)\n\\end{aligned}\\]\n\\[\\Rightarrow \\mu = \\mathbb{E}[Y].\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#expected-value-as-best-prediction",
    "href": "slides/lecture03-1-probmodels.html#expected-value-as-best-prediction",
    "title": "Probability Models and EDA",
    "section": "Expected Value As Best Prediction",
    "text": "Expected Value As Best Prediction\nIn other words, the best predicted value of a random variable is its expectation.\nBut:\n\nWe usually don’t have enough data to know what \\(\\mathbb{E}[Y]\\) is: need to make probabilistic assumptions;\nIn many applications, we don’t just want a point estimate, we need some estimate of ranges of values we might observe."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#uses-of-models",
    "href": "slides/lecture03-1-probmodels.html#uses-of-models",
    "title": "Probability Models and EDA",
    "section": "Uses of Models",
    "text": "Uses of Models\nWhat can we use a probability model for?\n\nSummaries of data: store \\(y = f(\\mathbf{x})\\) instead of all data points \\((y, x_1, \\ldots, x_n)\\)\nSmooth values by removing noise\nPredict new data (interpolation/extrapolation): \\(\\hat{f} = f(\\hat{\\mathbf{x}})\\)\nInfer relationships between variables (interpret coefficients of \\(f\\))"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#my-philosophical-position",
    "href": "slides/lecture03-1-probmodels.html#my-philosophical-position",
    "title": "Probability Models and EDA",
    "section": "My Philosophical Position",
    "text": "My Philosophical Position\n\n\n\nProbability theory helps us deduce logical implications of theories conditional on our assumptions\nCannot use an “objective” procedure to avoid subjective responsibility\n\n\n\n\n\n\nSpiderman Meme"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#linear-regression-1",
    "href": "slides/lecture03-1-probmodels.html#linear-regression-1",
    "title": "Probability Models and EDA",
    "section": "Linear Regression",
    "text": "Linear Regression\nThe “simplest” model is linear:\n\n\n\\[\\begin{aligned}\ny_i &\\sim N(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\sum_j \\beta_j x^j_i\n\\end{aligned}\n\\]\n\n\\[\\begin{aligned}\ny_i &= \\sum_j \\beta_j x^j_i + \\varepsilon_i \\\\\n\\varepsilon_i &\\sim N(0, \\sigma^2)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#why-linear-models",
    "href": "slides/lecture03-1-probmodels.html#why-linear-models",
    "title": "Probability Models and EDA",
    "section": "Why Linear Models?",
    "text": "Why Linear Models?\n\n\nTwo main reasons to use linear models/normal distributions:\n\nInferential: “Least informative” distribution assuming only finite mean/variance;\nGenerative: Central Limit Theorem (summed fluctuations are asymptotically normal)\n\n\n\n\n\nWeight stack Gaussian distribution\n\n\n\nSource: r/GymMemes\n\n\n\nOne key thing: normal distributions are the “least informative” distribution given constraints on mean and variance. So all else being equal, this is a useful machine if all we’re interested in are those two moments."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#linear-regression-as-probability-model",
    "href": "slides/lecture03-1-probmodels.html#linear-regression-as-probability-model",
    "title": "Probability Models and EDA",
    "section": "Linear Regression As Probability Model",
    "text": "Linear Regression As Probability Model\n\\[\ny &= \\underbrace{\\sum_j \\beta_j x^j_i)}_{\\mathbb{E}[y | x^j]} + \\underbrace{\\varepsilon_i}_{\\text{noise}}, \\quad \\varepsilon_i \\sim N(0, \\sigma^2)\n\\]\n\n\\(\\mathbb{E}[y | x^j]\\): Best linear prediction of \\(y\\) conditional on choice of predictors \\(x^j\\).\nThe noise defines the probability distribution (here: Gaussian) of the observations: \\[y \\sim N(\\sum_j \\beta_j x^j_i), \\sigma^2).\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#implications-of-gaussian-noise",
    "href": "slides/lecture03-1-probmodels.html#implications-of-gaussian-noise",
    "title": "Probability Models and EDA",
    "section": "Implications of Gaussian Noise",
    "text": "Implications of Gaussian Noise\nWithout explicit modeling of sources of measurement uncertainty, in LR we can’t separate a noisy system state (due to e.g. omitted variables) from an uncertain measurement error (more on this later).\n\\[\n\\begin{array}{l}\nX \\sim N(\\mu_1, \\sigma_1^2) \\\\\nY \\sim N(\\mu_2, \\sigma_2^2) \\\\\nZ = X + Y\n\\end{array}\n\\qquad \\Rightarrow \\qquad Z \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\n\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#central-limit-theorem",
    "href": "slides/lecture03-1-probmodels.html#central-limit-theorem",
    "title": "Probability Models and EDA",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIf\n\n\\(\\mathbb{E}[X_i] = \\mu\\)\nand \\(\\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\),\n\n\\[\\begin{aligned}\n\\lim_{n \\to \\infty} \\sqrt{n}(\\bar{X}_n - \\mu ) &= \\mathcal{N}(0, \\sigma^2) \\\\\n\\Rightarrow & \\bar{X}_n \\overset{\\text{approx}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2/n)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#central-limit-theorem-more-intuitive",
    "href": "slides/lecture03-1-probmodels.html#central-limit-theorem-more-intuitive",
    "title": "Probability Models and EDA",
    "section": "Central Limit Theorem (More Intuitive)",
    "text": "Central Limit Theorem (More Intuitive)\n\n\nFor a large enough set of samples:\nThe sampling distribution of a sum or mean of random variables is approximately normal distribution, even if the random variables themselves are not.\n\n\n\n\nSmall n Meme\n\n\n\nSource: Unknown"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#probability-models-1",
    "href": "slides/lecture03-1-probmodels.html#probability-models-1",
    "title": "Probability Models and EDA",
    "section": "Probability Models",
    "text": "Probability Models\n\nAlmost anything we want to use data for involves an assumption about the underlying probability model for the data.\nOur goal this semester is to develop a toolkit to specify, test, and use these probability models."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#next-classes",
    "href": "slides/lecture03-1-probmodels.html#next-classes",
    "title": "Probability Models and EDA",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Fitting models with maximum likelihood\nFriday: Maximum likelihood uncertainty"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#assessments",
    "href": "slides/lecture03-1-probmodels.html#assessments",
    "title": "Probability Models and EDA",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 Due Friday, 2/6."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#references-scroll-for-full-list",
    "href": "slides/lecture03-1-probmodels.html#references-scroll-for-full-list",
    "title": "Probability Models and EDA",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nHacking, I. (1990). The Taming of Chance. Cambridge, England: Cambridge University Press. https://doi.org/10.1017/CBO9780511819766"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#extreme-values",
    "href": "slides/lecture12-2-missing-data.html#extreme-values",
    "title": "Missing Data",
    "section": "Extreme Values",
    "text": "Extreme Values\nValues with a very low probability of occurring, not necessarily high-impact events (which don’t have to be rare!).\n\n“Block” extremes, e.g. annual maxima (block maxima)\nValues which exceed a certain threshold (peaks over threshold)"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#statistics-and-missing-data",
    "href": "slides/lecture12-2-missing-data.html#statistics-and-missing-data",
    "title": "Missing Data",
    "section": "Statistics and Missing Data",
    "text": "Statistics and Missing Data\n\n\n…Statistics is a missing data problem.\n\n\n– Little (2013)"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#missing-vs.-latent-variables",
    "href": "slides/lecture12-2-missing-data.html#missing-vs.-latent-variables",
    "title": "Missing Data",
    "section": "Missing vs. Latent Variables",
    "text": "Missing vs. Latent Variables\nMissing Data: Variables which are inconsistently observed.\nLatent Variables: Unobserved variables which influence data-generating process.\n\nIn both cases, we would like to understand the complete-data likelihood (data-generating process including the missing/latent data)."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#common-but-flawed-approach-complete-case-analysis",
    "href": "slides/lecture12-2-missing-data.html#common-but-flawed-approach-complete-case-analysis",
    "title": "Missing Data",
    "section": "Common (But Flawed) Approach: Complete-Case Analysis",
    "text": "Common (But Flawed) Approach: Complete-Case Analysis\nComplete-case Analysis: Only consider data for which all variables are available.\n\nCan result in bias if there is a missing values have a systematic pattern.\nCould result in discarding a large amount of data."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#importance-of-assumptions",
    "href": "slides/lecture12-2-missing-data.html#importance-of-assumptions",
    "title": "Missing Data",
    "section": "Importance of Assumptions",
    "text": "Importance of Assumptions\nBecause we don’t observe the missing data, all approaches require assumptions about how the missing data might have looked.\nExamples:\n\nWhether data is missing is entirely random;\nData can be linearly inter-/extrapolated."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#missing-data-example",
    "href": "slides/lecture12-2-missing-data.html#missing-data-example",
    "title": "Missing Data",
    "section": "Missing Data Example",
    "text": "Missing Data Example\n\n\n\n\nCode\nn = 50\nx = rand(Uniform(0, 100), n)\nlogit(x) = log(x / (1 - x))\ninvlogit(x) = exp(x) / (1 + exp(x))\nf(x) = invlogit(0.05 * (x - 50) + rand(Normal(0, 1)))\ny = f.(x)\n\nm(y) = invlogit(0.75 * logit(y))\nprob_missing_y = m.(y)\nmissing_y = Bool.(rand.(Binomial.(1, prob_missing_y)))\nxobs = x[.!(missing_y)]\nyobs = y[.!(missing_y)]\n\np_alldat = scatter(xobs, yobs, xlabel=L\"$x$\", ylabel=L\"$y$\", label=\"Observations\", markersize=5, size=(600, 500), ylims=(-0.05, 1))\nscatter!(x[missing_y], zeros(sum(missing_y)), markershape=:x, markersize=3, label=\"Missing Observations\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Missing Data Running Example\n\n\n\n\n\n\n\nCode\nlinfit = lm([ones(length(xobs)) xobs], yobs)\nlinpred = predict(linfit, [ones(sum(missing_y)) x[missing_y]])\np1 = deepcopy(p_alldat)\nscatter!(p1, x[missing_y], linpred, label=\"Imputed Values\", markersize=5, markershape=:diamond, legend=false)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Missing Data Running Example"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#missing-data-example-1",
    "href": "slides/lecture12-2-missing-data.html#missing-data-example-1",
    "title": "Missing Data",
    "section": "Missing Data Example",
    "text": "Missing Data Example\n\n\n\n\nCode\np_alldat\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Missing Data Running Example\n\n\n\n\n\n\n\nCode\nrepeatpred = sample(yobs, sum(missing_y), replace=true)\np2 = deepcopy(p_alldat)\nscatter!(p2, x[missing_y], repeatpred, label=\"Imputed Values\", markersize=5, markershape=:diamond, legend=false)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Missing Data Running Example"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#missing-data-example-2",
    "href": "slides/lecture12-2-missing-data.html#missing-data-example-2",
    "title": "Missing Data",
    "section": "Missing Data Example",
    "text": "Missing Data Example\n\n\n\n\nCode\np_alldat\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Missing Data Running Example\n\n\n\n\n\n\n\nCode\np3 = deepcopy(p_alldat)\nscatter!(p3, x[missing_y], y[missing_y], label=\"Imputed Values\", markersize=5, markershape=:diamond, legend=false)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Missing Data Running Example"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#comparison-of-imputations",
    "href": "slides/lecture12-2-missing-data.html#comparison-of-imputations",
    "title": "Missing Data",
    "section": "Comparison of Imputations",
    "text": "Comparison of Imputations"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#notation",
    "href": "slides/lecture12-2-missing-data.html#notation",
    "title": "Missing Data",
    "section": "Notation",
    "text": "Notation\nLet \\(M_Y\\) be the indicator function for whether \\(Y\\) is missing and let \\(\\pi(x) = \\mathbb{P}(M_Y = 0 | X = x)\\) be the inclusion probability.\nGoal: Understand the complete-data distribution \\(\\mathbb{P}(Y=y | X=x)\\).\nBut we only have the observed distribution \\(\\mathbb{P}(Y = y | X=x, M_Y = 0) \\pi(x)\\). We are missing \\(\\mathbb{P}(Y = y | X=x, M_Y = 1) (1-\\pi(x))\\)."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#missingness-complete-at-random-mcar",
    "href": "slides/lecture12-2-missing-data.html#missingness-complete-at-random-mcar",
    "title": "Missing Data",
    "section": "Missingness Complete At Random (MCAR)",
    "text": "Missingness Complete At Random (MCAR)\n\n\nMCAR: \\(M_Y\\) is independent of \\(X=x\\) and \\(Y=y\\).\nComplete cases are fully representative of the complete data:\n\\[\\mathbb{P}(Y=y) = P(Y=y | M_Y=0)\\]\n\n\n\nCode\nflin(x) = 0.25 * x + 2 + rand(Normal(0, 7))\ny = flin.(x)\nxpred = collect(0:0.1:100)\n\nlm_all = lm([ones(length(x)) x], y)\ny_lm_all = predict(lm_all, [ones(length(xpred)) xpred])\n\nmissing_y = Bool.(rand(Binomial.(1, 0.25), length(y)))\nxobs = x[.!(missing_y)]\nyobs = y[.!(missing_y)]\nlm_mcar = lm([ones(n - sum(missing_y)) xobs], yobs)\ny_lm_mcar = predict(lm_mcar, [ones(length(xpred)) xpred])\n\n\np1 = scatter(xobs, yobs, xlabel=L\"$x$\", ylabel=L\"$y$\", label=false, markersize=5, size=(600, 500), color=:blue)\nscatter!(x[missing_y], y[missing_y], alpha=0.9, color=:lightgrey, label=false, markersize=5)\nplot!(xpred, y_lm_all, color=:red, lw=3, label=\"Complete-Data Inference\", ribbon=GLM.dispersion(lm_all), fillalpha=0.2)\nplot!(xpred, y_lm_mcar, color=:blue, lw=3, linestyle=:dot, label=\"Observed-Data Inference\", ribbon=GLM.dispersion(lm_mcar), fillalpha=0.2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Illustration of MCAR Data"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#missingness-at-random-mar",
    "href": "slides/lecture12-2-missing-data.html#missingness-at-random-mar",
    "title": "Missing Data",
    "section": "Missingness At Random (MAR)",
    "text": "Missingness At Random (MAR)\n\n\nMAR: \\(M_Y\\) is independent of \\(Y=y\\) conditional on \\(X=x\\).\nAlso called ignorable or uninformative missingness.\n\\[\n\\begin{align*}\n\\mathbb{P}&(Y=y | X=x) \\\\\n&= \\mathbb{P}(Y=y | X=x, M_Y=0)\n\\end{align*}\n\\]\n\n\n\nCode\nmissing_y = Bool.(rand.(Binomial.(1,  invlogit.(0.1 * (x .- 75)))))\nxobs = x[.!(missing_y)]\nyobs = y[.!(missing_y)]\nlm_mar = lm([ones(n - sum(missing_y)) xobs], yobs)\ny_lm_mar = predict(lm_mar, [ones(length(xpred)) xpred])\n\np2 = scatter(xobs, yobs, xlabel=L\"$x$\", ylabel=L\"$y$\", label=false, markersize=5, size=(600, 500), color=:blue)\nscatter!(x[missing_y], y[missing_y], alpha=0.9, color=:lightgrey, label=false, markersize=5)\nplot!(xpred, y_lm_all, color=:red, lw=3, label=\"Complete-Data Inference\", ribbon=GLM.dispersion(lm_all), fillalpha=0.2)\nplot!(xpred, y_lm_mar, color=:blue, lw=3, linestyle=:dot, label=\"Observed-Data Inference\", ribbon=GLM.dispersion(lm_mar), fillalpha=0.2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Illustration of MAR Data"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#missingness-not-at-random-mnar",
    "href": "slides/lecture12-2-missing-data.html#missingness-not-at-random-mnar",
    "title": "Missing Data",
    "section": "Missingness Not-At-Random (MNAR)",
    "text": "Missingness Not-At-Random (MNAR)\n\n\nMNAR: \\(M_Y\\) is dependent on \\(Y=y\\) (and/or unmodeled variables).\nAlso called non-ignorable or informative missingness.\n\\[\n\\begin{align*}\n\\mathbb{P}&(Y=y | X=x) \\\\\n&\\neq \\mathbb{P}(Y=y | X=x, M_Y=0)\n\\end{align*}\n\\]\n\n\n\nCode\nmissing_y = Bool.(rand.(Binomial.(1,  invlogit.(0.9 * (y .- 15)))))\nxobs = x[.!(missing_y)]\nyobs = y[.!(missing_y)]\nlm_mnar = lm([ones(n - sum(missing_y)) xobs], yobs)\ny_lm_mnar = predict(lm_mnar, [ones(length(xpred)) xpred])\n\np2 = scatter(xobs, yobs, xlabel=L\"$x$\", ylabel=L\"$y$\", label=false, markersize=5, size=(600, 500), color=:blue)\nscatter!(x[missing_y], y[missing_y], alpha=0.9, color=:lightgrey, label=false, markersize=5)\nplot!(xpred, y_lm_all, color=:red, lw=3, label=\"Complete-Data Inference\", ribbon=GLM.dispersion(lm_all), fillalpha=0.2)\nplot!(xpred, y_lm_mnar, color=:blue, lw=3, linestyle=:dot, label=\"Observed-Data Inference\", ribbon=GLM.dispersion(lm_mnar), fillalpha=0.2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Illustration of MCAR Data"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#implications-of-missingness-mechanism",
    "href": "slides/lecture12-2-missing-data.html#implications-of-missingness-mechanism",
    "title": "Missing Data",
    "section": "Implications of Missingness Mechanism",
    "text": "Implications of Missingness Mechanism\n\nMCAR: Strong, but generally implausible. Can only use complete cases as observed data is fully representative.\nMAR: More plausible than MCAR, can still justify complete-case analysis as conditional observed distributions are unbiased estimates of conditional complete distributions.\nMNAR: Deletion is a bad idea. The observed data does not follow the same conditional distribution. Missingness can be informative: try to model the missingness mechanism."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#checking-mcar",
    "href": "slides/lecture12-2-missing-data.html#checking-mcar",
    "title": "Missing Data",
    "section": "Checking MCAR",
    "text": "Checking MCAR\nIn general, we can’t know for sure if missingness \\(M_Y\\) is informative about \\(Y\\) (since we can’t see it!).\nBut we can check if \\(M_Y\\) is independent of \\(X\\): if not, reject MCAR.\nCan we conclude MCAR if, in our dataset, \\(M_Y\\) appears independent of \\(X\\)?"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#distinguishing-mar-from-mnar",
    "href": "slides/lecture12-2-missing-data.html#distinguishing-mar-from-mnar",
    "title": "Missing Data",
    "section": "Distinguishing MAR from MNAR",
    "text": "Distinguishing MAR from MNAR\nCan’t do this statistically!\nMAR: \\(\\mathbb{P}(Y=y | X-x, M_Y=1) = \\mathbb{P}(Y=y | X-x, M_Y=0)\\)\nBut the data tells us nothing about \\(\\mathbb{P}(Y=y | X-x, M_Y=1)\\). Need to bring to bear understanding of data-collection process.\nInstead, try a few different models reflecting different assumptions about missingness: do your conclusions change?"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#methods-for-dealing-with-missing-data-1",
    "href": "slides/lecture12-2-missing-data.html#methods-for-dealing-with-missing-data-1",
    "title": "Missing Data",
    "section": "Methods for Dealing with Missing Data",
    "text": "Methods for Dealing with Missing Data\n\nImputation: substitute values for missing data before analysis;\nAveraging: find expected values over all possible values of the missing variables."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#imputation",
    "href": "slides/lecture12-2-missing-data.html#imputation",
    "title": "Missing Data",
    "section": "Imputation",
    "text": "Imputation\nImputation does not create “new” information, it reuses existing information to allow the use of standard procedures.\nExample: Missing observations in a time series, want to insert values to fit AR(1) model or estimate autocorrelation using “simple” estimators.\nAs a result, it’s convenient but can create systematic distortions."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#imputation-under-mar",
    "href": "slides/lecture12-2-missing-data.html#imputation-under-mar",
    "title": "Missing Data",
    "section": "Imputation Under MAR",
    "text": "Imputation Under MAR\n\nImpute from the marginal distribution (parametrically or non-parametrically), \\[p(Y_\\text{miss}) = p(Y_\\text{obs}).\\] This can create distortions if meaningful relationships are neglected.\nImpute using a regression model (such as linear imputation). This generalizes relationships but requires missingness being uninformative about \\(Y\\)."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#imputation-under-mar-1",
    "href": "slides/lecture12-2-missing-data.html#imputation-under-mar-1",
    "title": "Missing Data",
    "section": "Imputation Under MAR",
    "text": "Imputation Under MAR\n\nImpute from the conditional distribution, \\[p(Y_\\text{miss} | X = x) = p(Y_\\text{obs} | X = x).\\] Can be done parametrically or non-parametrically.\nImpute using matching: find a closest predictor and copy value of \\(Y\\). Can work okay or be a terrible idea."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#imputation-under-mnar",
    "href": "slides/lecture12-2-missing-data.html#imputation-under-mnar",
    "title": "Missing Data",
    "section": "Imputation Under MNAR",
    "text": "Imputation Under MNAR\nNeed to model missingness mechanism (censoring, etc).\nOften need to make assumptions about how the relationship extrapolates.\n\nModel relationship between predictors and missing data;\nAdd unknown constant to imputed data to reflect biases.\n\nUltimately, MNAR requires a sensitivity analysis."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#multiple-imputation",
    "href": "slides/lecture12-2-missing-data.html#multiple-imputation",
    "title": "Missing Data",
    "section": "Multiple Imputation",
    "text": "Multiple Imputation\n\n\nImputing one value for a missing datum cannot be correct in general, because we don’t know what value to impute with certainty (if we did, it wouldn’t be missing).\n\n\n—- Rubin (1987)"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#multiple-imputation-steps",
    "href": "slides/lecture12-2-missing-data.html#multiple-imputation-steps",
    "title": "Missing Data",
    "section": "Multiple Imputation Steps",
    "text": "Multiple Imputation Steps\n\nGenerate \\(m\\) imputations \\(\\hat{y}_i\\) by sampling missing values;\nEstimate statistics \\(\\hat{t}_i\\) for each imputation\nPool \\(\\{\\hat{t}_i\\}\\) and estimate \\[\\bar{t} = \\frac{1}{m} \\sum_i \\hat{t}_i\\] \\[\\bar{\\sigma}^2 = \\frac{1}{m}\\sum_{i=1}^m \\hat{\\sigma}_i^2 + (1 + \\frac{1}{m}) \\text{Var}(\\hat{t}_i)\\]"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#methods-for-multiple-imputation",
    "href": "slides/lecture12-2-missing-data.html#methods-for-multiple-imputation",
    "title": "Missing Data",
    "section": "Methods for Multiple Imputation",
    "text": "Methods for Multiple Imputation\n\nPrediction with noise: Fit a regression model and add noise to expected value. Better to use the bootstrap to also include parameter uncertainty.\nPredictive mean matching: Sample missing values from cases with close values of predictors.\n\nIn both cases, important to include as much information as possible in the imputation model!"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#multiple-imputation-models",
    "href": "slides/lecture12-2-missing-data.html#multiple-imputation-models",
    "title": "Missing Data",
    "section": "Multiple Imputation Models",
    "text": "Multiple Imputation Models\nNo need to be limited to linear regression!\n\nClassification and Regression Trees very common (random forests probably better for additional variation);\nCould set up time-specific models."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#bayesian-imputation",
    "href": "slides/lecture12-2-missing-data.html#bayesian-imputation",
    "title": "Missing Data",
    "section": "Bayesian Imputation",
    "text": "Bayesian Imputation\nBayesian imputation involves putting a prior over the missing values and treating them as model parameters, resulting in a joint distribution of imputed values and parameters:\n\\[p(\\theta, y_\\text{miss} | Y=y_\\text{obs}, X=x)\\]"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#key-points",
    "href": "slides/lecture12-2-missing-data.html#key-points",
    "title": "Missing Data",
    "section": "Key Points",
    "text": "Key Points\n\nMissing data is very common in environmental contexts.\nAbility to draw unbiased inferences depends on MCAR, MAR, or MNAR/informativeness of missingness.\nBest approach to missing data is to not have any.\nOtherwise, try multiple imputation based on understanding/theories of missing mechanisms. Use as much data as possible in these models."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#upcoming-schedule",
    "href": "slides/lecture12-2-missing-data.html#upcoming-schedule",
    "title": "Missing Data",
    "section": "Upcoming Schedule",
    "text": "Upcoming Schedule\nMonday: Mixture Models and Model-Based Clustering or Gaussian Processes and Emulation.\nNext Wednesday (4/23): No Class"
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#assessments",
    "href": "slides/lecture12-2-missing-data.html#assessments",
    "title": "Missing Data",
    "section": "Assessments",
    "text": "Assessments\nHW5 released, due 5/2.\nLiterature Critique: Due 5/2.\nProject Presentations: 4/48, 4/30, 5/5."
  },
  {
    "objectID": "slides/lecture12-2-missing-data.html#references",
    "href": "slides/lecture12-2-missing-data.html#references",
    "title": "Missing Data",
    "section": "References",
    "text": "References\n\n\n\n\nLittle, R. J. (2013). In praise of simplicity not mathematistry! Ten simple powerful ideas for the statistical scientist. J. Am. Stat. Assoc., 108, 359–369. https://doi.org/10.1080/01621459.2013.787932\n\n\nRubin, D. B. (1987). Multiple Imputation for Nonresponse in Surveys. (D. B. Rubin, Ed.) (99th ed.). Nashville, TN: John Wiley & Sons. https://doi.org/10.1002/9780470316696"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#time-series-and-environmental-data",
    "href": "slides/lecture03-2-timeseries.html#time-series-and-environmental-data",
    "title": "Time Series",
    "section": "Time Series and Environmental Data",
    "text": "Time Series and Environmental Data\n\n\nMany environmental datasets involve time series: repeated observations over time:\n\\[X = \\{X_t, X_{t+h}, X_{t+2h}, \\ldots, X_{t+nh}\\}\\]\nMore often: ignore sampling time in notation,\n\\[X = \\{X_1, X_2, X_3, \\ldots, X_n\\}\\]\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Example of time series; trapped lynx populations in Canada."
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#when-do-we-care-about-time-series",
    "href": "slides/lecture03-2-timeseries.html#when-do-we-care-about-time-series",
    "title": "Time Series",
    "section": "When Do We Care About Time Series?",
    "text": "When Do We Care About Time Series?\n\n\nDependence: History or sequencing of the data matters\n\\[p(y_t) = f(y_1, \\ldots, y_{t-1})\\]\nSerial dependence captured by autocorrelation:\n\\[\\varsigma(i) = \\rho(y_t, y_{t-i}) = \\frac{\\text{Cov}[y_t, y_{t+i}]}{\\mathbb{V}[y_t]} \\]\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Autocorrelation for the Lynx data."
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#stationarity",
    "href": "slides/lecture03-2-timeseries.html#stationarity",
    "title": "Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nI.I.D. Data: All data drawn from the same distribution, \\(y_i \\sim \\mathcal{D}\\).\nEquivalent for time series \\(\\mathbf{y} = \\{y_t\\}\\) is stationarity.\n\nStrict stationarity: \\(\\{y_t, \\ldots, y_{t+k-1}\\} \\sim \\mathcal{D}\\) for all \\(t\\).\nWeak stationarity: \\(\\mathbb{E}[X_1] = \\mathbb{E}[X_t]\\) and \\(\\text{Cov}(X_1, X_k) = \\text{Cov}(X_t, X_{t+k-1})\\) for all \\(t\\)."
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#stationary-vs.-non-stationary-series",
    "href": "slides/lecture03-2-timeseries.html#stationary-vs.-non-stationary-series",
    "title": "Time Series",
    "section": "Stationary vs. Non-Stationary Series",
    "text": "Stationary vs. Non-Stationary Series\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Stationary vs. Non-stationary time series\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#autocorrelation",
    "href": "slides/lecture03-2-timeseries.html#autocorrelation",
    "title": "Time Series",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Lag-1 autocorrelation for an AR model and Gaussian noise.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#autoregressive-ar-models",
    "href": "slides/lecture03-2-timeseries.html#autoregressive-ar-models",
    "title": "Time Series",
    "section": "Autoregressive (AR) Models",
    "text": "Autoregressive (AR) Models\nAR(p): (autoregressive of order \\(p\\)):\n\\[\n\\begin{align*}\ny_t &= \\sum_{i=1}^p \\rho_{i} y_{t-i} + \\varepsilon \\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\end{align*}\n\\]\ne.g. AR(1):\n\\[\n\\begin{align*}\ny_t &= \\rho y_{t-1} + \\varepsilon \\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#uses-of-ar-models",
    "href": "slides/lecture03-2-timeseries.html#uses-of-ar-models",
    "title": "Time Series",
    "section": "Uses of AR Models",
    "text": "Uses of AR Models\nAR models are commonly used for prediction: bond yields, prices, electricity demand, short-run weather.\nBut may have little explanatory power: what causes the autocorrelation?"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#ar1-models",
    "href": "slides/lecture03-2-timeseries.html#ar1-models",
    "title": "Time Series",
    "section": "AR(1) Models",
    "text": "AR(1) Models\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) AR(1) model\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#diagnosing-autocorrelation",
    "href": "slides/lecture03-2-timeseries.html#diagnosing-autocorrelation",
    "title": "Time Series",
    "section": "Diagnosing Autocorrelation",
    "text": "Diagnosing Autocorrelation\n\n\nPlot \\(\\varsigma(i)\\) over a series of lags.\nData generated by an AR(1) with \\(\\rho = 0.7\\).\nNote: Even without an explicit dependence between \\(y_{t-2}\\) and \\(y_t\\), \\(\\varsigma(2) \\neq 0\\).\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Autocorrelation Function"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#partial-autocorrelation",
    "href": "slides/lecture03-2-timeseries.html#partial-autocorrelation",
    "title": "Time Series",
    "section": "Partial Autocorrelation",
    "text": "Partial Autocorrelation\n\n\nInstead, can isolate \\(\\varsigma(i)\\) independent of \\(\\varsigma(i-k)\\) through partial autocorrelation.\nTypically estimated through regression.\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Partial autocorrelation Function"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#ar1-and-stationarity",
    "href": "slides/lecture03-2-timeseries.html#ar1-and-stationarity",
    "title": "Time Series",
    "section": "AR(1) and Stationarity",
    "text": "AR(1) and Stationarity\n\\[\\begin{align*}\ny_{t+1} &= \\rho y_t + \\varepsilon_t \\\\\ny_{t+2} &= \\rho^2 y_t + \\rho \\varepsilon_t + \\varepsilon_{t+1} \\\\\ny_{t+3} &= \\rho^3 y_t + \\rho^2 \\varepsilon_t + \\rho \\varepsilon_{t+1} + \\varepsilon_{t+2} \\\\\n&\\vdots\n\\end{align*}\n\\]\nUnder what condition will \\(\\mathbf{y}\\) be stationary?\n\nStationarity requires \\(| \\rho | &lt; 1\\)."
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#ar1-variance",
    "href": "slides/lecture03-2-timeseries.html#ar1-variance",
    "title": "Time Series",
    "section": "AR(1) Variance",
    "text": "AR(1) Variance\nThe conditional variance \\(\\mathbb{V}[y_t | y_{t-1}] = \\sigma^2\\).\nUnconditional variance for stationary \\(\\mathbb{V}[y_t]\\):\n\\[\n\\begin{align*}\n\\mathbb{V}[y_t] &= \\rho^2 \\mathbb{V}[y_{t-1}] + \\mathbb{V}[\\varepsilon] \\\\\n&= \\rho^2 \\mathbb{V}[y_t] + \\sigma^2 \\\\\n&= \\frac{\\sigma^2}{1 - \\rho^2}.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#ar1-joint-distribution",
    "href": "slides/lecture03-2-timeseries.html#ar1-joint-distribution",
    "title": "Time Series",
    "section": "AR(1) Joint Distribution",
    "text": "AR(1) Joint Distribution\nAssume stationarity and zero-mean process.\nNeed to know \\(\\text{Cov}[y_t, y_{t+h}]\\) for arbitrary \\(h\\).\n\\[\n\\begin{align*}\n\\text{Cov}[y_t, y_{t-h}] &= \\text{Cov}[\\rho^h y_{t-h}, y_{t-h}] \\\\\n&= \\rho^h \\text{Cov}[y_{t-h}, y_{t-h}] \\\\\n&= \\rho^h \\frac{\\sigma^2}{1-\\rho^2}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#ar1-joint-distribution-1",
    "href": "slides/lecture03-2-timeseries.html#ar1-joint-distribution-1",
    "title": "Time Series",
    "section": "AR(1) Joint Distribution",
    "text": "AR(1) Joint Distribution\n\\[\n\\begin{align*}\n\\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{0}, \\Sigma) \\\\\n\\Sigma &= \\frac{\\sigma^2}{1 - \\rho^2} \\begin{pmatrix}1 & \\rho & \\ldots & \\rho^{T-1}  \\\\ \\rho & 1 & \\ldots & \\rho^{T-2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\rho^{T-1} & \\rho^{T-2} & \\ldots & 1\\end{pmatrix}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#alternatively..",
    "href": "slides/lecture03-2-timeseries.html#alternatively..",
    "title": "Time Series",
    "section": "Alternatively..",
    "text": "Alternatively..\nAn often “easier approach” (often more numerically stable) is to whiten the series sample/compute likelihoods in sequence:\n\\[\n\\begin{align*}\ny_1 & \\sim N\\left(0, \\frac{\\sigma^2}{1 - \\rho^2}\\right) \\\\\ny_t &\\sim N(\\rho y_{t-1} , \\sigma^2)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#dealing-with-trends",
    "href": "slides/lecture03-2-timeseries.html#dealing-with-trends",
    "title": "Time Series",
    "section": "Dealing with Trends",
    "text": "Dealing with Trends\n\\[y_t = \\underbrace{x_t}_{\\text{fluctuations}} + \\underbrace{z_t}_{\\text{trend}}\\]\n\nModel trend with regression: \\[y_t - a - bt \\sim N(\\rho (y_{t-1} - a - bt), \\sigma^2)\\]\nModel the spectrum (frequency domain).\nDifference values: \\(\\hat{y}_t = y_t - y_{t-1}\\)"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#be-cautious-with-detrending",
    "href": "slides/lecture03-2-timeseries.html#be-cautious-with-detrending",
    "title": "Time Series",
    "section": "Be Cautious with Detrending!",
    "text": "Be Cautious with Detrending!\n\n\nDragons: Extrapolating trends identified using “curve-fitting” is highly fraught, complicating projections.\nBetter to have an explanatory model (next week!)…\n\n\n\n\nDog Growth Extrapolation Cartoon\n\n\n\nSource: Reddit (original source unclear…)"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#code-for-ar1-model",
    "href": "slides/lecture03-2-timeseries.html#code-for-ar1-model",
    "title": "Time Series",
    "section": "Code for AR(1) model",
    "text": "Code for AR(1) model\n\n\nCode\nfunction ar1_loglik_whitened(θ, dat)\n    # might need to include mean or trend parameters as well\n    # subtract trend from data to make this mean-zero in this case\n    ρ, σ = θ\n    T = length(dat)\n    ll = 0 # initialize log-likelihood counter\n    for i = 1:T\n        if i == 1\n            ll += logpdf(Normal(0, sqrt(σ^2 / (1 - ρ^2))), dat[i])\n        else\n            ll += logpdf(Normal(ρ * dat[i-1], σ), dat[i])\n        end\n    end\n    return ll\nend\n\nfunction ar1_loglik_joint(θ, dat)\n    # might need to include mean or trend parameters as well\n    # subtract trend from data to make this mean-zero in this case\n    ρ, σ = θ\n    T = length(dat)\n    # compute all of the pairwise lags\n    # this is an \"outer product\"; syntax will differ wildly by language\n    H = abs.((1:T) .- (1:T)')\n    P = ρ.^H # exponentiate ρ by each lag\n    Σ = σ^2 / (1 - ρ^2) * P\n    ll = logpdf(MvNormal(zeros(T), Σ), dat)\n    return ll\nend\n\n\n\n\nar1_loglik_joint (generic function with 1 method)"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#ar1-example",
    "href": "slides/lecture03-2-timeseries.html#ar1-example",
    "title": "Time Series",
    "section": "AR(1) Example",
    "text": "AR(1) Example\n\n\n\n\nCode\nρ = 0.6\nσ = 0.25\nT = 25\nts_sim = zeros(T)\n# simulate synthetic AR(1) series\nfor t = 1:T\n    if t == 1\n        ts_sim[t] = rand(Normal(0, sqrt(σ^2 / (1 - ρ^2))))\n    else\n        ts_sim[t] = rand(Normal(ρ * ts_sim[t-1], σ))\n    end\nend\n\nplot(1:T, ts_sim, linewidth=3, xlabel=\"Time\", ylabel=\"Value\", title=L\"$ρ = 0.6, σ = 0.25$\")\nplot!(size=(600, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Simulated AR(1) data\n\n\n\n\n\n\n\nCode\nlb = [-0.99, 0.01]\nub = [0.99, 5]\ninit = [0.6, 0.3]\n\noptim_whitened = Optim.optimize(θ -&gt; -ar1_loglik_whitened(θ, ts_sim), lb, ub, init)\nθ_wn_mle = round.(optim_whitened.minimizer; digits=2)\n@show θ_wn_mle;\n\noptim_joint = Optim.optimize(θ -&gt; -ar1_loglik_joint(θ, ts_sim), lb, ub, init)\nθ_joint_mle = round.(optim_joint.minimizer; digits=2)\n@show θ_joint_mle;\n\n\nθ_wn_mle = [0.4, 0.2]\nθ_joint_mle = [0.4, 0.2]"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#key-points-1",
    "href": "slides/lecture03-2-timeseries.html#key-points-1",
    "title": "Time Series",
    "section": "Key Points",
    "text": "Key Points\n\nTime series exhibit serial dependence (autocorrelation\nAR(1) probability models: joint vs. whitened likelihoods\nIn general, AR models useful for forecasting/when we don’t care about explanation, pretty useless for explanation.\nSimilar concepts in spatial data: spatial correlation (distance/adjacency vs. time), lots of different models."
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#for-more-on-time-series",
    "href": "slides/lecture03-2-timeseries.html#for-more-on-time-series",
    "title": "Time Series",
    "section": "For More On Time Series",
    "text": "For More On Time Series\n\n\n\nCourses\n\nSTSCI 4550/5550 (Applied Time Series Analysis),\nCEE 6790 (heavy focus on spectral analysis and signal processing)\n\n\n\nBooks\n\nShumway & Stoffer (2025)\nHyndman & Athanasopoulos (2021)\nDurbin & Koopman (2012)\nBanerjee et al. (2011)\nCressie & Wikle (2011)"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#questions-to-seed-discussion",
    "href": "slides/lecture03-2-timeseries.html#questions-to-seed-discussion",
    "title": "Time Series",
    "section": "Questions to Seed Discussion",
    "text": "Questions to Seed Discussion\n\nWhat was your key takeaway?\nWhat do you think the pros/cons are of the risk and storyline approaches?\nHow well do you think the authors argued their case?"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#assessments",
    "href": "slides/lecture03-2-timeseries.html#assessments",
    "title": "Time Series",
    "section": "Assessments",
    "text": "Assessments\nHW1: Due Friday at 9pm.\nQuiz: Available after class."
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#references-scroll-for-full-list",
    "href": "slides/lecture03-2-timeseries.html#references-scroll-for-full-list",
    "title": "Time Series",
    "section": "References (Scroll For Full List)",
    "text": "References (Scroll For Full List)\n\n\n\n\nBanerjee, S., Carlin, B. P., & Gelfand, A. E. (2011). Hierarchical modeling and analysis for spatial data, second edition (2nd ed.). Philadelphia, PA: Chapman & Hall/CRC. https://doi.org/10.1201/b17115\n\n\nCressie, N., & Wikle, C. K. (2011). Statistics for Spatio-Temporal Data. Hoboken, NJ: Wiley.\n\n\nDurbin, J., & Koopman, S. J. (2012). Time series analysis by state space methods (2nd ed.). London, England: Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199641178.001.0001\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3rd ed). Melbourne, Australia: OTexts. Retrieved from https://otexts.com/fpp3/\n\n\nShumway, R. H., & Stoffer, D. S. (2025). Time series analysis and its applications. Springer."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#last-unit",
    "href": "slides/lecture11-2-modeling-extremes.html#last-unit",
    "title": "Modeling Extreme Values",
    "section": "Last Unit",
    "text": "Last Unit\nModel Evaluation: How to assess and compare models for selection or assessment of evidence?\n\nOver/Underfitting;\nScoring Rules\nCross-Validation;\nInformation Criteria"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#what-are-some-examples-of-extremes",
    "href": "slides/lecture11-2-modeling-extremes.html#what-are-some-examples-of-extremes",
    "title": "Modeling Extreme Values",
    "section": "What Are Some Examples of Extremes?",
    "text": "What Are Some Examples of Extremes?\n\n\nWhen An Event is Rare?\nWhen A Variable Exceeds Some High Threshold?\nWhen An Event Causes a Catastrophe?"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#two-ways-to-frame-extreme-values",
    "href": "slides/lecture11-2-modeling-extremes.html#two-ways-to-frame-extreme-values",
    "title": "Modeling Extreme Values",
    "section": "Two Ways To Frame “Extreme” Values",
    "text": "Two Ways To Frame “Extreme” Values\nThese are values with a very low probability of occurring, not necessarily high-impact events (which don’t have to be rare!).\n\n“Block” extremes, e.g. annual maxima (block maxima)?\nValues which exceed a certain threshold (peaks over threshold)?"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#example-tide-gauge-data",
    "href": "slides/lecture11-2-modeling-extremes.html#example-tide-gauge-data",
    "title": "Modeling Extreme Values",
    "section": "Example: Tide Gauge Data",
    "text": "Example: Tide Gauge Data\n\n\nCode\nfunction load_data(fname)\n    date_format = \"yyyy-mm-dd HH:MM\"\n    # this uses the DataFramesMeta package -- it's pretty cool\n    return @chain fname begin\n        CSV.File(; dateformat=date_format)\n        DataFrame\n        rename(\n            \"Time (GMT)\" =&gt; \"time\", \"Predicted (m)\" =&gt; \"harmonic\", \"Verified (m)\" =&gt; \"gauge\"\n        )\n        @transform :datetime = (Date.(:Date, \"yyyy/mm/dd\") + Time.(:time))\n        select(:datetime, :gauge, :harmonic)\n        @transform :weather = :gauge - :harmonic\n        @transform :month = (month.(:datetime))\n    end\nend\n\ndat = load_data(\"data/surge/norfolk-hourly-surge-2015.csv\")\n\np1 = plot(dat.datetime, dat.gauge; ylabel=\"Gauge Measurement (m)\", label=\"Observed\", legend=:topleft, xlabel=\"Date/Time\", bottom_margin=5mm, left_margin=5mm, right_margin=5mm)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: 2015 tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#example-tide-gauge-data-1",
    "href": "slides/lecture11-2-modeling-extremes.html#example-tide-gauge-data-1",
    "title": "Modeling Extreme Values",
    "section": "Example: Tide Gauge Data",
    "text": "Example: Tide Gauge Data\n\n\nCode\nplot!(p1, dat.datetime, dat.harmonic, label=\"Predicted\", alpha=0.7)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: 2015 tide gauge data with predicted harmonics from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#example-detrended-data",
    "href": "slides/lecture11-2-modeling-extremes.html#example-detrended-data",
    "title": "Modeling Extreme Values",
    "section": "Example: Detrended Data",
    "text": "Example: Detrended Data\n\n\nCode\nplot(dat.datetime, dat.weather; ylabel=\"Gauge Weather Variability (m)\", label=\"Detrended Data\", linewidth=3, legend=:topleft,  xlabel=\"Date/Time\", bottom_margin=5mm, left_margin=5mm, right_margin=5mm)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: 2015 detrended tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#example-block-maxima",
    "href": "slides/lecture11-2-modeling-extremes.html#example-block-maxima",
    "title": "Modeling Extreme Values",
    "section": "Example: Block Maxima",
    "text": "Example: Block Maxima\n\n\nCode\npbm = plot(dat.datetime, dat.weather; ylabel=\"Gauge Weather Variability (m)\", label=\"Detrended Data\", linewidth=2, legend=:topleft, xlabel=\"Date/Time\", bottom_margin=5mm, left_margin=5mm, right_margin=5mm)\nmax_dat = combine(dat -&gt; dat[argmax(dat.weather), :], groupby(transform(dat, :datetime =&gt; x-&gt;yearmonth.(x)), :datetime_function))\nscatter!(max_dat.datetime, max_dat.weather, label=\"Monthly Maxima\", markersize=5)\nmonth_start = collect(Date(2015, 01, 01):Dates.Month(1):Date(2015, 12, 01))\nvline!(DateTime.(month_start), color=:black, label=:false, linestyle=:dash)\n\np = histogram(\n    max_dat.weather,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"Count\",\n    bins=5,\n    ylabel=\"\",\n    yticks=[]\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(pbm, p; layout=l, link=:y, ylims=(-0.4, 1.4), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: 2015 detrended tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#example-peaks-over-threshold",
    "href": "slides/lecture11-2-modeling-extremes.html#example-peaks-over-threshold",
    "title": "Modeling Extreme Values",
    "section": "Example: Peaks Over Threshold",
    "text": "Example: Peaks Over Threshold\n\n\nCode\nthresh = 0.5\nppot = plot(dat.datetime, dat.weather; linewidth=2, ylabel=\"Gauge Weather Variability (m)\", label=\"Observations\", legend=:top, xlabel=\"Date/Time\")\nhline!([thresh], color=:red, linestyle=:dash, label=\"Threshold\")\nscatter!(dat.datetime[dat.weather .&gt; thresh], dat.weather[dat.weather .&gt; thresh], markershape=:x, color=:black, markersize=3, label=\"Exceedances\")\n\np2 = histogram(\n    dat.weather[dat.weather .&gt; thresh],\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"Count\",\n    ylabel=\"\",\n    yticks=[]\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(ppot, p2; layout=l, link=:y, ylims=(-0.4, 1.4), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: 2015 detrended tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#block-maxima-1",
    "href": "slides/lecture11-2-modeling-extremes.html#block-maxima-1",
    "title": "Modeling Extreme Values",
    "section": "Block Maxima",
    "text": "Block Maxima\nGiven independent and identically-distributed random variables \\(X_1, X_2, \\ldots, X_{mk}\\), what is the distribution of maxima of “blocks” of size \\(m\\):\n\\[\\tilde{X}_i = \\max_{(i-1)m &lt; j \\leq im} X_j,\\]\nfor \\(i = 1, 2, \\ldots, k\\)?"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#how-are-maxima-distributed",
    "href": "slides/lecture11-2-modeling-extremes.html#how-are-maxima-distributed",
    "title": "Modeling Extreme Values",
    "section": "How Are Maxima Distributed?",
    "text": "How Are Maxima Distributed?\nConsider the CDF \\(F_X\\) of \\(X\\) and let \\(Y_n = \\max \\{X_1, \\ldots, X_n \\}\\).\n\\[\n\\begin{align*}\nF_{Y_n}(z) &= \\mathbb{P}[Y_n \\leq z] \\\\\n&= \\mathbb{P}(X_1 \\leq z, \\ldots, X_n \\leq z) \\\\\n&= \\mathbb{P}(X_1 \\leq z) \\times \\ldots \\times \\mathbb{P}(X_n \\leq z) \\\\\n&= \\mathbb{P}(X \\leq z)^n = F_X^n(z)\n\\end{align*}\n\\]\nThis means that errors in estimating \\(F_X\\) become exponentially worse for \\(F_{Y_n}\\)."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#sum-stable-distributions",
    "href": "slides/lecture11-2-modeling-extremes.html#sum-stable-distributions",
    "title": "Modeling Extreme Values",
    "section": "Sum-Stable Distributions",
    "text": "Sum-Stable Distributions\nIf we have independent and identically-distributed variables$\\(X_1, X_2, \\ldots, X_n\\).\n\\[Y_1 = \\sum_{i=1}^k X_i, \\quad Y_2 = \\sum_{i={k+1}}^{2k} X_i, \\quad \\ldots, \\quad Y_m = \\sum_{i={n-k+1}}^{n} X_i\\]\nSum-Stability: \\(Y \\stackrel{d}{=} a_kX + b_k\\) for some constants \\(a, b \\geq 0\\).\nExample: If \\(X \\sim N(\\mu, \\sigma)\\), \\(Y \\sim N(k\\mu, k\\sigma)\\)"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#max-stability",
    "href": "slides/lecture11-2-modeling-extremes.html#max-stability",
    "title": "Modeling Extreme Values",
    "section": "Max Stability",
    "text": "Max Stability\n\\[\\begin{align*}\n\\max\\{x_1, \\ldots, &x_{2n}\\}\\\\\n&= \\max\\{\\max\\{x_1, \\ldots, x_n\\}, \\max\\{x_{n+1}, \\ldots, x_2n\\} \\}\n\\end{align*}\\]\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: 2015 detrended tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#stability-postulate",
    "href": "slides/lecture11-2-modeling-extremes.html#stability-postulate",
    "title": "Modeling Extreme Values",
    "section": "Stability Postulate",
    "text": "Stability Postulate\nBy analogy with sum stability, postulate that for a max-stable process,\n\\[F^n(z) = F(a_n z + b_n)\\]\nfor some constants \\(a_n, b_n \\geq 0\\)."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#extremal-types-theorem",
    "href": "slides/lecture11-2-modeling-extremes.html#extremal-types-theorem",
    "title": "Modeling Extreme Values",
    "section": "Extremal Types Theorem",
    "text": "Extremal Types Theorem\nLet \\(X_1, \\ldots, X_n\\) be a sample of i.i.d. random variables.\nIf a limiting distribution for \\(Y = \\max\\{X_1, \\ldots, X_n\\}\\) exists, it can only by given as a Generalized Extreme Value (GEV) distribution:\n\\[H(y) = \\exp\\left\\{-\\left[1 + \\xi\\left(\\frac{y-\\mu}{\\sigma}\\right)\\right]^{-1/\\xi}\\right\\},\\] defined for \\(y\\) such that \\(1 + \\xi(y-\\mu)/\\sigma &gt; 0\\)."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#gev-distributions",
    "href": "slides/lecture11-2-modeling-extremes.html#gev-distributions",
    "title": "Modeling Extreme Values",
    "section": "GEV Distributions",
    "text": "GEV Distributions\nGEV distributions have three parameters:\n\nlocation \\(\\mu\\);\nscale \\(\\sigma &gt; 0\\);\nshape \\(\\xi\\)."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#gev-types",
    "href": "slides/lecture11-2-modeling-extremes.html#gev-types",
    "title": "Modeling Extreme Values",
    "section": "GEV “Types”",
    "text": "GEV “Types”\n\n\n\n\\(\\xi &gt; 0\\): Frèchet (heavy-tailed)\n\\(\\xi = 0\\): Gumbel (light-tailed)\n\\(\\xi &lt; 0\\): Weibull (bounded)\n\n\n\n\nCode\np1 = plot(-2:0.1:6, GeneralizedExtremeValue(0, 1, 0.5), linewidth=3, color=:red, label=L\"$\\xi = 1/2$\", lw=3)\nplot!(-4:0.1:6, GeneralizedExtremeValue(0, 1, 0), linewidth=3, color=:green, label=L\"$\\xi = 0$\", lw=3)\nplot!(-4:0.1:2, GeneralizedExtremeValue(0, 1, -0.5), linewidth=3, color=:blue, label=L\"$\\xi = -1/2$\", lw=3)\nscatter!((-2, 0), color=:red, label=:false)\nscatter!((2, 0), color=:blue, label=:false)\nylabel!(\"Density\")\nxlabel!(L\"$x$\")\nplot!(size=(600, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Shape of the GEV distribution with different choices of \\(\\xi\\)."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#gev-types-1",
    "href": "slides/lecture11-2-modeling-extremes.html#gev-types-1",
    "title": "Modeling Extreme Values",
    "section": "GEV Types",
    "text": "GEV Types\n\n\\(\\xi &lt; 0\\): extremes are bounded (the Weibull distribution comes up in the context of temperature and wind speed extremes).\n\\(\\xi &gt; 0\\): tails are heavy, and there is no expectation if \\(\\xi &gt; 1\\). Common for streamflow, storm surge, precipitation.\nThe Gumbel distribution (\\(\\xi = 0\\)) is common for extremes from normal distributions, doesn’t occur often in real-world data."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#return-levels",
    "href": "slides/lecture11-2-modeling-extremes.html#return-levels",
    "title": "Modeling Extreme Values",
    "section": "Return Levels",
    "text": "Return Levels\nReturn Levels are a central (if poorly named) concept in risk analysis.\nThe \\(T\\)-year return level is the value expected to be observed on average once every \\(T\\) years.\nFrom a GEV fit to annual maxima: \\(T\\)-year return level is the \\(1-1/T\\) quantile."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#return-periods",
    "href": "slides/lecture11-2-modeling-extremes.html#return-periods",
    "title": "Modeling Extreme Values",
    "section": "Return Periods",
    "text": "Return Periods\nThe return period of an extreme value is the inverse of the exceedance probability.\nExample: The 100-year return period has an exceedance probability of 1%, e.g. the 0.99 quantile.\nReturn levels are associated with the analogous return period."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#san-francisco-tide-gauge-data",
    "href": "slides/lecture11-2-modeling-extremes.html#san-francisco-tide-gauge-data",
    "title": "Modeling Extreme Values",
    "section": "San Francisco Tide Gauge Data",
    "text": "San Francisco Tide Gauge Data\n\n\nCode\n# load SF tide gauge data\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\nd_sf = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=d_sf.datetime[ma_offset+1:end-ma_offset], residual=d_sf.gauge[ma_offset+1:end-ma_offset] .- moving_average(d_sf.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5\n)\np2 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"Count\",\n    ylabel=\"\",\n    yticks=[]\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(1, 1.7), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n# find GEV fit\n# for most distributions we could use Distributions.fit(), but this isn't implemented in Distributions.jl for GEV\ninit_θ = [1.0, 1.0, 1.0]\ngev_lik(θ) = -sum(logpdf(GeneralizedExtremeValue(θ[1], θ[2], θ[3]), dat_annmax.residual))\nθ_mle = Optim.optimize(gev_lik, init_θ).minimizer\n\n\n\n\n3-element Vector{Float64}:\n 1.2587093828052343\n 0.05626679672643659\n 0.0171950735119165\n\n\nFigure 8: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#gev-parameters",
    "href": "slides/lecture11-2-modeling-extremes.html#gev-parameters",
    "title": "Modeling Extreme Values",
    "section": "GEV Parameters",
    "text": "GEV Parameters\n\n\n\n\\(\\mu =  1\\.26\\)\n\\(\\sigma = 0\\.06\\)\n\\(\\xi =  0\\.02\\)\n\n\n\n\nCode\np = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    label=\"Data\",\n    xlabel=\"Annual Maximum (m)\",\n    ylabel=\"PDF\",\n    yticks=[],\n    left_margin=10mm,\n    right_margin=10mm\n)\n\nplot!(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), linewidth=5, label=\"GEV Fit\", color=:gold4)\nxlims!((1, 1.75))\nplot!(size=(800, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: GEV fit to annual maxima of San Francisco Tide Gauge Data"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#gev-return-levels",
    "href": "slides/lecture11-2-modeling-extremes.html#gev-return-levels",
    "title": "Modeling Extreme Values",
    "section": "GEV Return Levels",
    "text": "GEV Return Levels\n\n\n100-year return level: 1.53m\n\n\n\nCode\ngevcdf(x) = cdf(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), x) # find quantiles of values\nrl_100 = quantile(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), 0.99)\n\nplot(1:0.01:1.75, 1 .- gevcdf.(1:0.01:1.75), yaxis=:log, yticks=10 .^ collect(-3.0:1.0:0.0), xlabel=\"Water Level (m)\", ylabel=\"Exceedance Probability (1/yr)\", lw=3)\nplot!(1:0.01:rl_100, 0.01 .+ zeros(length(1:0.01:rl_100)), color=:red, lw=2)\nplot!(rl_100 .+ zeros(length(-4:0.01:-2)), 10 .^collect(-4.0:0.01:-2.0), color=:red, lw=2)\nscatter!([rl_100], [0.01], color=:red, markersize=5)\nplot!(size=(800, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: GEV fit to annual maxima of San Francisco Tide Gauge Data"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#gev-vs-lognormal",
    "href": "slides/lecture11-2-modeling-extremes.html#gev-vs-lognormal",
    "title": "Modeling Extreme Values",
    "section": "GEV vs LogNormal",
    "text": "GEV vs LogNormal\n\n\nCode\np = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    label=\"Data\",\n    xlabel=\"Annual Maximum (m)\",\n    ylabel=\"PDF\",\n    yticks=[],\n    left_margin=10mm,\n    right_margin=10mm\n)\nplot!(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), linewidth=5, label=\"GEV Fit\", color=:gold4)\nplot!(fit(LogNormal, dat_annmax.residual), linewidth=5, label=\"LogNormal Fit\", color=:darkred)\nxlims!((1, 1.75))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: GEV fit to annual maxima of San Francisco Tide Gauge Data"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#gev-q-q-plot",
    "href": "slides/lecture11-2-modeling-extremes.html#gev-q-q-plot",
    "title": "Modeling Extreme Values",
    "section": "GEV Q-Q Plot",
    "text": "GEV Q-Q Plot\n\nCode\np1 = qqplot(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), dat_annmax.residual, \n    linewidth=3, markersize=5,\n    xlabel=\"Theoretical Quantile\",\n    ylabel=\"Empirical Quantile\"\n)\nplot!(p1, size=(600, 450))\n\nreturn_periods = 2:500\n# get GEV return levels\nreturn_levels = quantile.(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), 1 .- (1 ./ return_periods))\n# fit lognormal to get return levels for comparison\nlognormal_fit = fit(LogNormal, dat_annmax.residual)\nreturn_levels_lognormal = quantile.(lognormal_fit, 1 .- (1 ./ return_periods))\n\n# function to calculate exceedance probability and plot positions based on data quantile\nfunction exceedance_plot_pos(y)\n    N = length(y)\n    ys = sort(y; rev=false) # sorted values of y\n    nxp = xp = [r / (N + 1) for r in 1:N] # exceedance probability\n    xp = 1 .- nxp\n    return xp, ys\nend\nxp, ys = exceedance_plot_pos(dat_annmax.residual)\n\np2 = plot(return_periods, return_levels, linewidth=5, color=:gold4, label=\"GEV Model Fit\", bottom_margin=5mm, left_margin=5mm, right_margin=10mm, legend=:bottomright)\nplot!(p2, return_periods, return_levels_lognormal, linewidth=5, color=:darkred, label=\"LogNormal Model Fit\")\nscatter!(p2, 1 ./ xp, ys, label=\"Observations\", color=:black, markersize=6)\nxlabel!(p2, \"Return Period (yrs)\")\nylabel!(p2, \"Return Level (m)\")\nxlims!(-1, 300)\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) GEV fit to annual maxima of San Francisco Tide Gauge Data\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 12"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#be-careful-about-the-shape-parameter",
    "href": "slides/lecture11-2-modeling-extremes.html#be-careful-about-the-shape-parameter",
    "title": "Modeling Extreme Values",
    "section": "Be Careful About The Shape Parameter!",
    "text": "Be Careful About The Shape Parameter!\n\n\n\n\\(\\xi\\) can be difficult to constrain;\nOften lots of uncertainty (large standard errors);\nControls frequency of “extreme” extremes\n\n\n\n\n\n\nHouse flood risk sensitivity\n\n\n\n\nSource: Zarekarizi et al. (2020)"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#choosing-a-block-size",
    "href": "slides/lecture11-2-modeling-extremes.html#choosing-a-block-size",
    "title": "Modeling Extreme Values",
    "section": "Choosing a Block Size",
    "text": "Choosing a Block Size\n\nSimilar to the “bias-variance tradeoff”\nSmall block sizes: poor approximation by GEV, resulting in greater bias\nLarge block sizes: fewer data points, greater estimation variance\nOne year is common for practical reasons, but can go finer with sufficient data so long as block maxima can be treated as independent."
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#getting-standard-errors",
    "href": "slides/lecture11-2-modeling-extremes.html#getting-standard-errors",
    "title": "Modeling Extreme Values",
    "section": "Getting Standard Errors",
    "text": "Getting Standard Errors\n\nFisher information: find Hessian of log-likelihood of GEV at MLE.\nParametric bootstrap from fitted GEV (will tend towards more narrow intervals)\nDo not use the non-parametric bootstrap"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#nonstationary-gevs",
    "href": "slides/lecture11-2-modeling-extremes.html#nonstationary-gevs",
    "title": "Modeling Extreme Values",
    "section": "Nonstationary GEVs",
    "text": "Nonstationary GEVs\nSuppose block maxima can change with the block (e.g. climate change).\n\\[Y_i \\sim GEV(\\mu(x_i), \\sigma(x_i), \\xi(x_i))\\]\nCommon to use generalized linear framework, e.g.\n\\[Y_i \\sim GEV(\\mu_0 + \\mu_1 x_i, \\sigma_0 + \\sigma_1 x_i, \\xi_0 + \\xi_1 x_i)\\]"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#be-careful-with-nonstationary-gevs",
    "href": "slides/lecture11-2-modeling-extremes.html#be-careful-with-nonstationary-gevs",
    "title": "Modeling Extreme Values",
    "section": "Be Careful with Nonstationary GEVs",
    "text": "Be Careful with Nonstationary GEVs\n\nGEV parameters are already subject to large uncertainties; nonstationarity makes this worse.\nBe particularly careful with non-stationary \\(\\xi\\) (shape).\nMost common approach: only treat \\(\\mu\\) as non-stationary, \\[Y_i \\sim GEV(\\mu_0 + \\mu_1 x_i, \\sigma, \\xi)\\]"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#nonstationary-return-levels",
    "href": "slides/lecture11-2-modeling-extremes.html#nonstationary-return-levels",
    "title": "Modeling Extreme Values",
    "section": "Nonstationary Return Levels",
    "text": "Nonstationary Return Levels\nNo non-conditional return periods since distribution depends on \\(x\\).\nFirst specify a covariate value \\(x\\), then can calculate return levels based on that value.\nFor example: what are 100-year return levels of temperatures in 2020, 2050, and 2100?"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#upcoming-schedule-1",
    "href": "slides/lecture11-2-modeling-extremes.html#upcoming-schedule-1",
    "title": "Modeling Extreme Values",
    "section": "Upcoming Schedule",
    "text": "Upcoming Schedule\nMonday: Peaks Over Thresholds\nWednesday: Missing and Censored Data"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#assessments",
    "href": "slides/lecture11-2-modeling-extremes.html#assessments",
    "title": "Modeling Extreme Values",
    "section": "Assessments",
    "text": "Assessments\nFriday: HW4 Due"
  },
  {
    "objectID": "slides/lecture11-2-modeling-extremes.html#references",
    "href": "slides/lecture11-2-modeling-extremes.html#references",
    "title": "Modeling Extreme Values",
    "section": "References",
    "text": "References\n\n\n\n\nZarekarizi, M., Srikrishnan, V., & Keller, K. (2020). Neglecting uncertainties biases house-elevation decisions to manage riverine flood risks. Nat. Commun., 11, 5361. https://doi.org/10.1038/s41467-020-19188-9"
  },
  {
    "objectID": "policies/integrity.html",
    "href": "policies/integrity.html",
    "title": "Academic Integrity",
    "section": "",
    "text": "While we don’t expect academic integrity to be a problem (we trust you, which is why the course is designed to be open and collaborative!), these policy statements are unfortunately necessary due to a very small minority of students. If you are worried about whether something you are doing is in danger of violating this course’s academic integrity policy, please don’t hesitate to ask during lecture, in office hours, by email, or on Ed Discussion.",
    "crumbs": [
      "Expectations",
      "Academic Integrity"
    ]
  },
  {
    "objectID": "policies/integrity.html#expectations",
    "href": "policies/integrity.html#expectations",
    "title": "Academic Integrity",
    "section": "Expectations",
    "text": "Expectations\n\n\n\n\n\n\nImportantOur Expectations\n\n\n\nEach student or group must write their own solutions in their own words and submit work that reflects their understanding. All external sources, including classmates, must be properly credited and referenced.**\n\n\nIf something is submitted with your name on it, you should be able to explain and defend its choices and outcomes. This is true in the workplace as well as in academia.\nFor example, see:\n\nThe Cornell University Code of Academic Integrity;\nThe American Society for Civil Engineers’ Code of Ethics.\n\nThis issue is particularly important in engineering, as lives and well-being depend on the honesty and integrity of engineers. While the stakes are obviously lower in our class, the habits you develop now are related to your practices in the future.\n\nCite Your Sources and Collaborators\nThis course is designed to be highly collaborative: you are free to work with classmates, seek out their advice and insights when you are stuck, and provide help to them when they are stuck. You are also highly encouraged to take advantage of external resources to help you solve problems. But you must cite your sources, with only three exceptions:\n\nHelp from the instructor or course staff;\nCourse materials from this semester;\nSources used in prerequisite courses.\n\nSubmitting someone else’s work or ideas without proper credit is plagiarism, even if you have their permission. Plagiarism, and facilitating plagiarism, are academic integrity violations and will be treated accordingly. Conversely, citing your sources will never negatively impact your grade, and may help if you relied on poor information which you then used properly.\n\n\nUse Your Own Words\nDuplication of any source, even with proper citation, is plagiarism. Your submissions must reflect your own ideas, understanding, and work. This means:\n\nVerbatim copying of lecture notes (including code samples) is plagiarism;\nCopying from old homework or exam solutions is plagiarism;\nSubmitting work in your name done by another student is plagiarism;\nAllowing another student is submit your work in their name is an academic integrity violation.\nMinor wordsmithing to change individual words while preserving a duplicated text’s flow of ideas is also an academic integrity violation. For example, taking ChatGPT output and copying it is clearly an academic integrity violation; changing a few words here or there to avoid “duplication” is still a violation.\n\nThis also goes for any code that you submit. While you are welcome to draw on sources such as ChatGPT or Stack Overflow for debugging, syntax help, or programming patterns, you should independently incorporate these ideas into your independently written problem solutions and code (and credit them in the comments!). If the instructor or TA ask you to explain what is going on in your code, and you cannot, there’s a problem.",
    "crumbs": [
      "Expectations",
      "Academic Integrity"
    ]
  },
  {
    "objectID": "policies/integrity.html#dont-cheat-or-lie",
    "href": "policies/integrity.html#dont-cheat-or-lie",
    "title": "Academic Integrity",
    "section": "Don’t Cheat or Lie",
    "text": "Don’t Cheat or Lie\nThe usual no-nos from other classes apply: don’t do anything like\n\nCollaborate on or copy exam answers;\nModify or destroy another students’ work;\nHave an imposter complete work on your behalf;\nChange answers before asking for a regrade;\nFalsely claim to have submitted an assignment or have taken an exam.",
    "crumbs": [
      "Expectations",
      "Academic Integrity"
    ]
  },
  {
    "objectID": "policies/integrity.html#penalties",
    "href": "policies/integrity.html#penalties",
    "title": "Academic Integrity",
    "section": "Penalties",
    "text": "Penalties\n\nIf you commit an academic integrity violation on a homework assignment, you will be given a zero for that entire homework assignment, which will not be dropped under any circumstances.\nIf you commit an academic integrity violation on an exam, you will be given a zero for the entire exam, which will not be dropped under any circumstances.\nIf you commit a second academic integrity violation, you will be given an F for the course.\nIf your first violation is particularly egregious, you will be given an F for the course.\n\nAll academic integrity violations will also be reported to the students’ college and department.",
    "crumbs": [
      "Expectations",
      "Academic Integrity"
    ]
  },
  {
    "objectID": "policies/integrity.html#group-work",
    "href": "policies/integrity.html#group-work",
    "title": "Academic Integrity",
    "section": "Group Work",
    "text": "Group Work\nYour term project involves the submission of work jointly as a group. This means that every member of the group is responsible for the entire project. At a minimum, every person whose name is on a submission must read, understand, and approve the entirety of the submission (this is the same standard used for authorship on academic publications). Allowing someone else to add your name to something you did not contribute to is an academic integrity violation and will result in a zero for the entire submission.",
    "crumbs": [
      "Expectations",
      "Academic Integrity"
    ]
  },
  {
    "objectID": "policies/exam.html",
    "href": "policies/exam.html",
    "title": "Exam Policies",
    "section": "",
    "text": "This page includes some information on the exams for BEE 4750/5750, including policies and logistics.",
    "crumbs": [
      "Grading",
      "Exam Policies"
    ]
  },
  {
    "objectID": "policies/exam.html#exam-logistics",
    "href": "policies/exam.html#exam-logistics",
    "title": "Exam Policies",
    "section": "Exam Logistics",
    "text": "Exam Logistics\nThis class has two prelims, both in class:\n\nPrelim 1: Friday, March 6;\nPrelim 2: Friday, May 1.\n\nEach is worth 15% of your grade. Exams will be hand-written and then will be scanned into Gradescope (by course staff) for grading and feedback; the physical copy of your exam will not be returned.\n\nExam Topics and Structure\nThe emphasis is on concepts; there will be no major formulas requiring memorization that you will need to use, and no programming. You will be potentially be asked to derive mathematical versions of models, apply or explain concepts from class, and interpret model outputs.\nIn general, the same homework grading rubric applies to exam problems, as appropriate (.e.g. nothing about code). The deadly sins will still be penalized, and your solutions should be clear with transparent and well-communicated reasoning.\n\n\nAcademic Integrity\nThe exams are closed-book and closed-note, and closed everything else. No medically unnecessary electronic devices are allowed to be used during the exam, including headphones and smart watches.\nExams are also strictly confidential until the conflict and accomodation exams have been taken. Do not discuss your exam with anyone in person or online. Once all exams are taken, we will discuss the exams in class.\n\n\nDifficulty\nWhile we will not hand out sample exams, exam problems will be closer in difficulty to lab problems or simple in-class examples than the homework, since you have two weeks to solve homework problems and have full acccess to external resources, course staff, and classmates. The goal is to assess your ability to understand concepts, set up problems, determine appropriateness of methods, and interpret the output, rather than on doing computations. We cannot, and do not, expect you to solve problems with homework-level difficulty in the restricted exam setting.",
    "crumbs": [
      "Grading",
      "Exam Policies"
    ]
  },
  {
    "objectID": "policies/exam.html#accomodations-conflicts-and-makeup-exams",
    "href": "policies/exam.html#accomodations-conflicts-and-makeup-exams",
    "title": "Exam Policies",
    "section": "Accomodations, Conflicts, and Makeup Exams",
    "text": "Accomodations, Conflicts, and Makeup Exams\n\nSDS Accomodations\nPlease make sure that any exam-related accomodations have been provided to the course staff ahead of the first exam. All exams requiring accomodations will be scheduled and delivered through the Alternative Testing Program.\n\n\nConflict and Makeup Exams\nIf you have an academic conflict or a university-approved reason why you cannot take the exam in class, you should schedule your conflict exam through the Alternative Testing Program.\nIn extreme circumstances, such as long-term illnesses or injuries, that prevent taking both the regular and the conflict exam, we may forgive the prelim and compute your grade without it. This will dramatically increase the weight given to other assessments.",
    "crumbs": [
      "Grading",
      "Exam Policies"
    ]
  },
  {
    "objectID": "policies/homework.html",
    "href": "policies/homework.html",
    "title": "Homework Policies",
    "section": "",
    "text": "This page includes some information on the homework assignments for BEE 4850/5850, including policies and logistics. The goal is to help you get as many points on your homework as possible and to return them as soon as possible.\nThis document is long, but please do read the whole thing (particularly What Homework Is For and the Logistics). Hopefully most of this is obvious, but some of it may not be.",
    "crumbs": [
      "Grading",
      "Homework Policies"
    ]
  },
  {
    "objectID": "policies/homework.html#what-homework-is-for",
    "href": "policies/homework.html#what-homework-is-for",
    "title": "Homework Policies",
    "section": "What Homework Is For",
    "text": "What Homework Is For\nThis class involves two components which are somewhat in tension pedagogically:\n\nConceptual what are systems, how do they behave, what are methods for their analysis and why do we use them;\nComputational how to implement systems analysis methods.\n\nOut of these, we are more interested in the conceptual: the computational methods are a means to an end. What matters more is the act of translating an environmental system into an appropriate mathematical model and interpreting the results of that model to provide insights into the design and management of the system. Getting better at that act requires practice: this is what homework is for! It’s easy to nod and follow along in lecture, or read a derivation, but that’s a trap. You can only learn by doing, and often, failing.\nHomework is the opportunity to practice doing. The practice is more important than the solution, which is why the solution is the least “rewarded” part of the homework rubrics. You can only learn by trying to solve the problem, and, often, getting stuck, because these are new skills that you are developing. And that’s ok — we encourage you to collaborate so you can share ideas and suggestions and give feedback, not to get solutions (and that’s also true for office hours, TA sessions, etc).\nYou also may not even be able to tell what parts of your homework solution are correct. And that’s also ok! We grade your homework and give you homework solutions so you can identify what, if anything, went wrong, and how to do better next time. We always aim for improvement, not perfection, and every homework problem is an opportunity to practice doing that type of problem.\nBut this emphasis on homework as practice also means that you should start the homework early. If you start early, and get stuck, you have the opportunity to get yourself unstuck, either by banging your head against the problem or by talking to others, and that’s how you learn. If you start late and get stuck, you either have to not finish (not great) or cheat (unacceptable under any circumstances).",
    "crumbs": [
      "Grading",
      "Homework Policies"
    ]
  },
  {
    "objectID": "policies/homework.html#homework-logistics",
    "href": "policies/homework.html#homework-logistics",
    "title": "Homework Policies",
    "section": "Homework Logistics",
    "text": "Homework Logistics\n\nAccepting Assignments\n\nHomework assignments will be distributed through GitHub Classroom. When an assignment is released, a link to accept the assignment will be posted on Ed Discussion. You must click this link to accept the assignment or subsequent steps will not work properly. Clicking this link will create your own repository with the files for the assignment. The first time you do this, you will be prompted to connect your GitHub username with your entry in the class roster.\nClone your repository to your computer.\n\n\n\n\n\n\nIt is important that you clone the repository created in the previous step, not the “original” repository. Make sure that the URL looks like https://www.github.com/BEE4750-FA25/&lt;username&gt;-hwxx.git, not https://www.github.com/bee-envsys-cornell/hwxx.git.\n\n\n\n\n\n\nWorking On Assignments\n\nOpen the local repository folder you created when you cloned the remote repository.\n\n\n\n\n\n\nUse File -&gt; Open Folder rather than File -&gt; Open to open the folder; this will make sure that all paths point to the right location and you can use the “Explorer” to see the different files.\n\n\n\nAfter the notebook loads, if prompted to, select the julia-1.11 kernel.\nMake sure that you run the initial notebook cell (starting with Pkg.activate()) to load the Julia environment and install any needed packages.\nYou don’t need to work in the notebook, per se, but if you choose to write your own code files, make sure to add them to the repository with git add. Make sure that you copy the initial notebook cell into your new script(s) to use the appropriate environment.\nAs you work on assignments, make sure to save and frequently push and commit back to GitHub.\nYou can toggle line numbers in a notebook cell with the L key.\n\n\n\nAsking for Assistance\n\nIf you need to share code to ask for help or to provide help, provide a link to the relevant file in your GitHub repository, along with an idea of the relevant cell and line number(s).\nMinimal code snippets can also be copied and pasted into Ed, but these should be self-contained, not chunks copied from your homework. These snippets should illustrate what you think the correct concept and/or syntax should be with a self-contained example; it’s unlikely you will receive useful help if the snippet can’t be evaluated.\nMake sure you document what you’ve tried (including any searches) so we can focus on new solutions, not repeating things that haven’t already worked. If you haven’t looked for help (e.g. searching for an error message), start there.\n\n\n\nSubmitting Assignments\n\nYour final submission must be in a PDF. You can generate this in several ways:\n\nIf you have set up LaTeX on your system, you can convert a notebook directly to PDF in VS Code or Jupyter Lab (this will automatically work on BioHPC without any additional setup);\n\n\n\n\n\n\nIf you are submitting a PDF from a notebook (versus a separate writeup in e.g. Word), make sure that you evaluate all cells in order with Run Cells before converting.\n\n\n\nIf you have not set up LaTeX, you can export a notebook to HTML and save it from your browser as a PDF;\nAs an alternative, when you commit and push a .ipynb file (a Jupyter Notebook) to your repository, we have set up a GitHub Action to automatically render your notebook to a PDF, which you can then download and submit to Gradescope. However, sometimes GitHub actions can take a while or can stall out, so you’ll need to monitor this and give yourself some time. If you’ve waited a while and your notebook isn’t rendering, reach out on Ed and we can figure out what’s going on. In the worst case (you push your changes close to the deadline), the timestamp of your commit will be evidence that you completed the assignment on time, even if we can’t render the PDF until the next day.\nDon’t worry if the GitHub Action says its failed if your assignment (or any given problem) isn’t complete: you may have an error due to your incomplete code that causes the Action to fail. You can ignore this unless you think everything should work!\nIf you have written a report outside of a notebook (e.g. in Word), save this to a PDF.\n\nYou do not need to include code in a written submission, but make sure your final notebook and/or scripts are on your GitHub repository in case there’s an issue.\nPDFs must be submitted by the due date and time to Gradescope with pages tagged. Failure to tag pages will receive a 10% deduction.\nIf you did not receive credit for your group’s submission, please have someone in your group use a regrade request to bring this to our attention, and we will use the list of names at the top of the page to confirm that you contributed to the assignment.\nLate submissions are accepted within 24 hours with a 50% penalty unless an extension was arranged ahead of the original deadline.",
    "crumbs": [
      "Grading",
      "Homework Policies"
    ]
  },
  {
    "objectID": "policies/homework.html#how-to-write-assignments",
    "href": "policies/homework.html#how-to-write-assignments",
    "title": "Homework Policies",
    "section": "How To Write Assignments",
    "text": "How To Write Assignments\nHere are some tips for how to make the grader (the TA or Prof. Srikrishnan) understand what you mean in the time they’re looking at your problem: as noted in the rubrics, if your solution isn’t clear enough for us to follow, you will not receive the points even if your answer is technically correct, because part of demonstrating understanding is the ability to organize and communicate.\n\nBe Honest\n\nWrite everything in your own words. It’s perfectly ok to work with others; in fact, we encourage it! But you should write up and implement everything yourself; it’s very easy to convince yourself that you understand something when you’re mimicking someone else’s solution, and often we realize we don’t actually understand something when we try to write it ourselves.\nCite every outside source you use. You are allowed, in fact encouraged, to use any outside source1 Getting an idea from an outside source is not a problem, and will not lower your grade; if you’re critically evaluating the idea and implementing and writing your solution yourself (see above), then you’re demonstrating understanding even if the idea originated with someone else. But you must give appropriate credit to that source. Taking credit for someone else’s ideas and failing to properly cite an outside source, including your classmates, is plagiarism, and will be treated accordingly.\nThe only sources you do not have to cite are official class materials. If you use the lectures, lecture notes, website materials, homework solutions, etc, you do not have to cite these.\nList everyone that you worked with. Give your classmates proper credit for their assistance. If you get an idea from Ed Discussion, credit the poster. If you’re not sure if you should list someone as a collaborator, err on the side of including them. For discussions in class or in office hours, you don’t have to list everyone who participated in the discussion (though you should if you worked one on one with them), but mention that the class discussion or the office hour was useful.\n\n1 Yes, including ChatGPT and other LLMs, though you should ask and describe how you used it, including queries and how you incorporated its output into your solution.\n\nBe Clear\n\nWrite legibly. This doesn’t refer to handwriting (since you’ll be submitting PDFs of Jupyter notebooks), but the text itself should be clearly written and well organized, so that it’s easy for the grader to follow your reasoning or explanation. Structuring your solution, and not writing in a stream of consciousness, helps you think more clearly. To reiterate: You will be given no points if the grader cannot easily follow your answer, and the graders have complete discretion to determine this.\nWrite clearly. Use proper spelling, grammar, logic, etc. We will try not to penalize people for not having complete mastery of English, but again, we need to be able to follow your reasoning.\nWrite carefully and completely. We can only grade what you write; nobody can read your mind, and we will not try. The solution that you submit must stand on its own. If your answer is ambiguous, the TA has been instructed to interpret it the wrong way. Regrade requests also cannot be used to add more information to a solution.\nDon’t submit your first draft. For most people2, first drafts are terrible. They are often poorly organized, unclear, and contain gaps or jumps in reasoning. You will likely need to revise, possibly several times, for your answer to be clear, careful, and complete. This is another reason to start the assignment early — if you start late, you may be stuck with your first draft, and your grade is likely to suffer for it.\nState your assumptions. If you think a problem statement is ambiguous and your solution depends on a particular interpretation, or you need to make some assumptions to solve the problem, make it explicit (though do also ask for clarification in class or on Ed Discussion if time allows).\nDon’t rely on your code. The TA will not scrutinize your code, which is a waste of time (again, the code is a means to an end — it works or it doesn’t). If you make it clear what your code is supposed to do with a description, then the TA can tell if this logic is correct and any mistake must be something minor in the code. If you just provide code (even if commented), the TA can’t do this without running your code and debugging, which is not a valuable use of time. We want to focus on your ideas.\n\n2 Myself included!",
    "crumbs": [
      "Grading",
      "Homework Policies"
    ]
  },
  {
    "objectID": "policies/homework.html#be-concise",
    "href": "policies/homework.html#be-concise",
    "title": "Homework Policies",
    "section": "Be Concise",
    "text": "Be Concise\n\nKeep solutions short. Organized answers should not be long. There’s a fine balance between conciseness and completeness: find it!\nDon’t regurgitate. You can reference concepts, models, etc from class without repeating them. Just make it clear what you’re modifying and how you’re using those concepts for that particular problem.\nDon’t bullshit. You will get no points for word salad, even if you accidentally hit on the right answer.",
    "crumbs": [
      "Grading",
      "Homework Policies"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Environmental Data Analysis and Simulation\n        ",
    "section": "",
    "text": "BEE 4850/5850Spring 2026Cornell University\n        \n        \n            This course focuses on probabilistic modeling of environmental data-generating processes, including simulation methods and model evaluation and selection."
  },
  {
    "objectID": "index.html#software-tools",
    "href": "index.html#software-tools",
    "title": "\n            Environmental Data Analysis and Simulation\n        ",
    "section": "Software Tools",
    "text": "Software Tools\n\nI will teach this course using the Julia programming language. Julia is a modern, free, open source language which is designed for scientific computing. No prior knowledge of Julia is required. My recommendation is to use Visual Studio Code with the official Julia extension for coding. However, you can use any programming language you would like to complete the assignments and the final project.\nAssignments will be distributed using GitHub Classroom. You should create a GitHub account, but we will walk through how to use it."
  },
  {
    "objectID": "setup/julia.html",
    "href": "setup/julia.html",
    "title": "Julia",
    "section": "",
    "text": "I recommend installing Julia using the juliaup tool, which will let you easily manage versions in the future and works seamlessly with VS Code. The instructions can be found at the JuliaUp GitHub repository, but we will summarize them here.\n\n\nIf your computer uses Windows, you can install Juliaup from the Windows Store.\n\n\n\nIf you have a Mac, open a terminal (such as the Terminal app) and enter:\ncurl -fsSL https://install.julialang.org | sh\n\n\n\nOnce you install Juliaup, install Julia version 1.11.5 by opening a terminal (in MacOS or Linux) or the command line (in Windows) and entering:\njuliaup add 1.11.5\njuliaup default 1.11.5\nThis will install Julia 1.11.5 and make it the default version, which should maximize package compatibility throughout this course. Going forward, if you want to add new versions or change the default, you can follow the Juliaup instructions. If your code returns an error when you try to load packages, it’s possible you have a different version of Julia: in this case, delete Manifest.toml1 and re-run a code block that contains\n1 Not Project.toml!import Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\nThe line Pkg.instantiate() will install the right package versions for your installation and create a new Manifest.toml file.",
    "crumbs": [
      "Software",
      "Julia"
    ]
  },
  {
    "objectID": "setup/julia.html#sec-install-julia",
    "href": "setup/julia.html#sec-install-julia",
    "title": "Julia",
    "section": "",
    "text": "I recommend installing Julia using the juliaup tool, which will let you easily manage versions in the future and works seamlessly with VS Code. The instructions can be found at the JuliaUp GitHub repository, but we will summarize them here.\n\n\nIf your computer uses Windows, you can install Juliaup from the Windows Store.\n\n\n\nIf you have a Mac, open a terminal (such as the Terminal app) and enter:\ncurl -fsSL https://install.julialang.org | sh\n\n\n\nOnce you install Juliaup, install Julia version 1.11.5 by opening a terminal (in MacOS or Linux) or the command line (in Windows) and entering:\njuliaup add 1.11.5\njuliaup default 1.11.5\nThis will install Julia 1.11.5 and make it the default version, which should maximize package compatibility throughout this course. Going forward, if you want to add new versions or change the default, you can follow the Juliaup instructions. If your code returns an error when you try to load packages, it’s possible you have a different version of Julia: in this case, delete Manifest.toml1 and re-run a code block that contains\n1 Not Project.toml!import Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\nThe line Pkg.instantiate() will install the right package versions for your installation and create a new Manifest.toml file.",
    "crumbs": [
      "Software",
      "Julia"
    ]
  },
  {
    "objectID": "setup/index.html",
    "href": "setup/index.html",
    "title": "Setup",
    "section": "",
    "text": "Getting set up for this course requires the following steps, if you haven’t done them for a previous course, club, or project:\n\nInstall git on your machine if necessary.\nInstall Julia using the juliaup tool.\nInstall and set up VS Code if you don’t already have an editor you’d like to use. VS Code is an editor of choice for Julia, as it as a rich and well-supported Julia extension. If you have a different coding environment, please find instructions for how to set it up to edit Julia.\n\n\n\n\n\n\n\nWarningMac Versions\n\n\n\nIf you have a Mac, be aware if you have an Apple Silicon (M1 or M2) Mac or an Intel Mac. Many of the installations you will go through in this exercise have different versions for each processor type, and the program may not work properly if you still the wrong version. There’s nothing that can’t be undone, but it’s easier to be install the correct version first!",
    "crumbs": [
      "Tools Setup"
    ]
  },
  {
    "objectID": "setup/index.html#setting-up-computing-environment",
    "href": "setup/index.html#setting-up-computing-environment",
    "title": "Setup",
    "section": "",
    "text": "Getting set up for this course requires the following steps, if you haven’t done them for a previous course, club, or project:\n\nInstall git on your machine if necessary.\nInstall Julia using the juliaup tool.\nInstall and set up VS Code if you don’t already have an editor you’d like to use. VS Code is an editor of choice for Julia, as it as a rich and well-supported Julia extension. If you have a different coding environment, please find instructions for how to set it up to edit Julia.\n\n\n\n\n\n\n\nWarningMac Versions\n\n\n\nIf you have a Mac, be aware if you have an Apple Silicon (M1 or M2) Mac or an Intel Mac. Many of the installations you will go through in this exercise have different versions for each processor type, and the program may not work properly if you still the wrong version. There’s nothing that can’t be undone, but it’s easier to be install the correct version first!",
    "crumbs": [
      "Tools Setup"
    ]
  },
  {
    "objectID": "setup/vscode.html",
    "href": "setup/vscode.html",
    "title": "VS Code",
    "section": "",
    "text": "You can skip this section if you already have a coding environment you like; just set it up to work with Julia1. Otherwise, VS Code is as close to an officially supported editor for Julia as you can get. We will follow this guide for setting up VS Code with Julia.\n1 I assume that if this is the case, you know/can figure out how to configure your editor. If you aren’t sure, post on Ed and we can find the instructions.\n\nYou can download VS Code here; open the downloaded file to install.\n\n\n\n\nOpen VS Code.\nSelect View and click Extensions to open the Extension View. This view can also be found on the sidebar with the following logo: \nSearch for julia in the search box. Click the green install button.\nRestart VS Code once the installation is complete. It should automatically find your Julia installation; talk to Vivek if not.\n\nThe Julia VS Code extension offers you some nice features. You can start a REPL (an interactive Julia coding environment) by opening the “Command Palette” (View -&gt; Command Palette, or CTRL/CMD+SHIFT+P) and typing “REPL” to bring up “Julia: Start REPL”. You can also create *.jl files to write Julia code and execute line by line. However, we will primarily use Jupyter notebooks in this class, but this might be useful for testing code or for your project.\n\n\n\nThe Jupyter Notebook extension allows you to export a Jupyter notebook to PDF or to HTML and then to PDF.\n\n\n\n\n\n\nTipPDF Export for Gradescope\n\n\n\nYou will need to export every notebook to a PDF for submission to Gradescope. Direct export to PDF requires a LaTeX installation. If you would like to go this route, please look at the LaTeX installation instructions for your operating system.\nOtherwise, exporting to HTML and then using your browser to save the resulting page to a PDF is a perfect solution.\n\n\nFollow the same instructions as above, but search for jupyter and install the Jupyter extension. Restart VS Code.",
    "crumbs": [
      "Software",
      "VS Code"
    ]
  },
  {
    "objectID": "setup/vscode.html#sec-install-code",
    "href": "setup/vscode.html#sec-install-code",
    "title": "VS Code",
    "section": "",
    "text": "You can skip this section if you already have a coding environment you like; just set it up to work with Julia1. Otherwise, VS Code is as close to an officially supported editor for Julia as you can get. We will follow this guide for setting up VS Code with Julia.\n1 I assume that if this is the case, you know/can figure out how to configure your editor. If you aren’t sure, post on Ed and we can find the instructions.\n\nYou can download VS Code here; open the downloaded file to install.\n\n\n\n\nOpen VS Code.\nSelect View and click Extensions to open the Extension View. This view can also be found on the sidebar with the following logo: \nSearch for julia in the search box. Click the green install button.\nRestart VS Code once the installation is complete. It should automatically find your Julia installation; talk to Vivek if not.\n\nThe Julia VS Code extension offers you some nice features. You can start a REPL (an interactive Julia coding environment) by opening the “Command Palette” (View -&gt; Command Palette, or CTRL/CMD+SHIFT+P) and typing “REPL” to bring up “Julia: Start REPL”. You can also create *.jl files to write Julia code and execute line by line. However, we will primarily use Jupyter notebooks in this class, but this might be useful for testing code or for your project.\n\n\n\nThe Jupyter Notebook extension allows you to export a Jupyter notebook to PDF or to HTML and then to PDF.\n\n\n\n\n\n\nTipPDF Export for Gradescope\n\n\n\nYou will need to export every notebook to a PDF for submission to Gradescope. Direct export to PDF requires a LaTeX installation. If you would like to go this route, please look at the LaTeX installation instructions for your operating system.\nOtherwise, exporting to HTML and then using your browser to save the resulting page to a PDF is a perfect solution.\n\n\nFollow the same instructions as above, but search for jupyter and install the Jupyter extension. Restart VS Code.",
    "crumbs": [
      "Software",
      "VS Code"
    ]
  },
  {
    "objectID": "setup/vscode.html#using-github-with-vs-code",
    "href": "setup/vscode.html#using-github-with-vs-code",
    "title": "VS Code",
    "section": "Using GitHub with VS Code",
    "text": "Using GitHub with VS Code\nThis Youtube video provides an overview of how to use GitHub within VS Code (with chapters for specific steps) and may be worth watching.",
    "crumbs": [
      "Software",
      "VS Code"
    ]
  },
  {
    "objectID": "resources/dataviz.html",
    "href": "resources/dataviz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Here are some resources on types of data visualizations and best practices:\n\nVisualization Theory YouTube series by Dr. Lace Padilla\nData Viz Catalogue\nData Visualization: A Practical Introduction by Dr. Kieran Healy"
  },
  {
    "objectID": "resources/julia.html",
    "href": "resources/julia.html",
    "title": "Julia Resources",
    "section": "",
    "text": "Setting up VS Code and Julia (Youtube)\nJulia Documentation\nJuliaAcademy: Check out the “Introduction to Julia (for programmers)” or “Julia Programming for Nervous Beginners” courses\nQuantEcon MATLAB-Python-Julia cheatsheet\nFastTrack to Julia cheatsheet\nPlotting cheatsheet\nIntroduction to Computational Thinking: a great Julia based course at MIT!\nComprehensive Julia Tutorials: YouTube playlist covering a variety of Julia topics, starting with an introduciton to the language."
  },
  {
    "objectID": "resources/homework.html",
    "href": "resources/homework.html",
    "title": "Homework Policies",
    "section": "",
    "text": "This page includes some information on the homework assignments for BEE 4850/5850, including policies and logistics. The goal is to help you get as many points on your homework as possible and to return them as soon as possible.\nThis document is long, but please do read the whole thing. Hopefully most of this is obvious, but some of it may not be."
  },
  {
    "objectID": "resources/homework.html#homework-logistics",
    "href": "resources/homework.html#homework-logistics",
    "title": "Homework Policies",
    "section": "Homework Logistics",
    "text": "Homework Logistics\nHomework assignments will be assigned on Monday and are generally due two Fridays hence (so you have two “school weeks” to work on the assignment). Solutions should be submitted as PDFs to Gradescope by 9pm on the due date. You do not need to include code unless you want to, but you should always include some description of the logic of the modeling or problem-solving process that your code implements."
  },
  {
    "objectID": "resources/homework.html#how-to-write-assignments",
    "href": "resources/homework.html#how-to-write-assignments",
    "title": "Homework Policies",
    "section": "How To Write Assignments",
    "text": "How To Write Assignments\nHere are some tips for how to make the grader (the TA or Prof. Srikrishnan) understand what you mean in the time they’re looking at your problem: as noted in the rubrics, if your solution isn’t clear enough for us to follow, you will not receive the points even if your answer is technically correct, because part of demonstrating understanding is the ability to organize and communicate.\n\nBe Honest\n\nWrite everything in your own words. It’s perfectly ok to work with others; in fact, we encourage it! But you should write up and implement everything yourself; it’s very easy to convince yourself that you understand something when you’re mimicking someone else’s solution, and often we realize we don’t actually understand something when we try to write it ourselves.\nCite every outside source you use. You are allowed, in fact encouraged, to use any outside source1 Getting an idea from an outside source is not a problem, and will not lower your grade; if you’re critically evaluating the idea and implementing and writing your solution yourself (see above), then you’re demonstrating understanding even if the idea originated with someone else. But you must give appropriate credit to that source. Taking credit for someone else’s ideas and failing to properly cite an outside source, including your classmates, is plagiarism, and will be treated accordingly.\nThe only sources you do not have to cite are official class materials. If you use the lectures, lecture notes, website materials, homework solutions, etc, you do not have to cite these.\nList everyone that you worked with. Give your classmates proper credit for their assistance. If you get an idea from Ed Discussion, credit the poster. If you’re not sure if you should list someone as a collaborator, err on the side of including them. For discussions in class or in office hours, you don’t have to list everyone who participated in the discussion (though you should if you worked one on one with them), but mention that the class discussion or the office hour was useful.\n\n1 Yes, including ChatGPT, though you should ask and describe how you used it.\n\nBe Clear\n\nWrite legibly. This doesn’t refer to handwriting (since you’ll be submitting PDFs of Jupyter notebooks), but the text itself should be clearly written and well organized, so that it’s easy for the grader to follow your reasoning or explanation. Structuring your solution, and not writing in a stream of consciousness, helps you think more clearly. To reiterate: You will be given no points if the grader cannot easily follow your answer, and the graders have complete discretion to determine this.\nWrite clearly. Use proper spelling, grammar, logic, etc. We will try not to penalize people for not having complete mastery of English, but again, we need to be able to follow your reasoning.\nWrite carefully and completely. We can only grade what you write; nobody can read your mind, and we will not try. The solution that you submit must stand on its own. If your answer is ambiguous, the TA has been instructed to interpret it the wrong way. Regrade requests also cannot be used to add more information to a solution.\nDon’t submit your first draft. For most people, first drafts are terrible. They are often poorly organized, unclear, and contain gaps or jumps in reasoning. You will likely need to revise, possibly several times, for your answer to be clear, careful, and complete. This is another reason to start the assignment early — if you start late, you may be stuck with your first draft, and your grade is likely to suffer for it.\nState your assumptions. If you think a problem statement is ambiguous and your solution depends on a particular interpretation, or you need to make some assumptions to solve the problem, make it explicit (though do also ask for clarification in class or on Ed Discussion).\nDon’t rely on your code. The TA will not try to scrutinize your code, which is a waste of time (again, the code is a means to an end — it works or it doesn’t). If you make it clear what your code is supposed to do, then the TA can tell if this logic is correct and any mistake must be something minor in the code. If you just provide code (even if commented), the TA can’t do this without running your code and debugging, which is not a valuable use of time. We want to focus on your ideas.\n\n\n\nBe Concise\n\nKeep solutions short. Organized answers should not be long. There’s a fine balance between conciseness and completeness: find it!\nDon’t regurgitate. You can reference concepts, models, etc from class without repeating them. Just make it clear what you’re modifying and how you’re using those concepts for that particular problem.\nDon’t bullshit. You will get no points for word salad, even if you accidentally hit on the right answer."
  },
  {
    "objectID": "resources/github.html",
    "href": "resources/github.html",
    "title": "GitHub Resources",
    "section": "",
    "text": "Git Basics from The Odin Project.\nLearn Git Branching: An interactive, visual tutorial to how git works.\nVersion Control from MIT’s “CS: Your Missing Semester” course.\nGit and GitHub for Poets: YouTube playlist covering the basics of git and GitHub."
  },
  {
    "objectID": "hw/hw01/hw01.html",
    "href": "hw/hw01/hw01.html",
    "title": "Homework 1: Thinking About Data",
    "section": "",
    "text": "ImportantDue Date\n\n\n\nFriday, 2/6/26, 9:00pm",
    "crumbs": [
      "Homework 1: Intro and EDA"
    ]
  },
  {
    "objectID": "hw/hw01/hw01.html#overview",
    "href": "hw/hw01/hw01.html#overview",
    "title": "Homework 1: Thinking About Data",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this homework assignment is to introduce you to simulation-based data analysis.\n\nProblem 1 asks you to explore whether a difference between data collected from two groups might be statistically meaningful or the result of noise. This problem repeats the analysis from Statistics Without The Agonizing Pain by John Rauser (which is a neat watch!).\nProblem 2 asks you to load and conduct an exploratory analysis of an air quality and health dataset.\nProblem 3 asks you to visualize and conduct an exploratory analysis of a cherry blossom dataset.\nProblem 4 asks you to explore the impacts of selection biases on statistical relationships.\n\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing Random # random number generation and seed-setting\nusing DataFrames # tabular data structure\nusing CSVFiles # reads/writes .csv files\nusing Distributions # interface to work with probability distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools",
    "crumbs": [
      "Homework 1: Intro and EDA"
    ]
  },
  {
    "objectID": "hw/hw01/hw01.html#problems",
    "href": "hw/hw01/hw01.html#problems",
    "title": "Homework 1: Thinking About Data",
    "section": "Problems",
    "text": "Problems\n\nProblem 1 (7 points)\nThe underlying question we would like to address is: what is the influence of drinking beer on the likelihood of being bitten by mosquitoes? There is a mechanistic reason why this might occur: mosquitoes are attracted by changes in body temperature and released CO2, and it might be that drinking beer induces these changes. We’ll analyze this question using (synthetic) data which separates an experimental population into two groups, one which drank beer and the other which drank only water.\nFirst, we’ll load data for the number of bites reported by the participants who drank beer. This is in a comma-delimited file, data/bites.csv (which is grossly overkill for this assignment). Each row contains two columns: the group (beer and water) the person belonged to and the number of times that person was bitten.\nIn Julia, we can do this using CSVFiles.jl, which will read in the .csv file into a DataFrame, which is a typical data structure for tabular data (and equivalent to a Pandas DataFrame in Python or a dataframe in R).\n\n\ndata = DataFrame(load(\"data/bites.csv\")) # load data into DataFrame\n\n# print data variable (semi-colon suppresses echoed output in Julia, which in this case would duplicate the output)\n@show data;\n\n\nHow can we tell if there’s a meaningful difference between the two groups? Naively, we might just look at the differences in group means.\n\n\n\n\n\n\n\nTipBroadcasting\n\n\n\nThe subsetting operations in the below code use .==, which “broadcasts” the element-wise comparison operator == across every element. The decimal in front of == indicates that this should be used element-wise (every pair of elements compared for equality, returning a vector of true or false values); otherwise Julia would try to just check for vector equality (returning a single true or false value).\nBroadcasting is a very specific feature of Julia, so this syntax would look different in a different programming language.\n\n\n\n# split data into vectors of bites for each group\nbeer = data[data.group .== \"beer\", :bites]\nwater = data[data.group .== \"water\", :bites]\n\nobserved_difference = mean(beer) - mean(water)\n@show observed_difference;\n\nobserved_difference = 4.37777777777778\n\n\n\nThis tells us that, on average, the participants in the experiment who drank beer were bitten approximately 4.4 more times than the participants who drank water! Does that seem like a meaningful difference, or could it be the result of random chance?\nWe will use a simulation approach to address this question, as follows.\n\nSuppose someone is skeptical of the idea that drinking beer could result in a higher attraction to mosquitoes, and therefore more bites. To this skeptic, the two datasets are really just different samples from the same underlying population of people getting bitten by mosquitoes, rather than two different populations with different propensities for being bitten. This is the skeptic’s hypothesis, versus our hypothesis that drinking beer changes body temperature and CO2 release sufficiently to attract mosquitoes.\nIf the skeptic’s hypothesis is true, then we can “shuffle” all of the measurements between the two datasets and re-compute the differences in the means. After repeating this procedure a large number of times, we would obtain a distribution of the differences in means under the assumption that the skeptic’s hypothesis is true.\nComparing our experimentally-observed difference to this distribution, we can then evaluate the consistency of the skeptic’s hypothesis with the experimental results.\n\n\n\n\n\n\n\n\nImportantWhy Do We Call This A Simulation-Based Approach?\n\n\n\nThis is a simulation-based approach because the “shuffling” is a non-parametric way of generating new samples from the underlying distribution (more on this later!).\nThe alternative to this approach is to use a statistical test, such as a t-test, which may have other assumptions which may not be appropriate for this setting, particularly given the seemingly small sample sizes.\n\n\n\nProblem 1.1\nConduct the above procedure to generate 50,000 simulated datasets under the skeptic’s hypothesis. Plot a histogram of the results and add a dashed vertical line to show the experimental difference (if you are using Julia, feel free to look at the Making Plots with Julia tutorial on the class website).\n\n\nProblem 1.2\nDraw conclusions about the plausibility of the skeptic’s hypothesis that there is no difference between groups. Feel free to use any quantitative or qualitative assessments of your simulations and the observed difference.\n\n\nProblem 2 (5 points)\n\nLet’s examine the influence of air pollution and temperature on mortality in Chicago, IL. The data/chicago.csv dataset (originally from the gamair R package) contains data on the relationship between environmental conditions (temperature and air quality) and deaths in Chicago from 1987–2000. The dataset contains the following variables:\n\nThe number of non-accidental deaths each day (deaths);\nThe median density over the city of PM10 (large particulate matter) particles (pm10median);\nThe median density over the city of PM 2.5 (small particulate matter) particles (pm25median);\nThe median concentration of ozone (O[3]) (o3median);\nThe median concentration of sulfur dioxide (SO[2]) (so2median);\nThe time in days (time);\nThe daily mean temperature (tmpd).\n\n\n\nProblem 2.1\n\nLoad the dataset and compute the following summary statistics for each column (you can do this using built-in functions as available in the language you are using):\n\nMinimum, 25th percentile, median, 75th percentile, and maximum;\nMean;\nNumber of missing data entries (NAs).\n\nWhat are the units for the temperatures? Explain why there are so many negative values for the pollution variables.\n\nProblem 2.2\nPlot the number of deaths versus the time (convert the time to the calendar date). Describe any patterns that you see.\n\n\nProblem 2.3\nPlot the number of deaths versus the temperature. Describe any patterns that you see.\n\n\nProblem 2.4\nWhat do you think has a clearer influence on the number of deaths: date or temperature?\n\n\nProblem 3 (3 points)\n\nCherry trees flower in the spring, but the opening of their blossoms requires warm temperatures and is delayed by cold. As a result, cherry blossom flowering dates are a good proxy for springtime temperatures. Due to the importance of cherry blossoms in Japanese culture, we have an (incomplete) record of full blossom dates going back to around 800 CE, which was developed by Prof. Yasuyuki Aono and several colleagues1 and used for temperature reconstructions.\n1 Aono, Y., & Omoto, Y. (1993). Variation in the March mean temperature deduced from cherry blossom in Kyoto since the 14th century. Journal of Agricultural Meteorology, 48(5), 635–638. https://doi.org/10.2480/agrmet.48.635\nAono, Y., & Omoto, Y. (1994). Estimation of temperature at Kyoto since the 11th century. Using flowering data of cherry trees in old documents. Journal of Agricultural Meteorology, 49(4), 263–272. https://doi.org/10.2480/agrmet.49.263\nAono, Y., & Kazui, K. (2008). Phenological data series of cherry tree flowering in Kyoto, Japan, and its application to reconstruction of springtime temperatures since the 9th century. International Journal of Climatology: A Journal of the Royal Meteorological Society, 28(7), 905–914. https://doi.org/10.1002/joc.15942 The full data set from Prof. Aono has much more information, such as the data sources and the type of historical evidenceThis blossom date data is provided in data/kyoto.csv2. The three columns are\n\nthe year (CE),\nthe numerical day of the year full flowering (e.g. Jan. 1 is day 1, Dec. 31 is day 365 or 366);\nthe date in the Gregorian calendar (e.g. Mar. 15 is 0315).\n\n\n\nProblem 3.1\n\nLoad the data file and produce the following summary statistics for the day (not the date) of full flowering (you can do this using built-in functions as available in the language you are using):\n\nMinimum, 25th percentile, median, 75th percentile, and maximum;\nMean;\nNumber of missing data entries (NAs).\n\nWhy would it not make sense to calculate these summary statistics for the date column?\n\n\n\nProblem 3.2\nPlot the day of flowering against the year. Add horizontal lines showing the mean date of flowering as well as the 25th and 75th percentiles. Do you see any trends or patterns in the data?\n\n\n\nProblem 4 (5 points)\n\nA scientific funding agency receives 200 proposals in response to a call. The panel is asked to evaluate each proposal on two criteria: scientific rigor and potential impact (or “newsworthiness”). After standardizing each of these scores to independently follow a standard normal distribution (\\(\\text{Normal}(0, 1)\\)), the two standardized scores are summed to get the total score for the proposal. Based on these total scores, the top 10% of the proposals are selected for funding.\nA researcher who is studying the relationship between rigor and impact has used data on the funded proposals to claim high-impact proposals are necessarily less rigorous, and indeed found a statistically significant negative correlation between the rigor and impact scores for the funded proposals. You are more skeptical and believe this effect is an artifact from the selection process, which would make the claim a bit ironic.\n\n\nProblem 4.1\nCreate a generative model for the grant-selection procedure under the null assumption of no correlation between rigor and impact. You can sample rigor and impact scores directly from \\(\\text{Normal}(0, 1)\\). Generate 1,000 datasets using this model and flag the proposals which would be funded in each. Plot a histogram of the correlations between rigor and data for (a) the funded proposals and (b) the overall pool. What do you observe?\n\n\nProblem 4.2\nExplain why, when conditioning on selection for funding, there might be a negative correlation between rigor and impact even if there none generally exists.\n\n\n\n\n\n\nImportantSelection-Distortion Effects\n\n\n\nThese selection-distortion effects are pretty common in observational data, going back to work by Dawes in the 1970s on the lack of predictive ability of admission variables on student success and including studies claiming to identify a causal fingerprint of genes on outcomes.\nThis is, of course, just one example of how not thinking carefully about data-generating processes can fundamentally contaminate statistical analyses, emphasizing that data do not have meaning absent a model for how they were generated.\n\n\n\n\n\nProblem 5 (5 points)\n\nYou are trying to detect how prevalent cheating was on an exam. You are skeptical of the efficacy of just asking the students if they cheated. You are also concerned about privacy — your goal is not to punish individual students, but to see if there are systemic problems that need to be addressed. Someone proposes the following interview procedure, which the class agrees to participate in:\n\nEach student flips a fair coin, with the results hidden from the interviewer. The student answers honestly if the coin comes up heads. Otherwise, if the coin comes up tails, the student flips the coin again, and answers “I did cheat” if heads, and “I did not cheat”, if tails.\n\nYou have a hypothesis that cheating was not prevalent, and the proportion of cheaters was no more than 5% of the class; in other words, we expect 5 “true” cheaters out of a class of 100 students. Our TA is more jaded and thinks that cheating was more rampant, and that 30% of the class cheated. The proposed interview procedure is noisy: the interviewer does not know if an admission means that the student cheated, or the result of a heads. However, it gives us a data-generating process that we can model and analyze for consistency with our hypothesis and that of the TA.\n\n\nProblem 5.1\nDerive and code a simulation model for the above interview procedure given a “true” probability of cheating \\(p\\). Simulate your model (for a class of 100 students) 50,000 times under your hypothesis of 5% cheating, the TA’s hypothesis of 30% cheating, and plot the resulting datasets.\n\n\nProblem 5.2\nHow many “Yes, I cheated” answers do you think you’d need to see to feel confident concluding that the TA was right and why? You don’t need to do a formal analysis, but can reason about this based on your plots.\n\n\n\nReferences",
    "crumbs": [
      "Homework 1: Intro and EDA"
    ]
  },
  {
    "objectID": "hw/hw01/hw01.html#references",
    "href": "hw/hw01/hw01.html#references",
    "title": "Homework 1: Thinking About Data",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Homework 1: Intro and EDA"
    ]
  },
  {
    "objectID": "hw/index.html",
    "href": "hw/index.html",
    "title": "Homework Assignments",
    "section": "",
    "text": "This page contains information about and a schedule of the homework assignments for the semester.",
    "crumbs": [
      "Assignments Overview"
    ]
  },
  {
    "objectID": "hw/index.html#general-information",
    "href": "hw/index.html#general-information",
    "title": "Homework Assignments",
    "section": "General Information",
    "text": "General Information\n\nWhile the instructions for each assignment are available through the linked pages for quick and public access, if you are in the class you must use the link provided in Ed Discussion to accept the assignment. This will ensure that:\n\nYou have compatible versions of all relevant packages provided in the environment;\nYou have a GitHub repository that you can use to share your code.\n\nSubmit assignments by 9:00pm Eastern Time on the due date on Gradescope.\nEvery student should submit their own PDF representing their own understanding of the material. If you worked with others, make sure to credit them.\nMore detailed homework instructions and logistics are available.\nSubmissions must be PDFs. Make sure that you tag the pages corresponding to each question; points will be deducted otherwise.\nTo convert the assignment notebook to PDF, you can use VS Code to render the notebook to HTML, and then use your browser to print to PDF. If you have set up LaTeX with VS Code, you can convert directly to a PDF.\nAs an alternative, when you commit and push a .ipynb file (a Jupyter Notebook) to your repository, we have set up a GitHub Action to automatically render your notebook to a PDF, which you can then download and submit to Gradescope. However, sometimes GitHub actions can take a while or can stall out, so you’ll need to monitor this and give yourself some time. If you’ve waited a while and your notebook isn’t rendering, reach out on Ed and we can figure out what’s going on. In the worst case (you push your changes close to the deadline), the timestamp of your commit will be evidence that you completed the assignment on time, even if we can’t render the PDF until the next day.",
    "crumbs": [
      "Assignments Overview"
    ]
  },
  {
    "objectID": "hw/index.html#grading",
    "href": "hw/index.html#grading",
    "title": "Homework Assignments",
    "section": "Grading",
    "text": "Grading\nMake sure to look over the standard homework rubric and familiarize yourself with the homework and grading policies.",
    "crumbs": [
      "Assignments Overview"
    ]
  },
  {
    "objectID": "hw/index.html#schedule",
    "href": "hw/index.html#schedule",
    "title": "Homework Assignments",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\n\n\nAssignment\nTopic\nInstructions\nSolutions\nAssigned\nDue\n\n\n\n\nHW 1\nIntro and Exploratory Data Analysis\n\n\nJan. 21, 2026\nFeb. 06, 2026\n\n\nHW 2\nLinear Models and DAGs\n \nFeb. 09, 2026\nFeb. 20, 2026\n\n\n\nHW 3\nGeneralized Linear Models and Time Series\n \nFeb. 23, 2026\nMar. 06, 2026\n\n\n\nHW 4\nMonte Carlo\n \nMar. 16, 2026\nMar. 27, 2026\n\n\n\nHW 5\nThe Bootstrap and Multiple Imputation\n \nApr. 06, 2026\nApr. 17, 2026\n\n\n\nHW 6\nModel Scoring and Hypothesis Testing\n \nApr. 20, 2026\nMay. 20, 2026",
    "crumbs": [
      "Assignments Overview"
    ]
  },
  {
    "objectID": "faq/index.html",
    "href": "faq/index.html",
    "title": "FAQ",
    "section": "",
    "text": "Julia is open source. It has a great balance between intuitive syntax and speed. It was originally designed to share MATLAB’s mathematically-oriented syntax and Python’s readability. It can be used interactively, as more of a “scripting” language, but can be optimized to run code almost as quickly as C/C++. And it has a committed development base which is actively extending its capabilities: JuMP.jl lets us program optimization problems in a syntax that’s very close to the mathematical expressions, and the Turing.jl ecosystem has added some fantastic statistical and machine-learning libraries, among others. It’s becoming a common language among power systems modelers and environmental economists, among other applied communities.\nWhy not some common alternatives?\n\nR is fantastic for statistics and data visualization, but it has a very unique syntax, and can be slow and struggles to scale to large-scale problems; while we won’t approach those limits in this class, there’s no need to go there for our purposes.\nPython is extremely readable, but also can be very slow unless you’re using libraries which are written in C++, and the syntax for these can be jarringly different than native Python.\nC/C++ are overkill for this class, and focusing on them would distract from our core engineering learning objectives.\nMATLAB is not free or open source. It also has some features which do not align with good broader coding and code maintainance practices (for example, having a single long function per file).\n\nUltimately, there are trade-offs with any language, but for me the pros outweigh the cons given the variety of programming tasks that we will be doing in this course.\n\n\n\nThere’s no “perfect” language. Julia has a lot of pros, but other languages are optimized for different use cases. R is designed for statistical computing, but can struggle with very large data and has…unique syntax. Python has the most well-developed ecosystem (it’s still the main option if you use geospatial data), but can be relatively slow even for common tasks without a lot of optimization.\n\n\n\nNo. You can solve all problems and work on your project using any programming language of your choice. Our ability to give you technical help may be limited if you do not use Julia, but we can talk through the logic of your problem-solving approach.\n\n\n\nThere are a few on the Julia Resources page; please suggest additional resources!\n\n\n\nFirst, look at the Julia tutorials on the website to see if the relevant syntax is covered. If not, check the Julia Resources page to see if one of the provided cheatsheets lets you translate from another programming language to Julia. If those still don’t help, try Google; in particular, look for answers from https://stackoverflow.com or https://discourse.julialang.org.\nIf all of that fails, please post on Ed Discussion, and include what you’ve searched for and why they aren’t quite what you need. A stylized example of what you’d like to achieve is also helpful. As a last resort, come to office hours, but be aware you’ll have a low priority if you haven’t done the above steps, and the odds are that we’ll have to do those as well."
  },
  {
    "objectID": "faq/index.html#julia",
    "href": "faq/index.html#julia",
    "title": "FAQ",
    "section": "",
    "text": "Julia is open source. It has a great balance between intuitive syntax and speed. It was originally designed to share MATLAB’s mathematically-oriented syntax and Python’s readability. It can be used interactively, as more of a “scripting” language, but can be optimized to run code almost as quickly as C/C++. And it has a committed development base which is actively extending its capabilities: JuMP.jl lets us program optimization problems in a syntax that’s very close to the mathematical expressions, and the Turing.jl ecosystem has added some fantastic statistical and machine-learning libraries, among others. It’s becoming a common language among power systems modelers and environmental economists, among other applied communities.\nWhy not some common alternatives?\n\nR is fantastic for statistics and data visualization, but it has a very unique syntax, and can be slow and struggles to scale to large-scale problems; while we won’t approach those limits in this class, there’s no need to go there for our purposes.\nPython is extremely readable, but also can be very slow unless you’re using libraries which are written in C++, and the syntax for these can be jarringly different than native Python.\nC/C++ are overkill for this class, and focusing on them would distract from our core engineering learning objectives.\nMATLAB is not free or open source. It also has some features which do not align with good broader coding and code maintainance practices (for example, having a single long function per file).\n\nUltimately, there are trade-offs with any language, but for me the pros outweigh the cons given the variety of programming tasks that we will be doing in this course.\n\n\n\nThere’s no “perfect” language. Julia has a lot of pros, but other languages are optimized for different use cases. R is designed for statistical computing, but can struggle with very large data and has…unique syntax. Python has the most well-developed ecosystem (it’s still the main option if you use geospatial data), but can be relatively slow even for common tasks without a lot of optimization.\n\n\n\nNo. You can solve all problems and work on your project using any programming language of your choice. Our ability to give you technical help may be limited if you do not use Julia, but we can talk through the logic of your problem-solving approach.\n\n\n\nThere are a few on the Julia Resources page; please suggest additional resources!\n\n\n\nFirst, look at the Julia tutorials on the website to see if the relevant syntax is covered. If not, check the Julia Resources page to see if one of the provided cheatsheets lets you translate from another programming language to Julia. If those still don’t help, try Google; in particular, look for answers from https://stackoverflow.com or https://discourse.julialang.org.\nIf all of that fails, please post on Ed Discussion, and include what you’ve searched for and why they aren’t quite what you need. A stylized example of what you’d like to achieve is also helpful. As a last resort, come to office hours, but be aware you’ll have a low priority if you haven’t done the above steps, and the odds are that we’ll have to do those as well."
  },
  {
    "objectID": "faq/index.html#github",
    "href": "faq/index.html#github",
    "title": "FAQ",
    "section": "GitHub",
    "text": "GitHub\n\nWhy are we using GitHub?\nGitHub is an industry-standard version control platform. Anecdotally, adding GitHub to your skillset is a great resume booster for engineers who work in computing-heavy environments. For the purposes of our class, GitHub facilitates:\n\nAssignment management through GitHub Classroom;\nSharing of code/notebooks for debugging and asking for help (otherwise, I end up with many files with the same name, and might not open the right one…);\nUsing your repository as your submission in case something goes wrong and you can’t submit your completed assignment on time.\n\n\n\nWhat level of GitHub familiarity will I need?\nNone from the start! And the following commands are all you will need for this course (you can also use GitHub Desktop or use the GitHub functionality in VS Code, which we will discuss in class):\n\ngit clone &lt;github-repository-url&gt;: This is needed to “clone” your assignment repository (initialize your local repository).\ngit commit -m &lt;message&gt;: This is used to “lock in” changes that you’ve made to your files. You should make commits frequently as you make changes so you can revert to prior versions if something goes wrong (and make your messages meaningful so you know what changes you’ve made!)\ngit push: This syncs any committed changes to the remote GitHub repository. You must do this prior to using your repository to ask for help."
  },
  {
    "objectID": "faq/index.html#assignments-homework-and-labs",
    "href": "faq/index.html#assignments-homework-and-labs",
    "title": "FAQ",
    "section": "Assignments (Homework and Labs)",
    "text": "Assignments (Homework and Labs)\n\nHow do I access assignments?\nWe will use GitHub Classroom to manage assessments. Links to accept the assignments and create your repository will be posted on Ed Discussion as they are released, and posts with the links for active assignments will be pinned. The class schedule will link to a page which has a preview of the assignment, but links are not provided from this page to accept the assignment.\nFor the first assignment you accept, you will need to link your GitHub account to your entry in the class roster. You will not need to do this again for the rest of the semester.\n\n\nWhat if there’s an error in the assignment repository?\nThat’s certainly possible! If you believe I’ve made a mistake in writing the assignment, please document it in the Ed Discussion forum. I’ll describe steps for how to fix your files (unfortunately, I can’t do this remotely and push it to everyone’s repositories).\n\n\nI think you made a mistake grading my submission. How do I ask for a regrade?\nOnce grades are released on Gradescope, you have a week to request a regrade for any problems. However:, as noted in the syllabus:\n\nYou must justify your regrade request by telling us what you believe we missed or graded incorrectly;\nWe will only evaluate regrades based on the content you submitted. You should tell us what you think we missed in the solution, but any additional context or solution that you add in your regrade request will not be considered;\nIf we realize that we missed an error in your submission, your grade could decrease.\n\nAs a result, please be thoughtful about regrade requests! If you’reunsure, I am happy to talk to you if you’re unsure why you lost points, and we might realize that a regrade request is appropriate.\nIf you think we were too lenient or incorrectly marked a wrong solution as correct, we want to encourage you to be honest and to reward you for so carefully looking at and self-evaluating your submission! If that happens, and you make the argument that your solution was actually wrong, you new score will be increased by the difference.\n\n\nI think the solutions have a mistake which impacted grades. What do I do?\nLet us know! You can do this on Ed Discussion or in person. We all make mistakes, and I want to be transparent about mine. We also have policies in the syllabus about this. In particular, if you find a major mistake, everyone in the class will get full credit. It is up to my discretion what a major mistake is, but in general it’s one which has a large impact on grades."
  },
  {
    "objectID": "faq/index.html#debugging-code",
    "href": "faq/index.html#debugging-code",
    "title": "FAQ",
    "section": "Debugging Code",
    "text": "Debugging Code\n\nHelp! My code isn’t working.\nBugs are a feature of life when you program. This debugging manifesto has some good tips and principles to keep in mind.\nAdditionally, here are some steps I would follow:\n\nSearch for the error message you’re seeing. Often, you can find posts in the r/Julia subreddit, the official Julia forum, or Stack Overflow for the same or similar issues. You should do this first: if you can find the answer this way, it will be faster than other options, and if you can rule out some approaches that you’ve already tried, it will make the following steps go more smoothly. It will also help you understand what has gone wrong!\nPost on Ed Discussion. For the most effective help, follow the following guidelines (adapted from Stack Overflow’s “How Do I Ask A Good Question?” and Julia’s “Make It Easier To Help You”):\n\nMake your post subject specific and descriptive. “Problem with HW 1 Problem 3” makes it hard for other members of the class community to know if they’re having a similar issue or if they can help. Since the TA and I may not be checking Ed religiously, you’ll get faster responses if other students feel prepared to help. A subject like “Loop not updating properly” makes it much more clear what your core problem is and will help others find your post if they have the same problem.\nIntroduce the problem. What have you tried? What error message are you getting? Have you been able to identify a specific line of code or section of code that’s causing the problem? Ideally, you know what parts of your code work and which have the problem; you will be asked to investigate and test your code accordingly if you haven’t, as otherwise you’re relying on course staff or your fellow students to do basic due diligence.\nProvide code. It’s very difficult to get a sense of what might be going wrong if you just say that you have a problem, even if you share the error message. If you have a small question (you can demonstrate the error with a brief snippet of code which creates the same error), you can directly put this into your Ed post (use the Insert Code button). This is the easiest way to get help! If that’s not possible, please provide a link to your GitHub repository and where to find the relevant codes (files/line numbers).\n\n\n\n\n\n\nNoteDon’t Post Screenshots!\n\n\n\nScreenshots of code are the most useless way to ask for help. Screenshots don’t allow for code to be downloaded or copy-pasted (limiting the ability to test), often can be hard to read, and aren’t accessible. If you post a screenshot, we will ask you to post an illustrative code snippet or a link to the file in your GitHub repository.\n\n\n\n\n\n\n\n\nNoteUpdate Your Repository!\n\n\n\nYou must commit and push your current files to GitHub prior to asking for help, or we won’t be able to see the code that you’re having trouble with.\n\n\n\nCome to office hours. If you can identify the problem, but we’re having trouble solving it via Ed or more urgency is required, please come to office hours and we can try to work through the issue. This is most helpful if you’ve already done some problem-solving on your own.\n\n\n\n\n\n\nWarningGive Yourself Time To Problem-Solve\n\n\n\nWaiting until the last day or two to start an assignment can cause problems if you don’t have time to work through the following steps. If office hours are busy and you haven’t gone through the prior steps, you may be prioritized below than students who are have, and we may not have time to solve your problem. Running into coding challenges is not a valid reason to get an extension on an assignment!\n\n\n\n\n\nCan you help me debug my code?\nYour first task should be to identify what part of your code works as desired, to help narrow down where the bug could be. If your code is broken into functions or smaller blocks, set up simple tests to make find which block(s) are causing the problem. If your code is just one big script, try to divide it into logical sections, then conduct these tests. Once you identify the part of the code that is not working, go line by line to see what particular command is doing something unexpected. Once you’ve identified what line is doing something strange, try to search for how to solve the problem and/or post on Ed with your code snippet (actually copy the code into the post or provide a link to your GitHub repository, with the line number; do not paste a screenshot) and a description of what you’re trying to accomplish and what the specific problem is.\nIf you are not sure how to articulate what your code is trying to do (or are similarly unsure how to divide your code into logical units), that suggests that your problem is not debugging, but rather conceptual: step back and try to write out the logic of your code and how it aligns with the strategy you’ve developed to solve the problem. It might be that there is a conceptual problem rather than a bug, which we can definitely discuss!\nIf, however, you’ve tried the above steps and cannot figure out the bug, come to office hours, but if you have not done the above steps (and so cannot point to where exactly you’re running into a problem and cannot describe what that part of code is trying to accomplish), we will ask you to figure that out on your own first. Otherwise, we cannot fruitfully help — nobody is more familiar with your code and problem-solving strategy than you are.\n\n\nCan I just copy code from slides for re-use?\nYou can, but I don’t recommend it. I provide these code snippets to give you an idea of how you might implement something and to reduce the friction of using Julia. However, copying and pasting almost always results in a lack of understanding of the choices that I’ve made in implementation. You’re typically better off writing the code out to replicate examples yourself to get additional practice. Please ask if you have questions about any of these snippets, especially if you’re trying to translate them into a different language!\n\n\nHow do I get a PDF of my notebook for submission?\nThere are two main options, depending on what you’ve already got set up on your computer.\n\n\n\n\n\n\nImportantRun Notebook Cells Before Submission\n\n\n\nRemember to “Run All” cells prior to conversion and submission, or else we won’t see your results!\n\n\n\nIf you have a Python installation, you can use a tool called nbconvert to convert to a PDF. This is the easiest thing to do within VS Code: with nbconvert and the Jupyter extension, you can [export your notebook] to HTML (which can be saved as a PDF) or directly to a PDF (if you have a TeX installation).\nIf you don’t have a Python installation, within VS Code, you can use IJulia.jl (which will be included in the environments provided with assignments in this class) as follows (enter these commands into the REPL) to open your notebook in your browser:\nusing IJulia\nnotebook()\nThis should open a Jupyter notebook interface in your browser. Navigate to and open the notebook, and once it has completed running, go to File -&gt; Print Preview in the browser menu and Print to PDF.\nThe least reliable option is to use an online renderer, such as nbviewer. nbviewer will import your notebook from GitHub and render it, in which case you can just print to a PDF from your browser. The downside to nbviewer is it sometimes doesn’t render figures properly.\n\nBut remember, you also don’t need to submit a PDF derived from a notebook! The code in the notebook will not be graded; this is just a means to an end. You are graded on how you set up the problem, the results, and your analysis: you can write up solutions in Word or some text editor and include plots, then export that to a PDF.\n\n\nHow do I submit my PDF to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here.\n\n\n\n\n\n\nImportantTag Pages\n\n\n\nMake sure that you mark/tag the pages corresponding to each question! Otherwise, we will deduct 10%. If a problem spans multiple pages, tag them all; if a page includes multiple problems, tag it several times. You may lose points if part of your answer is untagged (these will not be returned through a regrade request)."
  },
  {
    "objectID": "resources/markdown.html",
    "href": "resources/markdown.html",
    "title": "Markdown Resources",
    "section": "",
    "text": "Markdown Cheatsheet\nGitHub Markdown Cheatsheet (note that not all of these commands may work in Jupyter notebooks, as GitHub Markdown is an expanded syntax\nLaTeX Cheatsheet"
  },
  {
    "objectID": "resources/general.html",
    "href": "resources/general.html",
    "title": "General Resources",
    "section": "",
    "text": "UDM14: Google search without AI distortion (or you could use the instructions to make &udm=14 the default)."
  },
  {
    "objectID": "resources/ai.html",
    "href": "resources/ai.html",
    "title": "Generative AI and LLMs",
    "section": "",
    "text": "TL; DR: My recommendation is that you do not use ChatGPT, NotebookLLM, or similar large language model (LLM) tools in this class for substantive tasks (annotating/summarizing readings, writing solutions), though they can be useful for programming tasks. If you do use these tools, please do so after independent effort, and clearly document how you’ve used them.\nI am not opposed to the thoughtful use of LLMs. Contrary to the endless hype and marketing served by their creators and the media, these tools have strong limitations and over-reliance on them can greatly impede the discovery and learning processes1. However, once you understand these limitations and have enough domain knowledge to look for red flags, they can be useful for certain (albeit limited) tasks, and learning how to engage with them responsibly is a legitimate professional skill. But you should be careful — many of these tools are provided at a loss to their companies and there may come a point in the near-to-medium future where they are not available to you without a massive cost.\nThe framing and policies on this page are heavily influenced by Andrew Heiss and Ed Zitron."
  },
  {
    "objectID": "resources/ai.html#bullshit",
    "href": "resources/ai.html#bullshit",
    "title": "Generative AI and LLMs",
    "section": "Bullshit",
    "text": "Bullshit\nThe fundamental problem with LLMs is that they are bullshit generators (Hannigan et al., 2024; Hicks et al., 2024). Bullshit, in the philosophical sense, is text produced without care for the truth (Frankfurt, 2005). It is not a lie, which specifically exists in opposition of truth, or a mistake, which is subject to correction when exposed to divergence from the truth, but it is agnostic to the truth; it simply exists to make the author sound like an authority. Truth simply does not matter to a bullshitter.\n\nHannigan, T. R., McCarthy, I. P., & Spicer, A. (2024). Beware of botshit: How to manage the epistemic risks of generative chatbots. Bus. Horiz., 67, 471–486. https://doi.org/10.1016/j.bushor.2024.03.001\n\nHicks, M. T., Humphries, J., & Slater, J. (2024). ChatGPT is bullshit. Ethics Inf. Technol., 26, 1–10. https://doi.org/10.1007/s10676-024-09775-5\n\nFrankfurt, H. G. (2005). On bullshit. Princeton, NJ: Princeton University Press.\n2 To paraphrase Cosma Shalizi, the appearance that LLMs “reason” or have insights is an ember of autoregression sprinkled with wishful mnemonics3 Even worse, there is evidence that LLMs will just give a plausible-seeming argument instead of following logical steps or transparently communicating their reasoning, even when they are instructed to make their reasoning transparent.4 As put by computational linguist Emily Bender, “If someone uses an LLM as a replacement for search, and the output they get is correct, this is just by chance. Furthermore, a system that is right 95% of the time is arguably more dangerous tthan [sic] one that is right 50% of the time. People will be more likely to trust the output, and likely less able to fact check the 5%.” It’s worth reading the whole post about how using LLM summaries as a substitute for search harms information literacy and disrupts sense-making processes.LLMs literally exist only to produce bullshit. They use a predictive statistical model to guess what next word (or sequence of words) is likely; there is no reference to whether the underlying idea produced by this sequence of words is truthful or even coherent2. This means that, once an LLM goes off track, they are subject to wild hallucinations where they may invent concepts or artifacts (books, articles, etc) that do not exist, merely because they seem plausible as a string of text3. This fundamentally affects the reliability of LLM results for information retrieval4. Additionally, given that LLMs are trained on publicly available text, an increasing amount of which is now generated by LLMs (so-called “AI slop”), the uncritical use of these results can just perpetuate the bullshit cycle.\nAs we are environmental scientists and engineers, there’s another problem, which is the impact on the environment of the computers needed to train and run LLMs. The more judicious we can be with these tools, the better we can manage their energy and water needs for when they’re actually useful."
  },
  {
    "objectID": "resources/ai.html#writing-and-reading",
    "href": "resources/ai.html#writing-and-reading",
    "title": "Generative AI and LLMs",
    "section": "Writing And Reading",
    "text": "Writing And Reading\nSince LLMs only produce bullshit, they cannot help you with the process of writing or engaging with readings. Writing, whether prose or technical solutions, forces you to clarify your ideas and confront where they are vague or half-baked. This is a critical part of the educational experience! Producing plausible-looking but substantively-empty text cannot achieve this goal.\nIf you have written your own initial text but would like to clean it up (grammar, concision, etc), LLMs may be helpful since the substance is already present in your text5. This type of engagement with LLM output can be useful.\n5 Just make sure to carefully edit it to ensure that your ideas are still present and clear and weren’t changed!If you do use an LLM at some part in your writing process, you should cite it and make clear how you engaged with the output. This includes:\n\nWhat prompt(s) did you use?\nHow did the LLM output influence your writing or framing?\n\nThis not only makes it clear to me whether you used the LLM responsibly (otherwise, this borders on plagiarism), but it actually helps you in case there is some bullshit that made it into your answer that is not actually a reflection of your understanding. I will not bother trying to guess if your writing is AI-generated6. Your work will be graded on its own merits, and since we’re looking for thoughtfulness and engagement in your written work, LLM-generated materials are likely to be penalized7 However, if your submission contains evidence of plagiarism or hallucinations (including references), at a minimum you will get a zero and, if the evidence is clear enough, this may be reasonably conclusive that you violated the academic integrity policy by using an LLM without referencing it.\n6 While there are tools that purport to do this, they do not reliably work.7 And if you did not disclose the role of LLMs in generating the work, this will not be a convincing reason for why you should not lose points."
  },
  {
    "objectID": "resources/ai.html#coding",
    "href": "resources/ai.html#coding",
    "title": "Generative AI and LLMs",
    "section": "Coding",
    "text": "Coding\nUsing LLMs for programming is a little different. There are many programming tasks (autocompletion of syntax, interpreting error messages) that are greatly facilitated by the use of LLM tools such as ChatGPT or GitHub CoPilot. Even people who already know what they’re doing tend to solve syntax and debugging problems by Googling or going to forums like Stack Overflow. LLMs are a shorcut for this approach8.\n8 Though you should still be careful, LLM code hallucinations can result in security risks. Caveat emptor.9 This is not going to be a problem in our class, but might be in your future.If you’re trying to learn how to program (or program in a new language or using a new toolkit), the use of LLMs, even for debugging, can be greatly detrimental to this process. As they training data for LLMs are often didactic examples, the output code is often wildly inefficient9. There are also likely to be errors due to the generation mechanism: all the LLM can do is guess what the next line of code is, not reason about whether the overall logic of the code makes sense or if it will run. It’s hard to track down these errors if you played no role in the development of the code. This is particularly true if you’re dependent on an LLM to think through debugging, since you won’t know how to find where the LLM.\nMy suggestion for how to use LLMs for coding are:\n\nTry to write your own code first. At the very least, think through the logic of what a solution would look like and write down any relevant equations. Then try to write down a version of that in code form. It’s okay to use syntax-checking and autocomplete tools here, but try to think about what the command is and look at some documentation (or ask on Ed Discussion) if it’s not clear to you.\nIf you run into errors, first see if they’re obvious. In Julia, for example, many errors are the result of not using broadcasting. Being able to spot these common error messages is useful and fast.\nIf you run into further errors, or cannot tell why a particular piece of code is not working, then feel free to use an LLM10. Just make sure you don’t just copy and paste output, but, if the code works, try to understand how it differed from your own so you can not make the same mistakes next time.\n\n\n\n10 Appropriately documented, of course."
  },
  {
    "objectID": "resources/data.html",
    "href": "resources/data.html",
    "title": "Data",
    "section": "",
    "text": "There are a number of places you can look to find interesting datasets.\n\nData is Plural: A weekly newsletter from Jèremy Singer-Vine. Not just (or typically) environmental data, but interesting nevertheless.\nGoogle Dataset Search\nClimate Trace: Emissions tracking data.\nOECD Environmental Data\nOur World in Data\nResource Watch\nEPA Data Catalog\nU.S. Government Open Data Portal"
  },
  {
    "objectID": "setup/homework.html",
    "href": "setup/homework.html",
    "title": "Assignment Workflow",
    "section": "",
    "text": "Your assignments (labs and homeworks) will be distributed as Jupyter Notebooks (.ipynb files) using GitHub Classroom, and will be submitted to Gradescope as PDF files. This means that the following steps are required for each assignment1.\nIf you are totally new to git, you can go through the Software Carpentry “Version Control with Git” course, which includes links to several git cheatsheets."
  },
  {
    "objectID": "setup/homework.html#accepting-assignments",
    "href": "setup/homework.html#accepting-assignments",
    "title": "Assignment Workflow",
    "section": "Accepting Assignments",
    "text": "Accepting Assignments\nWhen an assignment is released, I will post a link to GitHub on Ed Discussion. Clicking on this link will cause you to “accept” the assignment from GitHub Classroom. This means that GitHub will create a repository under your account which duplicates the repository I’ve released.\nThe first time you accept an assignment (probably Lab 1), you will need to link your GitHub account to the course roster before you can accept the assignment. If you find that either your spot in the course roster has already been assigned to a different GitHub account or that you are not on the roster, please reach out ASAP.\nBecause GitHub Classroom creates a new repository for you, you should not use the “template” repository, as you will be unable to push any changes back to GitHub. In other words, do not clone a repository with the name bee-envsys-cornell/hwx.git or bee-envsys-cornell/labx.git, but look for a repository named like bee-envsys-cornell-FA25/&lt;your-username&gt;-hwx.git. If you accidentally clone the template repository, do not worry: you can re-clone your repository and move any files you edited to the new location."
  },
  {
    "objectID": "setup/homework.html#syncing-changes-to-github",
    "href": "setup/homework.html#syncing-changes-to-github",
    "title": "Assignment Workflow",
    "section": "Syncing Changes To GitHub",
    "text": "Syncing Changes To GitHub\nYou are not required to engaged with GitHub beyond accepting and cloning assignment repositories to your local computer. However, to\n\nlearn how to work more fluently with version control2;\nprepare to work with a group on your project; or\nask for help3;\n\n2 A hireable skill that you should put on your resume!3 It’s much easier for us (or your classmates) to look at a file on GitHub, especially if you can provide line numbers of the code section(s) that you are unsure about4 Which you should always do before making any changes locally to avoid so-called fast-forward errors.it is useful to commit and push your files to GitHub so they are synced with the remote repository hosted by GitHub. We will walk through each of these steps, as well as pulling changes back from the remote repository to your local machine4.\ngit works by saving “snapshots” of sequences of changes to the files tracked in a repository. Once one of these “snapshots” is saved, git automatically tracks the changes to the files that were changed between earlier saved “snapshots” and the current one; this means that visually, the repository directory just looks like the set of files without any redundancies for previous versions5.\n5 Contrast this with a form of version control which you may have used before, where you save many different versions of files reflecting different changes, such as assignment_v1.ipynb, assignment_v2.ipynb, …, assignment_v10.ipynb, etc.6 A good commit message should succintly reflect the purpose of those changes: write “Solve problem 2” instead of listing all of the changes to files.Changes to files are recorded through commits, which keep track of the states of a set of files and tags them with a message which reflects the changes which were made6. The commit-based approach is useful because it lets you see exactly what was changed for a given commit (using git diff &lt;filename&gt;) and to roll back changes associated with a particular commit if you decide that the changes were bad, and by looking at the commit messages you can see a history of the purpose of changes to the code.\nHow do you commit changes? Let’s suppose that you’re working on Homework 1, with a file called hw01.ipynb. You have tried solving problems 1 and 2 and would like to save those changes to GitHub instead of just locally7.\n7 In an ideal world, you’d have committed the changes after you solve problem 1, but let’s say you skipped that step\nYou need to add the file(s) which had changes you want to record (you don’t have to do this for all files which have changes, just the ones you want to associate with this commit). From a terminal:\ngit add hw01.ipynb &lt;optionally add other files in a list&gt; \nYou can also use a GitHub GUI8 or the VS Code git interface (which we will discuss in class).\nThen commit the changes:\ngit commit -m \"Solved problems 1 and 2\"\nFinally, push the changes to the remote repository:\ngit push\n\n8 I don’t have any experience with this, but there should be instructions9 For example, you make changes on your computer at home, commit and push them, and then use your laptop to make changes but not starting from the same point that you left off.Once you start working with GitHub, you’ll want to ensure that your repositories are synced before you make any local changes, as many GitHub errors are caused by commit sequences which are out of sync9. To do this, you should reflexively use git pull before making any changes; this will sync your local repository with the last committed state of the remote repository."
  },
  {
    "objectID": "setup/homework.html#exporting-to-pdf",
    "href": "setup/homework.html#exporting-to-pdf",
    "title": "Assignment Workflow",
    "section": "Exporting To PDF",
    "text": "Exporting To PDF\nEventually, you will need to convert your solution to a PDF. This can be a little tricky with Jupyter Notebooks. Before you do anything, make sure you run all of your notebook cells in order.\nThe most direct option uses a Python package called nbconvert, which requires both Python and nbconvert to be installed. Using this route, you can convert to an HTML website and then save that website to a PDF, or (if you have LaTeX installed; I wouldn’t encourage this if you don’t have any other need for it) you can convert directly to a PDF.\nSome other options include:\n\nnbviewer: share the GitHub URL of your notebook and it will render it. There are sometimes weird issues with images.\nbinder\nfastpages: this will use some code to automatically render your might require a little bit of setup with your GitHub repository, but I’m happy to help if you decide to go this route.\n\nIn the absolute worst case, if you are unable to get any of these solutions to work, send us (on Ed) a link to your GitHub repository. We will take the last commited version of your notebook which was before the due date and can render it as a PDF for you."
  },
  {
    "objectID": "setup/homework.html#submitting-to-gradescope",
    "href": "setup/homework.html#submitting-to-gradescope",
    "title": "Assignment Workflow",
    "section": "Submitting to Gradescope",
    "text": "Submitting to Gradescope\nSubmitting solution PDFs to Gradescope should be straightforward. The most important things are:\n\nMake sure to tag pages associated with a given problem! Not doing this creates a lot of extra work for the TA and will result in a 10% penalty. If you make a mistake tagging and this results in a loss of points, you can file a regrade request.\nGradescope will automatically flag an assignment as late if it is received after the due date/time (typically 9:00pm on the due date). You will not be able to submit after the late deadline (typically 24 hours after the original due date/time). If you need an extension to either the regular or late due dates, please reach out to the course staff ahead of the due date; our options to facilitate extensions afterwards are limited as we will upload solutions after the late deadline."
  },
  {
    "objectID": "setup/git.html",
    "href": "setup/git.html",
    "title": "git and GitHub",
    "section": "",
    "text": "If you have already installed git, you don’t need to worry about this section. If you haven’t, download the appropriate version for your operating system.",
    "crumbs": [
      "Software",
      "git and GitHub"
    ]
  },
  {
    "objectID": "setup/git.html#installing-git",
    "href": "setup/git.html#installing-git",
    "title": "git and GitHub",
    "section": "",
    "text": "If you have already installed git, you don’t need to worry about this section. If you haven’t, download the appropriate version for your operating system.",
    "crumbs": [
      "Software",
      "git and GitHub"
    ]
  },
  {
    "objectID": "setup/git.html#create-github-account",
    "href": "setup/git.html#create-github-account",
    "title": "git and GitHub",
    "section": "Create GitHub Account",
    "text": "Create GitHub Account",
    "crumbs": [
      "Software",
      "git and GitHub"
    ]
  },
  {
    "objectID": "setup/git.html#setting-up-github-account",
    "href": "setup/git.html#setting-up-github-account",
    "title": "git and GitHub",
    "section": "Setting Up GitHub Account",
    "text": "Setting Up GitHub Account\n\n\n\n\n\n\nIf you already have a GitHub account, you can use that for this course and do not need to create a new account.\n\n\n\nCreate a GitHub account. It doesn’t have to be linked to your Cornell email or use your Cornell NetID.\nFor labs and homework assignments (we will work on Lab 1 in class and Homework 1 is available), you should use the GitHub Classroom link to “accept” the assignment provided on Ed Discussion, which will give you your own GitHub repository for that assignment. The first time you click one of these links, you will need to link your place on the course roster with your GitHub account. After this, you will not need to do so again.",
    "crumbs": [
      "Software",
      "git and GitHub"
    ]
  },
  {
    "objectID": "lit_critique/index.html",
    "href": "lit_critique/index.html",
    "title": "Literature Critique Instructions",
    "section": "",
    "text": "For the literature critique, you should pick a paper of your choice involving some aspect of data analysis and critically evaluate its statistical and scientific choices. Your critique should discuss how the statistical, data, and modeling choices support (or don’t) the scientific conclusions of the paper and provide ideas for how these could be strengthened.\nYou should submit a short written evaluation (2-3 pages, not including figures or references) on 5/4. It should address the following questions:\n\nWhat is the key question that the paper is trying to address? Why is it scientifically meaningful?\nWhat data are the paper using to address their question? How appropriate is this choice of data?\nHow are the data modeled? What assumptions are embedded in the approach?\nWhat are the key conclusions? How are they presented?\nHow supportable are the conclusions based on the data and methods? Are they too strong?\nWhat would you improve or build upon the analysis? This can include different data or modeling choices, approaches to the analysis, and/or visualizations and presentations of results."
  },
  {
    "objectID": "policies/grading.html",
    "href": "policies/grading.html",
    "title": "Grading Policies",
    "section": "",
    "text": "Homework assignments are graded by the TA on Gradescope based on the rubrics. Homework will be graded as soon as possible, ideally within a couple of weeks.\nLabs are graded by the TA based on effort and progress towards completion, on a scale of 0-3.\nExams are graded by Prof. Srikrishnan and the TA using similar rubrics to the homework, though exams will focus less on solving problems. These will be scanned and graded on Gradescope.\nProjects are graded by Prof. Srikrishnan. The proposals will be graded primarily for completeness and for feedback and will be returned ASAP. Presentations will be graded by a combination of peer reviewers and Prof. Srikrishnan.",
    "crumbs": [
      "Grading",
      "Grading Policies"
    ]
  },
  {
    "objectID": "policies/grading.html#how-work-is-graded",
    "href": "policies/grading.html#how-work-is-graded",
    "title": "Grading Policies",
    "section": "",
    "text": "Homework assignments are graded by the TA on Gradescope based on the rubrics. Homework will be graded as soon as possible, ideally within a couple of weeks.\nLabs are graded by the TA based on effort and progress towards completion, on a scale of 0-3.\nExams are graded by Prof. Srikrishnan and the TA using similar rubrics to the homework, though exams will focus less on solving problems. These will be scanned and graded on Gradescope.\nProjects are graded by Prof. Srikrishnan. The proposals will be graded primarily for completeness and for feedback and will be returned ASAP. Presentations will be graded by a combination of peer reviewers and Prof. Srikrishnan.",
    "crumbs": [
      "Grading",
      "Grading Policies"
    ]
  },
  {
    "objectID": "policies/grading.html#late-policies-and-extension-requests",
    "href": "policies/grading.html#late-policies-and-extension-requests",
    "title": "Grading Policies",
    "section": "Late Policies and Extension requests",
    "text": "Late Policies and Extension requests\n\nAssignments (exercises and homeworks) can be submitted up to 24 hours late for 50% credit. Submissions after 24 hours will not be accepted unless an extension was granted (in which case there is no late penalty until the extension date). This lets us release solutions and return grades ASAP.\nRequest extensions ahead of the deadline. Justified extensions will only be granted for university-approved reasons or emergencies such as illness, injury, learning accomodations, etc and will be limited to 1-2 days, depending on the circumstance. Deadlines in other courses, job interviews, etc. are not reasons for extensions. Ask yourself if the circumstances leading to your extension request only interfered with submission due to poor planning or because they rendered you incapable of work and/or submission. Extensions requested after the deadline will only be considered prior to solution release and for extreme circumstances (e.g. hospitalization during the last day; hence limited communication access).\nUnder extreme circumstances, we will forgive assignments. There are circumstances under which a 24 hour extension is insufficient, such as long-term illness or serious injury. In these cases, we will compute your course grade as if the forgiven assignment never occurred, which will result in your exams and other assignments having more weight.",
    "crumbs": [
      "Grading",
      "Grading Policies"
    ]
  },
  {
    "objectID": "policies/grading.html#regrade-requests",
    "href": "policies/grading.html#regrade-requests",
    "title": "Grading Policies",
    "section": "Regrade Requests",
    "text": "Regrade Requests\n\nSubmit regrade requests on Gradescope within one week of the grade release. We recommend talking to Prof. Srikrishnan or your TA about grading concerns before submitting a regrade request, but no grades will be changed outside of a formal request through Gradescope.\nAll regrade requests must include a brief justification for the request or they will not be considered. Good justifications include (but are not limited to):\n\nMy answer agrees with the posted solution, but I still lost points.\nI lost 4 points for something, but the rubric says it should only be worth 2 points.\nYou took points off for something, but it’s right here.\nMy answer is correct, even though it does not match the posted solution.\nThere is no explanation for my grade.\nI got a perfect score, but my solution has a mistake (you will receive extra credit for this! see below!)\nThere is a major error in the posted solution; here is an explanation (full credit for everyone, but Prof. Srikrishnan will decide what constitutes a “major error”! see below!).\n\nWe will only regrade what you submitted. Any “new” information in the regrade request will not be considered beyond the justification. An explanation may help us identify where to look if your solution is different than the official solution but correct, but the submitted answer must be interpretable as a correct solution on its own merits.\nThe first regrade request will be handled by the member of the course staff who graded the problem. If Prof. Srikrishnan did not review the first request, he will handle any subsequent regrade requests for the same submission. Once Prof. Srikrishnan issues a response to a regrade request, further requests for that submission will be ignored.\nIf you submit a regrade request correctly reporting that your problem was graded too leniently, your score will be increased by the difference (we want to reward this type of honesty and self-awareness!). For example, if your score was 8/10, and you point out that you should have gotten a 3/10, your score will be a 13/10.\nIf a significant error is found in the official solutions, everyone in the class will receive full credit for the relevant (sub)problem. Prof. Srikrishnan has discretion as to what errors are “significant.”",
    "crumbs": [
      "Grading",
      "Grading Policies"
    ]
  },
  {
    "objectID": "policies/index.html",
    "href": "policies/index.html",
    "title": "Course Policies",
    "section": "",
    "text": "This section contains an overview of course policies, at a higher level of detail than what is in the syllabus.",
    "crumbs": [
      "Policies"
    ]
  },
  {
    "objectID": "policies/groups.html",
    "href": "policies/groups.html",
    "title": "Group Expectations",
    "section": "",
    "text": "We expect that members of groups will contribute to the group effort as equal participants. This means:\n\nSetting and fulfilling expectations that are fairly agreed upon by the group;\nContributing to discussions and the generation of ideas (note: these contributions don’t have to be good, necessarily; brainstorming will necessarily involve throwing out half-baked ideas, but this is still a contribution);\nContributing to the analysis and writing efforts (at minimum, reading, understanding, and agreeing to everything submitted);\nCommunicating actively and proactively about statuses and challenges.\n\nEvery member of a group will be evaluated on these bases at the end of the semester by their groupmates, which could influence their grade.",
    "crumbs": [
      "Expectations",
      "Group Expectations"
    ]
  },
  {
    "objectID": "policies/groups.html#expectations",
    "href": "policies/groups.html#expectations",
    "title": "Group Expectations",
    "section": "",
    "text": "We expect that members of groups will contribute to the group effort as equal participants. This means:\n\nSetting and fulfilling expectations that are fairly agreed upon by the group;\nContributing to discussions and the generation of ideas (note: these contributions don’t have to be good, necessarily; brainstorming will necessarily involve throwing out half-baked ideas, but this is still a contribution);\nContributing to the analysis and writing efforts (at minimum, reading, understanding, and agreeing to everything submitted);\nCommunicating actively and proactively about statuses and challenges.\n\nEvery member of a group will be evaluated on these bases at the end of the semester by their groupmates, which could influence their grade.",
    "crumbs": [
      "Expectations",
      "Group Expectations"
    ]
  },
  {
    "objectID": "policies/groups.html#recommended-practices",
    "href": "policies/groups.html#recommended-practices",
    "title": "Group Expectations",
    "section": "Recommended Practices",
    "text": "Recommended Practices\nTo facilitate these contributions, we recommend the following practices (though specifics may vary depending on the group):\n\nClearly set expectations and timelines for each group members’ contributions (for example, create a schedule or a Gantt chart), including backup plans if unexpected challenges or circumstances arise during the semester;\nUse collaborative writing and coding/code-sharing platforms (e.g. Google Docs, Overleaf, and GitHub) to update work frequently and solicit feedback from group members;\nSchedule regular meetings for updates on status and progress;\nSeek advice from the course staff proactively if challenges emerge that might compromise your plan or timeline.",
    "crumbs": [
      "Expectations",
      "Group Expectations"
    ]
  },
  {
    "objectID": "policies/rubric.html",
    "href": "policies/rubric.html",
    "title": "Standard Rubrics",
    "section": "",
    "text": "These rubrics are intended as a description for students and a guide to the grader for how to assign partial credit.",
    "crumbs": [
      "Grading",
      "Standard Rubrics"
    ]
  },
  {
    "objectID": "policies/rubric.html#meta-rubric",
    "href": "policies/rubric.html#meta-rubric",
    "title": "Standard Rubrics",
    "section": "Meta-Rubric",
    "text": "Meta-Rubric\n\nIndividual problems may vary from these standard rubrics based on the assessed learning outcomes for the problem, but they should provide an overview of what features students ought to be included in a given solution.\nEach bullet point should appear as a separate rubric item in Gradescope. We will use positive grading (points awarded for each component).\nEach standard rubric describes partial credit for a problem with 10 points. Partial credit for (sub)problems worth less than 10 points should be scaled appropriately. Full problems usually combine one of the modeling methods with some interpretation questions, so the rubric will be a combination of the individual components.\nGenerally, rubrics for 10 point “entire problems” can be summarized with the following:\n\n+4 points for an answer with a correct implementation (including model setup).\n+2 points for the correct solution (including details such as labels, units, etc.).\n+4 points for the interpretation. These points may be broken up across subproblems, but the general assignment of points should follow this summary.\n\nNote that the points may not scale with the distribution of work for a given problem. That’s life! It may take more work to derive and implement your model than it does to intrepret your results, but in this class we care a lot about your ability to critically evaluate and interpret modeling results.\nSometimes problems will be broken up into\nThe graders cannot read your mind and will not try to. Submissions that are unclear for any reason, including but not limited to unclear syntax (English or code), uncommented code, lack of reasoning or derivation, too much detail or writing, will not be given credit. The grader has complete discretion here. If something in your solution is ambiguous, the grader has been instructed to interpret it the “wrong” way. Clear responses are a sign of understanding, which is part of what we’re assessing.\nYou will not be doubly penalized for getting the “right” solution to the “wrong” setup (you would have been penalized above), but this requires the grader to be able to easily identify that your implementation is correct given the wrong model. If your implementation is unclear, you are likely to lose points here because we have no way of knowing that your answer is “right” without re-coding your problem.\nYou will not be given credit for your code (this isn’t a programming class); the code is a means to solving the problem, and we’re more interested in how you set up the problem and interpret the solution. This does mean that code that is sloppy but works is perfectly acceptable. However, your code may get you partial credit if the TA can easily find where you made a mistake, so make sure your submitted code is well commented. It’s important to write down the mathematics of what your code is trying to implement, because the TA will only take a cursory look at your code.\nSome rubrics include “deadly sins” which will cause an immediate zero to be given for the problem. Do not do these!\nRegardless of the problem type, the following penalties will be applied per incident:\n\n-1 for missing units.",
    "crumbs": [
      "Grading",
      "Standard Rubrics"
    ]
  },
  {
    "objectID": "policies/rubric.html#standard-rubrics",
    "href": "policies/rubric.html#standard-rubrics",
    "title": "Standard Rubrics",
    "section": "Standard Rubrics",
    "text": "Standard Rubrics\n\nSimulation Rubric\n10 points =\n\n+4 for the model derivation.\n\nThis includes any relevant reasoning from mass-balance or other principles.\n\n+4 for discretizing the model correctly.\n\n-1 if no justification is provided for the chosen step-size(s).\n\n+2 points for obtaining the correct solution.\n\n-1 if the error is from a minor bug. I recommend providing an explicit sketch of your code’s procedure, so if that is correct any differences are likely to be the result of a minor bug, such as a misentered number (not ideal, but you understand what you’re doing!). Otherwise you are relying on the TA to interpret your code.\n\n\n\n\nMonte Carlo Rubric\n10 points =\n\n+4 for a clear English explanation of the sampling plan\n\nYou must justify each distribution used, including your choice of mean and standard deviation.\n\n+2 for justification of Monte Carlo sample size.\n\nDeadly Sin: No points will be given for a Monte Carlo solution with an arbitrary sample size.\n\n+2 for an estimate of the Monte Carlo standard error.\n+1 for setting a seed for reproducibility.\n+2 for correct estimate.\n\nDeviations from the posted solution are ok if they’re within the Monte Carlo standard error.\n\n\n\n\nFigure Rubrics\n10 points =\n\n+2 for appropriate choice of axes.\n\nDeadly Sin!: No points will be given for the figure if the axes are not labelled.\n\n+4 for the correct data series.\n+2 for a descriptive legend.\n+2 for a succinct description or caption.\n\n-1 for a description which is too wordy but contains the relevant information.\n\n\n\n\nInterpretation Rubrics\n10 points =\n\n+4 for specific reference to the modeling results;\n\n-2 if the results are not specifically referenced and the reader has to refer to the previous problems to understand the interpretation.\n\n+6 for thoughtfulness of the interpretation;\n\n-4 for not specifically referencing relevant model assumptions;\nDeadly Sin: Do not neglect fundamental engineering principles in any of your interpretations. You will be given a zero if you make a recommendation which e.g. violates an engineering code or standard.\nDeadly Sin: Completely generic interpretations will result in zero points.",
    "crumbs": [
      "Grading",
      "Standard Rubrics"
    ]
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#planetary-energy-balance",
    "href": "slides/lecture04-2-correlatedresiduals.html#planetary-energy-balance",
    "title": "Correlated Discrepancies",
    "section": "Planetary Energy Balance",
    "text": "Planetary Energy Balance\n\nRepresentation of Planetary Energy Balance\nSource: Reprinted from A Climate Modeling Primer, A. Henderson-Sellers and K. McGuffie, Wiley, pg. 58, (1987) via https://www.e-education.psu.edu/meteo469/node/137."
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#the-energy-balance-model-ebm",
    "href": "slides/lecture04-2-correlatedresiduals.html#the-energy-balance-model-ebm",
    "title": "Correlated Discrepancies",
    "section": "The Energy Balance Model (EBM)",
    "text": "The Energy Balance Model (EBM)\n\n\n\\[\\begin{align*}\n\\overbrace{\\frac{dH}{dt}}^{\\text{change in heat}} &= \\overbrace{F}^{\\text{RF}} - \\overbrace{\\lambda T}^{\\substack{\\text{change in} \\\\ \\text{temperature}}} \\\\[1em]\n\\Rightarrow C\\frac{dT}{dt} &= F - \\lambda T - \\gamma(T-T_D)\\\\\nC_D\\frac{dT_D}{dt} &= \\gamma(T-T_D)\n\\end{align*}\\]\n\n\n\n\nTwo Layer EBM Schematic\n\n\n\nSource: Palmer et al. (2018)"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#ebm-discretization",
    "href": "slides/lecture04-2-correlatedresiduals.html#ebm-discretization",
    "title": "Correlated Discrepancies",
    "section": "EBM Discretization",
    "text": "EBM Discretization\nUse Euler discretization:\n\\[\\begin{align*}\nT(t+1) &= T(t) + \\frac{F(t) - \\lambda T(t) - \\gamma(T(t) - T_D(d))}{C} \\Delta t \\\\[0.5em]\nT_D(t+1) &= T_D(t) + \\frac{\\gamma (T(t) - T_D(t))}{C_D} \\Delta t\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#equilibrium-climate-sensitivity-ecs",
    "href": "slides/lecture04-2-correlatedresiduals.html#equilibrium-climate-sensitivity-ecs",
    "title": "Correlated Discrepancies",
    "section": "Equilibrium Climate Sensitivity (ECS)",
    "text": "Equilibrium Climate Sensitivity (ECS)\nUnder steady-state conditions (constant \\(F\\) and \\(dT/dt = 0\\)), \\[T = \\frac{F}{\\lambda}.\\]\nWhen we double atmospheric CO2, we refer to the equilibrium temperature \\(S\\) as the equilibrium climate sensitivity:\n\\[S = \\underbrace{F_{2\\times \\text{CO}_2}}_{\\approx 4 \\text{W/m}^2}/\\lambda\\]"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#probability-models-for-simulation-models",
    "href": "slides/lecture04-2-correlatedresiduals.html#probability-models-for-simulation-models",
    "title": "Correlated Discrepancies",
    "section": "Probability Models for Simulation Models",
    "text": "Probability Models for Simulation Models\nMost common setting (e.g. Brynjarsdóttir & O’Hagan (2014)):\n\\[\\mathbf{y} = \\underbrace{\\eta(\\mathbf{x}; \\theta)}_{\\text{model}} + \\underbrace{\\delta(x)}_{\\text{discrepancy}} + \\underbrace{\\varepsilon}_{\\text{error}}\\]"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#model-data-discrepancy",
    "href": "slides/lecture04-2-correlatedresiduals.html#model-data-discrepancy",
    "title": "Correlated Discrepancies",
    "section": "Model-Data Discrepancy",
    "text": "Model-Data Discrepancy\nFor example, \\(\\delta\\) might capture:\n\nBias (e.g.: model consistently over/underpredicts);\nAccumulations of error (e.g.: persistent model underestimates);\nPartial observations (e.g.: do you count every animal?)"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#mle-for-gaussian-discrepancy",
    "href": "slides/lecture04-2-correlatedresiduals.html#mle-for-gaussian-discrepancy",
    "title": "Correlated Discrepancies",
    "section": "MLE for Gaussian Discrepancy",
    "text": "MLE for Gaussian Discrepancy\n\n\n\n\n\nParameters\nMLE\n\n\n\n\nS\n3.4\n\n\nγ\n1.5\n\n\nα\n1.0\n\n\nd\n77.8\n\n\nD\n623.7\n\n\nT₀\n-0.1\n\n\nσ\n0.1\n\n\n\n\n\n\nCode\nn_samples = 10_000\ntemp_iid = ebm_wrap(θ_iid) # simulate IID best fit\n# simulate projections with discrepancy and errors\ntemp_iid_proj = zeros(n_samples, length(temp_sd))\nfor i = 1:length(temp_sd)\n    temp_iid_err = rand(Normal(0, sqrt.(θ_iid[end]^2 .+ temp_sd[i]^2)), n_samples)\n    temp_iid_proj[:, i] = temp_iid[i] .+ temp_iid_err\nend\n# calculate quantiles\ntemp_iid_q = mapslices(col -&gt; quantile(col, [0.05, 0.5, 0.95]), temp_iid_proj; dims=1)\n\np = scatter(time_obs, temp_obs, color=:black, label=\"Observations\", ylabel=\"(°C)\", xlabel=\"Year\", title=\"Temperature Anomaly\")\nplot!(p, time_obs, temp_iid_q[2, :], ribbon=(temp_iid_q[2, :] - temp_iid_q[1, :], temp_iid_q[3, :] - temp_iid_q[2, :]), color=:red, fillalpha=0.3, label=\"Gaussian Discrepancy\")\nplot!(size=(600, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n1850\n\n\n1900\n\n\n1950\n\n\n2000\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−0.5\n\n\n0.0\n\n\n0.5\n\n\n1.0\n\n\n1.5\n\n\n(°C)\n\n\nTemperature Anomaly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nGaussian Discrepancy\n\n\n\n\nFigure 1: MLE Fit for Gaussian discrepancy"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#analyzing-residual-assumptions",
    "href": "slides/lecture04-2-correlatedresiduals.html#analyzing-residual-assumptions",
    "title": "Correlated Discrepancies",
    "section": "Analyzing Residual Assumptions",
    "text": "Analyzing Residual Assumptions\n\n\nCode\nresids_homogauss = temp_obs - temp_iid\n\np1 = qqnorm(resids_homogauss, xlabel=\"Theoretical Values\", ylabel=\"Empirical Values\", title=\"Normal Q-Q Plot\", size=(600, 500))\npacf_homogauss = pacf(resids_homogauss, 1:5)\np2 = plot(1:5, pacf_homogauss, marker=:circle, line=:stem, linewidth=3, markersize=8, tickfontsize=16, guidefontsize=18, legend=false, ylabel=\"Partial Autocorrelation\", xlabel=\"Time Lag\", title=\"Partial Autocorrelation Plot\", size=(600, 500))\nhline!(p2, [0], color=:black, linestyle=:dash)\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n−0.4\n\n\n−0.2\n\n\n0.0\n\n\n0.2\n\n\nTheoretical Values\n\n\n\n\n\n\n\n\n\n\n\n\n−0.4\n\n\n−0.2\n\n\n0.0\n\n\n0.2\n\n\nEmpirical Values\n\n\nNormal Q-Q Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Residual diagnostics\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\nTime Lag\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.0\n\n\n0.1\n\n\n0.2\n\n\n0.3\n\n\n0.4\n\n\n0.5\n\n\nPartial Autocorrelation\n\n\nPartial Autocorrelation Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#ar1-residuals",
    "href": "slides/lecture04-2-correlatedresiduals.html#ar1-residuals",
    "title": "Correlated Discrepancies",
    "section": "AR(1) Residuals",
    "text": "AR(1) Residuals\n\n\nAutocorrelated\n\\[\n\\begin{align*}\ny_t &= \\text{EBM}(\\theta; F_t) + \\delta_t + \\varepsilon_t \\\\\n\\delta_t &= \\rho \\delta_{t-1} + \\omega_t \\\\\n\\varepsilon_t &\\sim N(0, \\sigma^2_{\\text{obs}, t}) \\\\\n\\omega_t &\\sim N(0, \\sigma^2)\n\\end{align*}\n\\]\n\nIndependent\n\\[\n\\begin{align*}\ny_t &= \\text{EBM}(\\theta; F_t) + \\delta_t + \\varepsilon_t \\\\\n\\delta_t &\\sim N(0, \\sigma^2) \\\\\n\\varepsilon_t &\\sim N(0, \\sigma^2_{\\text{obs}, t})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#likelihood-function",
    "href": "slides/lecture04-2-correlatedresiduals.html#likelihood-function",
    "title": "Correlated Discrepancies",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nWithout observation errors (\\(\\varepsilon\\)), can whiten residuals:\n\\[\n\\begin{align*}\ny_1 & \\sim N\\left(0, \\frac{\\sigma^2}{1 - \\rho^2}\\right) \\\\\ny_t - \\rho y_{t-1}  &\\sim N(0, \\sigma^2)\n\\end{align*}\n\\]\nWhen observation errors are given in the data (as in here), this also works."
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#non-identifiability-of-parameters",
    "href": "slides/lecture04-2-correlatedresiduals.html#non-identifiability-of-parameters",
    "title": "Correlated Discrepancies",
    "section": "Non-Identifiability of Parameters",
    "text": "Non-Identifiability of Parameters\nBut when observation errors are also uncertain, run into non-identifiability:\n\\[N(0, \\sigma^2) + N(0, \\sigma_\\text{obs}^2) = N(0, {\\color{red}\\sigma^2 + \\sigma_\\text{obs}^2})\\]"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#joint-likelihood-for-ar1",
    "href": "slides/lecture04-2-correlatedresiduals.html#joint-likelihood-for-ar1",
    "title": "Correlated Discrepancies",
    "section": "Joint Likelihood for AR(1)",
    "text": "Joint Likelihood for AR(1)\nCould also use joint likelihood for residuals \\(y_t - \\text{EBM}(\\theta; F_t)\\):\n\\[\n\\begin{align*}\n\\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{0}, \\Sigma) \\\\\n\\Sigma &= \\frac{\\sigma^2}{1 - \\rho^2} \\begin{pmatrix}1 + \\sigma_{\\text{obs}, 1}^2 & \\rho & \\ldots & \\rho^{T-1}  \\\\ \\rho & 1 + \\sigma_{\\text{obs}, 2}^2 & \\ldots & \\rho^{T-2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\rho^{T-1} & \\rho^{T-2} & \\ldots & 1+ \\sigma_{\\text{obs}, T}^2\\end{pmatrix}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#whitened-likelihood-for-ar1-in-code",
    "href": "slides/lecture04-2-correlatedresiduals.html#whitened-likelihood-for-ar1-in-code",
    "title": "Correlated Discrepancies",
    "section": "Whitened Likelihood for AR(1) (in code)",
    "text": "Whitened Likelihood for AR(1) (in code)\n\n\nCode\n# temp_obs: temperature data\n# temp_err: standard deviations for temperatures\n# m: model function\nfunction ar_loglik(params, temp_obs, temp_err, m)\n    S, γ, α, d, D, T₀, ρ, σ = params \n    ebm_sim = m((S, γ, α, d, D, T₀))\n    residuals = temp_obs - ebm_sim\n    # whiten residuals\n    ll = 0\n    # notice addition of observation errors\n    for t = 1:length(temp_sd)\n        if t == 1\n            ll += logpdf(Normal(0, sqrt(σ^2 / (1 - ρ^2) + temp_err[1]^2)), residuals[1])\n        else\n            resid_wn = residuals[t] - ρ * residuals[t-1]\n            ll += logpdf(Normal(0, sqrt(σ^2 + temp_err[t]^2)), resid_wn)\n        end\n    end\n    return ll\nend"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#ar1-mle-code",
    "href": "slides/lecture04-2-correlatedresiduals.html#ar1-mle-code",
    "title": "Correlated Discrepancies",
    "section": "AR(1) MLE (Code)",
    "text": "AR(1) MLE (Code)\n\n\nCode\nlower = [1.0, 0.5, 0.0, 50.0, 200.0, temp_lo[1], -1.0, 0.0]\nupper = [5.0, 1.5, 2.0, 200.0, 1000.0, temp_hi[1], 1.0, 10.0]\np0 = [3.0, 1.0, 1.0, 100.0, 800.0,temp_obs[1], 0.0, 5.0]\n\nresult = Optim.optimize(params -&gt; -ar_loglik(params, temp_obs, temp_sd, ebm_wrap), lower, upper, p0)\nθ_ar = result.minimizer"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#comparison-of-mle",
    "href": "slides/lecture04-2-correlatedresiduals.html#comparison-of-mle",
    "title": "Correlated Discrepancies",
    "section": "Comparison of MLE",
    "text": "Comparison of MLE\n\n\n\nParameters\nIID\nAR\n\n\n\n\nS\n3.4\n3.3\n\n\nγ\n1.5\n1.5\n\n\nα\n1.0\n0.9\n\n\nd\n77.8\n88.6\n\n\nD\n623.7\n649.1\n\n\nT₀\n-0.1\n-0.1\n\n\nρ\n-\n0.5\n\n\nσ\n0.1\n0.1"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#ar1-hindcast",
    "href": "slides/lecture04-2-correlatedresiduals.html#ar1-hindcast",
    "title": "Correlated Discrepancies",
    "section": "AR(1) Hindcast",
    "text": "AR(1) Hindcast\n\n\n\n\nCode\nn = 10_000\n\n# get model hindcasts\ntemp_iid = ebm_wrap(θ_iid[1:end-1])\ntemp_ar = ebm_wrap(θ_ar[1:end-2])\n\n# get iid and AR residuals from relevant processes\nresiduals_iid = stack(rand.(Normal.(0, sqrt.(temp_sd.^2 .+ θ_iid[end].^2)), n), dims=1)\nresiduals_ar = zeros(length(hind_idx), n)\nfor t = 1:length(temp_sd)\n    if t == 1\n        residuals_ar[t, :] = rand(Normal(0, sqrt(θ_ar[end]^2 / (1 - θ_ar[end-1]^2) + temp_sd[1]^2)), n)\n    else\n        residuals_ar[t, :] = θ_ar[end-1] * residuals_ar[t-1, :] + rand(Normal(0, sqrt(θ_ar[end]^2 + temp_sd[t]^2)), n)\n    end\nend\n\n# add residuals back to model simulations\nmodel_sim_iid = (residuals_iid .+ temp_iid)'\nmodel_sim_ar = (residuals_ar .+ temp_ar)'\n\n# get quantiles\nq90_iid = mapslices(col -&gt; quantile(col, [0.05, 0.5, 0.95]), model_sim_iid; dims=1) # compute 90% prediction interval\nq90_ar = mapslices(col -&gt; quantile(col, [0.05, 0.5, 0.95]), model_sim_ar; dims=1) # compute 90% prediction interval\n\np = scatter(time_obs, temp_obs, yerr=(temp_obs - temp_lo, temp_hi - temp_obs), color=:black, label=\"Observations\", ylabel=\"(°C)\", xlabel=\"Year\", title=\"Temperature Anomaly\", markersize=5)\nplot!(p, hind_years, q90_iid[2, :], ribbon=(q90_iid[2, :] - q90_iid[1, :], q90_iid[3, :] - q90_iid[2, :]), fillalpha=0.2, linewidth=3, label=\"IID\")\nplot!(p, hind_years, q90_ar[2, :], ribbon=(q90_ar[2, :] - q90_ar[1, :], q90_ar[3, :] - q90_ar[2, :]), fillalpha=0.2, linewidth=3, label=\"AR\")\nplot!(size=(700, 550))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n1850\n\n\n1900\n\n\n1950\n\n\n2000\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−0.5\n\n\n0.0\n\n\n0.5\n\n\n1.0\n\n\n1.5\n\n\n(°C)\n\n\nTemperature Anomaly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIID\n\n\n\nAR\n\n\n\n\nFigure 3: Hindcast of the EBM with IID and AR(1) residuals.\n\n\n\n\n\nCoverage Rates:\n\nIID: 90.2%\nAR(1): 93.1%"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#have-we-captured-residual-autocorrelation",
    "href": "slides/lecture04-2-correlatedresiduals.html#have-we-captured-residual-autocorrelation",
    "title": "Correlated Discrepancies",
    "section": "Have We Captured Residual Autocorrelation?",
    "text": "Have We Captured Residual Autocorrelation?\n\n\nUse simulations to look at distributions of residual probability assumptions.\n\n\n\nCode\nresids_ar_sim = model_sim_ar .- temp_obs'\nboxplot(mapslices(col -&gt; pacf(col, 1:5), resids_ar_sim; dims=1)', label=:false, xlabel=\"Lag\", ylabel=\"Partial Autocorrelation\")\nplot!(size=(500, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\nLag\n\n\n\n\n\n\n\n\n\n\n\n\n−0.02\n\n\n0.00\n\n\n0.02\n\n\n0.04\n\n\nPartial Autocorrelation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Distribution of residual partial autocorrelations"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#hindcasts-may-not-differ-much",
    "href": "slides/lecture04-2-correlatedresiduals.html#hindcasts-may-not-differ-much",
    "title": "Correlated Discrepancies",
    "section": "Hindcasts May Not Differ Much…",
    "text": "Hindcasts May Not Differ Much…\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n1850\n\n\n1900\n\n\n1950\n\n\n2000\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−0.5\n\n\n0.0\n\n\n0.5\n\n\n1.0\n\n\n1.5\n\n\n(°C)\n\n\nTemperature Anomaly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\nIID\n\n\n\nAR\n\n\n\n\nFigure 5: Hindcast of the EBM with IID and AR(1) residuals."
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#but-projections-can",
    "href": "slides/lecture04-2-correlatedresiduals.html#but-projections-can",
    "title": "Correlated Discrepancies",
    "section": "…But Projections Can",
    "text": "…But Projections Can\n\n\nCode\nebm_sim_85(params) = ebm(forcing_non_aerosol_85[sim_idx], forcing_aerosol_85[sim_idx], p = params)\nebm_sim_26(params) = ebm(forcing_non_aerosol_26[sim_idx], forcing_aerosol_26[sim_idx], p = params)\n\n# iid residuals\ny_err = zeros(length(sim_idx))\ny_err[1:length(hind_idx)] = temp_sd\n\nresiduals_iid = stack(rand.(Normal.(0, sqrt.(y_err.^2 .+ θ_iid[end]^2)), n), dims=1)\nmodel_iid_85 = ebm_sim_85(θ_iid[1:end-1])\nmodel_iid_26 = ebm_sim_26(θ_iid[1:end-1])\nmodel_sim_iid_85 = (residuals_iid .+ model_iid_85)'\nmodel_sim_iid_26 = (residuals_iid .+ model_iid_26)'\nq90_iid_85 = mapslices(col -&gt; quantile(col, [0.05, 0.5, 0.95]), model_sim_iid_85; dims=1) # compute 90% prediction interval```\nq90_iid_26 = mapslices(col -&gt; quantile(col, [0.05, 0.5, 0.95]), model_sim_iid_26; dims=1) # compute 90% prediction interval```\n\n# AR residuals\nresiduals_ar = zeros(length(sim_idx), n)\nfor t = 1:length(sim_idx)\n    if t == 1\n        residuals_ar[t, :] = rand(Normal(0, sqrt(θ_ar[end]^2 / (1 - θ_ar[end-1]^2 + temp_sd[1]^2))), n)\n    elseif t &lt;= length(hind_idx)\n        residuals_ar[t, :] = θ_ar[end-1] * residuals_ar[t-1, :] + rand(Normal(0, sqrt(θ_ar[end]^2 + temp_sd[t]^2)), n)\n    else\n        residuals_ar[t, :] = θ_ar[end-1] * residuals_ar[t-1, :] + rand(Normal(0, θ_ar[end]), n)\n    end\nend\nmodel_ar_85 = ebm_sim_85(θ_ar[1:end-2])\nmodel_ar_26 = ebm_sim_26(θ_ar[1:end-2])\nmodel_sim_ar_26 = (residuals_ar .+ model_ar_26)'\nmodel_sim_ar_85 = (residuals_ar .+ model_ar_85)'\nq90_ar_26 = mapslices(col -&gt; quantile(col, [0.05, 0.5, 0.95]), model_sim_ar_26; dims=1) # compute 90% prediction interval\nq90_ar_85 = mapslices(col -&gt; quantile(col, [0.05, 0.5, 0.95]), model_sim_ar_85; dims=1) # compute 90% prediction interval\n\np_sim= scatter(time_obs, temp_obs, color=:black, label=\"Data\", ylabel=\"Temperature Anomaly (°C)\", xlabel=\"Year\", right_margin=5mm)\nplot!(p_sim, sim_years, q90_iid_26[2, :], ribbon=(q90_iid_26[2, :] - q90_iid_26[1, :], q90_iid_26[3, :] - q90_iid_26[2, :]), fillalpha=0.2, linewidth=3, color=:royalblue, label=\"IID/SSP1-2.6\")\nplot!(p_sim, sim_years, q90_ar_26[2, :], ribbon=(q90_ar_26[2, :] - q90_ar_26[1, :], q90_ar_26[3, :] - q90_ar_26[2, :]), fillalpha=0.2, linewidth=3, color=:firebrick1, label=\"AR/SSP1-2.6\")\nplot!(p_sim, sim_years, q90_iid_85[2, :], ribbon=(q90_iid_85[2, :] - q90_iid_85[1, :], q90_iid_85[3, :] - q90_iid_85[2, :]), fillalpha=0.2, linewidth=3, color=:blue3, label=\"IID/SSP5-8.5\")\nplot!(p_sim, sim_years, q90_ar_85[2, :], ribbon=(q90_ar_85[2, :] - q90_ar_85[1, :], q90_ar_85[3, :] - q90_ar_85[2, :]), fillalpha=0.2, linewidth=3, color=:firebrick, label=\"AR/SSP5-8.5\")\nplot!(p_sim, size=(1100, 550))\nxlims!((2000, 2100))\nylims!((0.75, 5.5))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2000\n\n\n2025\n\n\n2050\n\n\n2075\n\n\n2100\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\nTemperature Anomaly (°C)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData\n\n\n\nIID/SSP1-2.6\n\n\n\nAR/SSP1-2.6\n\n\n\nIID/SSP5-8.5\n\n\n\nAR/SSP5-8.5\n\n\n\n\nFigure 6: Differences in the 90% confidence intervals"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#these-differences-could-be-decision-relevant",
    "href": "slides/lecture04-2-correlatedresiduals.html#these-differences-could-be-decision-relevant",
    "title": "Correlated Discrepancies",
    "section": "These Differences Could Be Decision-Relevant",
    "text": "These Differences Could Be Decision-Relevant\n\nCode\np1 = histogram(model_sim_iid_26[:, end], color=:blue, xlabel=\"°C\", ylabel=\"Count\", label=\"IID\", size=(600, 450), alpha=0.4, title=\"SSP1-2.6\")\nhistogram!(p1, model_sim_ar_26[:, end], color=:red, label=\"AR\", alpha=0.4)\np2 = histogram(model_sim_iid_85[:, end], color=:blue, xlabel=\"°C\", ylabel=\"Count\", label=\"IID\", size=(600, 450), alpha=0.4, title=\"SSP5-8.5\")\nhistogram!(p2, model_sim_ar_85[:, end], color=:red, label=\"AR\", alpha=0.4)\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.6\n\n\n1.8\n\n\n2.0\n\n\n2.2\n\n\n2.4\n\n\n°C\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n200\n\n\n400\n\n\n600\n\n\n800\n\n\nCount\n\n\nSSP1-2.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIID\n\n\n\n\nAR\n\n\n\n\n(a) Projections of global mean temperature in 2100\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.6\n\n\n4.8\n\n\n5.0\n\n\n5.2\n\n\n5.4\n\n\n°C\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n300\n\n\n600\n\n\n900\n\n\nCount\n\n\nSSP5-8.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIID\n\n\n\n\nAR\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 7"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#lynx-and-hare-pelts",
    "href": "slides/lecture04-2-correlatedresiduals.html#lynx-and-hare-pelts",
    "title": "Correlated Discrepancies",
    "section": "Lynx and Hare Pelts",
    "text": "Lynx and Hare Pelts\n\n\nCode\nlh_obs = DataFrame(CSV.File(\"data/ecology/Lynx_Hare.txt\", header=[:Year, :Hare, :Lynx]))[:, 1:3]\nplot(lh_obs[!, :Year], lh_obs[!, :Lynx], xlabel=\"Year\", ylabel=\"Pelts (thousands)\", markersize=5, markershape=:circle, markercolor=:red, color=:red, linewidth=3, label=\"Lynx\")\nplot!(lh_obs[!, :Year], lh_obs[!, :Hare], markersize=5, markershape=:circle, markercolor=:blue, color=:blue, linewidth=3, label=\"Hare\")\nplot!(size=(1100, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n1860\n\n\n1880\n\n\n1900\n\n\n1920\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n50\n\n\n100\n\n\n150\n\n\nPelts (thousands)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLynx\n\n\n\n\nHare\n\n\n\n\nFigure 8: Lynx and Hare pelt dataset"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#predator-prey-dynamics",
    "href": "slides/lecture04-2-correlatedresiduals.html#predator-prey-dynamics",
    "title": "Correlated Discrepancies",
    "section": "Predator-Prey Dynamics",
    "text": "Predator-Prey Dynamics\n\\[\n\\begin{align*}\n\\frac{dH}{dt} &= H_t \\underbrace{b_H}_{\\substack{\\text{birth} \\\\ \\text{rate}}} - H_t (\\underbrace{L_t m_H}_{\\substack{\\text{impact of} \\\\ \\text{lynxes}}}) \\\\\n\\frac{dL}{dt} &= L_t (H_t b_L) - L_t m_L\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#can-the-model-replicate-the-patterns",
    "href": "slides/lecture04-2-correlatedresiduals.html#can-the-model-replicate-the-patterns",
    "title": "Correlated Discrepancies",
    "section": "Can The Model Replicate The Patterns?",
    "text": "Can The Model Replicate The Patterns?\n\n\nCode\n# specifiyng the diffeq problem using DifferentialEquations.jl\nfunction lynx_hare!(dP, P, θ, t)\n    H, L = P\n    bh, mh, bl, ml = θ\n    dP[1] = (bh - L * mh) * H\n    dP[2] = (bl * H - ml) * L\nend\n\n# run a simulation based on the lynx_hare! solution\nfunction lh_sim(params, N)\n    H₁, L₁ = params[end-1:end]\n    prob = ODEProblem(lynx_hare!, [H₁, L₁], N-1, params[1:end-2])\n    sol = solve(prob, saveat=1)\n    H = map(first, sol.u[1:N])\n    L = map(last, sol.u[1:N])\n    return (H, L)\nend\n\nparams = (0.54, 0.005, 0.005, 0.8, 190, 30)\nH, L = lh_sim(params, nrow(lh_obs))\nplot(lh_obs[!, :Year], H, label=\"Hare\", linewidth=3, xlabel=\"Year\", ylabel=\"Population (thousands)\", title=\"Simulated Population Dynamics\", color=:blue)\nplot!(lh_obs[!, :Year], L, label=\"Lynx\", linewidth=3, color=:red)\nplot!(size=(1100, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n1860\n\n\n1880\n\n\n1900\n\n\n1920\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n50\n\n\n100\n\n\n150\n\n\n200\n\n\n250\n\n\n300\n\n\n350\n\n\nPopulation (thousands)\n\n\nSimulated Population Dynamics\n\n\n\n\n\n\n\nHare\n\n\n\nLynx\n\n\n\n\nFigure 9: Synthetic data from predator-prey model"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#what-is-a-generative-process-for-pelts",
    "href": "slides/lecture04-2-correlatedresiduals.html#what-is-a-generative-process-for-pelts",
    "title": "Correlated Discrepancies",
    "section": "What Is A Generative Process For Pelts?",
    "text": "What Is A Generative Process For Pelts?\n\n\nInitial population changes according to predator-prey model;\nSome fraction of population are trapped;\nTrap rates differ by species and can vary by year."
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#predator-prey-probability-model",
    "href": "slides/lecture04-2-correlatedresiduals.html#predator-prey-probability-model",
    "title": "Correlated Discrepancies",
    "section": "Predator-Prey Probability Model",
    "text": "Predator-Prey Probability Model\n\\[\\underbrace{h_t}_{\\substack{\\text{hare} \\\\ \\text{pelts}}} \\sim \\text{LogNormal}(\\log(\\underbrace{p_H}_{\\substack{\\text{trap} \\\\ \\text{rate}}} H_T), \\sigma_H)\\] \\[l_t \\sim \\text{LogNormal}(\\log(p_L L_T), \\sigma_L)\\]\n\n\n\\[\n\\begin{align*}\n\\frac{dH}{dt} &= H_t b_H - H_t (L_t m_H) \\\\\nH_T &= H_1 + \\int_1^T \\frac{dH}{dt}dt\n\\end{align*}\n\\]\n\n\\[\n\\begin{align*}\n\\frac{dL}{dt} &= L_t (H_t b_L) - L_t m_L \\\\\nL_T &= L_1 + \\int_1^T \\frac{dL}{dt}dt\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#key-points-1",
    "href": "slides/lecture04-2-correlatedresiduals.html#key-points-1",
    "title": "Correlated Discrepancies",
    "section": "Key Points",
    "text": "Key Points\n\nThink generatively about probability models for calibrating models:\nDiscrepancy: corrects for mismatches between model output and “state” of system\nObservation errors: Probability distribution for observations given discrepancy adjustment\nChoice of probability model (including discrepancy) can impact projections even if hindcast (“validation”) does not appear very different."
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#questions-to-seed-discussion",
    "href": "slides/lecture04-2-correlatedresiduals.html#questions-to-seed-discussion",
    "title": "Correlated Discrepancies",
    "section": "Questions To Seed Discussion",
    "text": "Questions To Seed Discussion\n\nWhat do you think are the differences between predictive and explanatory modeling?\nWhat can go wrong when we conflate the two?\nCan you think of approaches or workflows which bridge the two paradigms?"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#next-classes",
    "href": "slides/lecture04-2-correlatedresiduals.html#next-classes",
    "title": "Correlated Discrepancies",
    "section": "Next Classes",
    "text": "Next Classes\nMonday: Feb Break!\nWednesday: Bayesian Statistics"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#assessments",
    "href": "slides/lecture04-2-correlatedresiduals.html#assessments",
    "title": "Correlated Discrepancies",
    "section": "Assessments",
    "text": "Assessments\nHomework 2 available; due next Friday (2/21).\nNo quiz or reading this week!"
  },
  {
    "objectID": "slides/lecture04-2-correlatedresiduals.html#references-scroll-for-full-list",
    "href": "slides/lecture04-2-correlatedresiduals.html#references-scroll-for-full-list",
    "title": "Correlated Discrepancies",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nBrynjarsdóttir, J., & O’Hagan, A. (2014). Learning about physical parameters: the importance of model discrepancy. Inverse Problems, 30, 114007. https://doi.org/10.1088/0266-5611/30/11/114007\n\n\nPalmer, M. D., Harris, G. R., & Gregory, J. M. (2018). Extending CMIP5 projections of global mean temperature change and sea level rise due to thermal expansion using a physically-based emulator. Environ. Res. Lett., 13, 084003. https://doi.org/10.1088/1748-9326/aad2e4"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#bias-vs.-variance",
    "href": "slides/lecture10-1-cross-validation.html#bias-vs.-variance",
    "title": "Scoring and Cross-Validation",
    "section": "Bias vs. Variance",
    "text": "Bias vs. Variance\n\nBias is error from mismatches between the model predictions and the data (\\(\\text{Bias}[\\hat{f}] = \\mathbb{E}[\\hat{f}] - y\\)).\nVariance is error from over-sensitivity to small fluctuations in training inputs \\(D\\) (\\(\\text{Variance} = \\text{Var}_D(\\hat{f}(x; D)\\)).\n\nCommonly discussed in terms of “bias-variance tradeoff”: more valuable to think of these as contributors to total error."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#overfitting-and-underfitting",
    "href": "slides/lecture10-1-cross-validation.html#overfitting-and-underfitting",
    "title": "Scoring and Cross-Validation",
    "section": "Overfitting and Underfitting",
    "text": "Overfitting and Underfitting\n\nUnderfitting: Model predicts individual data points poorly, high bias, think approximation error\nOverfitting: Model generalizes poorly, high variance, think estimation error."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#model-degrees-of-freedom",
    "href": "slides/lecture10-1-cross-validation.html#model-degrees-of-freedom",
    "title": "Scoring and Cross-Validation",
    "section": "Model Degrees of Freedom",
    "text": "Model Degrees of Freedom\nPotential to overfit vs. underfit isn’t directly related to standard metrics of “model complexity” (number of parameters, etc).\nInstead, think of degrees of freedom: how much flexibility is there given the model parameterization to “chase” reduced model error?"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#regularization",
    "href": "slides/lecture10-1-cross-validation.html#regularization",
    "title": "Scoring and Cross-Validation",
    "section": "Regularization",
    "text": "Regularization\n\n\nCan reduce degrees of freedom with regularization: tighter/more skeptical priors, shrinkage of estimates (e.g. LASSO) vs. “raw” MLE.\n\n\n\n\nRegularization Meme\n\n\n\nSource: Richard McElreath"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#what-makes-a-good-prediction",
    "href": "slides/lecture10-1-cross-validation.html#what-makes-a-good-prediction",
    "title": "Scoring and Cross-Validation",
    "section": "What Makes A Good Prediction?",
    "text": "What Makes A Good Prediction?\nWhat do we want to see in a probabilistic projection \\(F\\)?\n\n\nCalibration: Does the predicted CDF \\(F(y)\\) align with the “true” distribution of observations \\(y\\)? \\[\\mathbb{P}(y \\leq F^{-1}(\\tau)) = \\tau \\qquad \\forall \\tau \\in [0, 1]\\]\nDispersion: Is the concentration (variance) of \\(F\\) aligned with the concentration of observations?\nSharpness: How concentrated are the forecasts \\(F\\)?"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#probability-integral-transform-pit",
    "href": "slides/lecture10-1-cross-validation.html#probability-integral-transform-pit",
    "title": "Scoring and Cross-Validation",
    "section": "Probability Integral Transform (PIT)",
    "text": "Probability Integral Transform (PIT)\nCommon to use the PIT to make these more concrete: \\(Z_F = F(y)\\).\nThe forecast is probabilistically calibrated if \\(Z_F \\sim Uniform(0, 1)\\).\nThe forecast is properly dispersed if \\(\\text{Var}(Z_F) = 1/12\\).\nSharpness can be measured by the width of a particular prediction interval. A good forecast is a sharp as possible subject to calibration (Gneiting et al., 2007)."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#pit-example-well-calibrated",
    "href": "slides/lecture10-1-cross-validation.html#pit-example-well-calibrated",
    "title": "Scoring and Cross-Validation",
    "section": "PIT Example: Well-Calibrated",
    "text": "PIT Example: Well-Calibrated\n\nCode\n# \"true\" observation distribution is N(2, 0.5)\nobs = rand(Normal(2, 0.5), 50)\n# forecast according to the \"correct\" distribution and obtain PIT\npit_corr = cdf(Normal(2, 0.45), obs)\np_corr = histogram(pit_corr, bins=10, label=false, xlabel=L\"$y$\", ylabel=\"Count\", size=(500, 500))\n\nxrange = 0:0.01:5\np_cdf1 = plot(xrange, cdf.(Normal(2, 0.4), xrange), xlabel=L\"$y$\", ylabel=\"Cumulative Density\", label=\"Forecast\", size=(500, 500))\nplot!(p_cdf1, xrange, cdf.(Normal(2, 0.5), xrange), label=\"Truth\")\n\ndisplay(p_cdf1)\ndisplay(p_corr)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Comparison of “proper” and overdispersed PIT.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#pit-example-underdispersed",
    "href": "slides/lecture10-1-cross-validation.html#pit-example-underdispersed",
    "title": "Scoring and Cross-Validation",
    "section": "PIT Example: Underdispersed",
    "text": "PIT Example: Underdispersed\n\nCode\n# forecast according to an underdispersed distribution and obtain PIT\npit_under = cdf(Normal(2, 0.1), obs)\np_under = histogram(pit_under, bins=10, label=false, xlabel=L\"$y$\", ylabel=\"Count\", size=(500, 500))\n\nxrange = 0:0.01:5\np_cdf2 = plot(xrange, cdf.(Normal(2, 0.1), xrange), xlabel=L\"$y$\", ylabel=\"Cumulative Density\", label=\"Forecast\", size=(500, 500))\nplot!(p_cdf2, xrange, cdf.(Normal(2, 0.5), xrange), label=\"Truth\")\n\ndisplay(p_cdf2)\ndisplay(p_under)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Comparison of “proper” and overdispersed PIT.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#pit-example-overdispersed",
    "href": "slides/lecture10-1-cross-validation.html#pit-example-overdispersed",
    "title": "Scoring and Cross-Validation",
    "section": "PIT Example: Overdispersed",
    "text": "PIT Example: Overdispersed\n\nCode\n# forecast according to an overdispersed distribution and obtain PIT\npit_over = cdf(Normal(2, 1), obs)\np_over = histogram(pit_over, bins=10, label=false, xlabel=L\"$y$\", ylabel=\"Count\", size=(500, 500))\n\nxrange = 0:0.01:5\np_cdf3 = plot(xrange, cdf.(Normal(2, 1), xrange), xlabel=L\"$y$\", ylabel=\"Cumulative Density\", label=\"Forecast\", size=(500, 500))\nplot!(p_cdf3, xrange, cdf.(Normal(2, 0.5), xrange), label=\"Truth\")\n\ndisplay(p_cdf3)\ndisplay(p_over)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Comparison of “proper” and overdispersed PIT.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#scoring-rules",
    "href": "slides/lecture10-1-cross-validation.html#scoring-rules",
    "title": "Scoring and Cross-Validation",
    "section": "Scoring Rules",
    "text": "Scoring Rules\nScoring rules compare observations against an entire probabilistic forecast.\nA scoring rule \\(S(F, y)\\) measures the “loss” of a predicted probability distribution \\(F\\) once an observation \\(y\\) is obtained.\nTypically oriented so smaller = better."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#scoring-rule-examples",
    "href": "slides/lecture10-1-cross-validation.html#scoring-rule-examples",
    "title": "Scoring and Cross-Validation",
    "section": "Scoring Rule Examples",
    "text": "Scoring Rule Examples\n\nLogarithmic: \\(S(F, y) = -\\log F(y)\\)\nQuadratic (Brier): \\(S(F, y) = -2F(y) - \\int_{-\\infty}^\\infty F^2(z) dz\\) / \\(B(F, y) = \\sum_i (y_i - F(y_i))^2\\)\nContinous Ranked Probability Score (CRPS): \\[\\begin{align*}\nS(F, y) &= -\\int (F(z) - \\mathbb{I}(y \\leq z))^2 dz \\\\\n&= \\mathbb{E}_F |Y -y| - \\frac{1}{2} E_F | Y - Y'|\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#proper-scoring-rules",
    "href": "slides/lecture10-1-cross-validation.html#proper-scoring-rules",
    "title": "Scoring and Cross-Validation",
    "section": "Proper Scoring Rules",
    "text": "Proper Scoring Rules\nProper scoring rules are intended to encourage forecasters to provide their full (and honest) forecasts.\nMinimized when the forecasted distribution matches the observed distribution:\n\\[\\mathbb{E}_Y(S(G, G)) \\leq \\mathbb{E}_Y(S(F, G)) \\qquad \\forall F.\\]\nIt is strictly proper if equality holds only if \\(F = G\\)."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#sidenote-why-not-use-classification-accuracy",
    "href": "slides/lecture10-1-cross-validation.html#sidenote-why-not-use-classification-accuracy",
    "title": "Scoring and Cross-Validation",
    "section": "Sidenote: Why Not Use Classification Accuracy?",
    "text": "Sidenote: Why Not Use Classification Accuracy?\nMost classification algorithms produce a probability (e.g. logistic regression) of different outcomes.\nA common skill metric for classification models is accuracy (sensitivity/specificity): given these probabilities and some threshold to translate them into categorical prediction."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#the-problem-with-classification-accuracy",
    "href": "slides/lecture10-1-cross-validation.html#the-problem-with-classification-accuracy",
    "title": "Scoring and Cross-Validation",
    "section": "The Problem With Classification Accuracy",
    "text": "The Problem With Classification Accuracy\nThe problem: This translation is a decision problem, not a statistical problem. A probabilistic scoring rule over the predicted probabilities more accurately reflects the skill of the statistical model."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#logarithmic-score-as-scoring-rule",
    "href": "slides/lecture10-1-cross-validation.html#logarithmic-score-as-scoring-rule",
    "title": "Scoring and Cross-Validation",
    "section": "Logarithmic Score As Scoring Rule",
    "text": "Logarithmic Score As Scoring Rule\nThe logarithmic score \\(S(F, y) = -\\log F(y)\\) is (up to equivalence) the only local strictly proper scoring rule (locality ⇒ score depends only on the observation).\nThis is the negative log-probability: straightforward to use for the likelihood (frequentist forecasts) or posterior (Bayesian forecasts) and generalizes MSE.\nWe will focus on the logarithmic score."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#important-caveat",
    "href": "slides/lecture10-1-cross-validation.html#important-caveat",
    "title": "Scoring and Cross-Validation",
    "section": "Important Caveat",
    "text": "Important Caveat\nA model can predict well without being “correct”!\nFor example, model selection using predictive criteria does not mean you are selecting the “true” model.\nThe causes of the data cannot be found in the data alone."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#can-we-drive-model-error-to-zero",
    "href": "slides/lecture10-1-cross-validation.html#can-we-drive-model-error-to-zero",
    "title": "Scoring and Cross-Validation",
    "section": "Can We Drive Model Error to Zero?",
    "text": "Can We Drive Model Error to Zero?\nEffectively, no. Why?\n\n\nInherent noise: even a perfect model wouldn’t perfectly predict observations.\nModel mis-specification (the cause of bias)\nModel estimation is never “right” (the cause of variance)"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#quantifying-generalization-error",
    "href": "slides/lecture10-1-cross-validation.html#quantifying-generalization-error",
    "title": "Scoring and Cross-Validation",
    "section": "Quantifying Generalization Error",
    "text": "Quantifying Generalization Error\nThe goal is then to minimize the generalized (expected) error:\n\\[\\mathbb{E}\\left[L(X, \\theta)\\right] = \\int_X L(x, \\theta) \\pi(x)dx\\]\nwhere \\(L(x, \\theta)\\) is an error function capturing the discrepancy between \\(\\hat{f}(x, \\theta)\\) and \\(y\\)."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#in-sample-error",
    "href": "slides/lecture10-1-cross-validation.html#in-sample-error",
    "title": "Scoring and Cross-Validation",
    "section": "In-Sample Error",
    "text": "In-Sample Error\nSince we don’t know the “true” distribution of \\(y\\), we could try to approximate it using the training data:\n\\[\\hat{L} = \\min_{\\theta \\in \\Theta} L(x_n, \\theta)\\]\nBut: This is minimizing in-sample error and is likely to result an optimistic score."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#held-out-data",
    "href": "slides/lecture10-1-cross-validation.html#held-out-data",
    "title": "Scoring and Cross-Validation",
    "section": "Held Out Data",
    "text": "Held Out Data\nInstead, let’s divide our data into a training dataset \\(y_k\\) and testing dataset \\(\\tilde{y}_l\\).\n\nFit the model to \\(y_k\\);\nEvaluate error on \\(\\tilde{y}_l\\).\n\nThis results in an unbiased estimate of \\(\\hat{L}\\) but is noisy."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#k-fold-cross-validation",
    "href": "slides/lecture10-1-cross-validation.html#k-fold-cross-validation",
    "title": "Scoring and Cross-Validation",
    "section": "\\(k\\)-Fold Cross-Validation",
    "text": "\\(k\\)-Fold Cross-Validation\nWhat if we repeated this procedure for multiple held-out sets?\n\nRandomly split data into \\(k = n / m\\) equally-sized subsets.\nFor each \\(i = 1, \\ldots, k\\), fit model to \\(y_{-i}\\) and test on \\(y_i\\).\n\nIf data are large, this is a good approximation."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#leave-one-out-cross-validation-loocv",
    "href": "slides/lecture10-1-cross-validation.html#leave-one-out-cross-validation-loocv",
    "title": "Scoring and Cross-Validation",
    "section": "Leave-One-Out Cross-Validation (LOOCV)",
    "text": "Leave-One-Out Cross-Validation (LOOCV)\nThe problem with \\(k\\)-fold CV, when data is scarce, is withholding \\(n/k\\) points.\nLOO-CV: Set \\(k=n\\)\nThe trouble: estimates of \\(L\\) are highly correlated since every two datasets share \\(n-2\\) points.\nThe benefit: LOO-CV approximates seeing “the next datum”."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#loo-cv-algorithm",
    "href": "slides/lecture10-1-cross-validation.html#loo-cv-algorithm",
    "title": "Scoring and Cross-Validation",
    "section": "LOO-CV Algorithm",
    "text": "LOO-CV Algorithm\n\nDrop one value \\(y_i\\).\nRefit model on rest of data \\(y_{-i}\\).\nPredict dropped point \\(p(\\hat{y}_i | y_{-i})\\).\nEvaluate score on dropped point (\\(-\\log p(y_i | y_{-i})\\)).\nRepeat on rest of data set."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#loo-cv-example",
    "href": "slides/lecture10-1-cross-validation.html#loo-cv-example",
    "title": "Scoring and Cross-Validation",
    "section": "LOO-CV Example",
    "text": "LOO-CV Example\n\n\nModel: \\[D \\rightarrow S \\ {\\color{purple}\\leftarrow U}\\] \\[S = f(D, U)\\]\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Data for CV example."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#loo-cv-flow",
    "href": "slides/lecture10-1-cross-validation.html#loo-cv-flow",
    "title": "Scoring and Cross-Validation",
    "section": "LOO-CV Flow",
    "text": "LOO-CV Flow\n\n\n\nDrop one value \\(y_i\\).\nRefit model on \\(y_{-i}\\).\nPredict \\(p(\\hat{y}_i | y_{-i})\\).\nEvaluate \\(-\\log p(y_i | y_{-i})\\).\nRepeat on rest of data set.\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Data for CV example."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#loo-cv-flow-1",
    "href": "slides/lecture10-1-cross-validation.html#loo-cv-flow-1",
    "title": "Scoring and Cross-Validation",
    "section": "LOO-CV Flow",
    "text": "LOO-CV Flow\n\n\n\nDrop one value \\(y_i\\).\nRefit model on \\(y_{-i}\\).\nPredict \\(p(\\hat{y}_i | y_{-i})\\).\nEvaluate \\(-\\log p(y_i | y_{-i})\\).\nRepeat on rest of data set.\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Data for CV example."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#loo-cv-flow-2",
    "href": "slides/lecture10-1-cross-validation.html#loo-cv-flow-2",
    "title": "Scoring and Cross-Validation",
    "section": "LOO-CV Flow",
    "text": "LOO-CV Flow\n\n\n\nDrop one value \\(y_i\\).\nRefit model on \\(y_{-i}\\).\nPredict \\(p(\\hat{y}_i | y_{-i})\\).\nEvaluate \\(-\\log p(y_i | y_{-i})\\).\nRepeat on rest of data set.\n\n\nOut of Sample:\n\\(p(y_i | y_{-i})\\) = 5.2\nIn Sample:\n\\(p(\\hat{y}_{-i} | y_{-i})\\) = 5.7"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#loo-cv-flow-3",
    "href": "slides/lecture10-1-cross-validation.html#loo-cv-flow-3",
    "title": "Scoring and Cross-Validation",
    "section": "LOO-CV Flow",
    "text": "LOO-CV Flow\n\n\n\nDrop one value \\(y_i\\).\nRefit model on \\(y_{-i}\\).\nPredict \\(p(\\hat{y}_i | y_{-i})\\).\nEvaluate \\(-\\log p(y_i | y_{-i})\\).\nRepeat on rest of data set.\n\n\nLOO-CV Score: 5.8\nThis is the average log-likelihood of out-of-sample data."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#bayesian-loo-cv",
    "href": "slides/lecture10-1-cross-validation.html#bayesian-loo-cv",
    "title": "Scoring and Cross-Validation",
    "section": "Bayesian LOO-CV",
    "text": "Bayesian LOO-CV\nBayesian LOO-CV involves using the posterior predictive distribution\n\\[\\begin{align*}\n\\text{lppd}_\\text{cv} &= \\sum_{i=1}^N \\log p_{\\text{post}}(y_i | \\theta_{-i}) \\\\\n&\\approx \\sum_{i=1}^N \\frac{1}{S} \\sum_{s=1}^S log p_{\\text{post}}(y_i | \\theta_{-i, s}),\n\\end{align*}\\]\nwhich requires refitting the model without \\(y_i\\) for every data point."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#leave-k-out-cross-validation",
    "href": "slides/lecture10-1-cross-validation.html#leave-k-out-cross-validation",
    "title": "Scoring and Cross-Validation",
    "section": "Leave-\\(k\\)-Out Cross-Validation",
    "text": "Leave-\\(k\\)-Out Cross-Validation\nDrop \\(k\\) values, refit model on rest of data, check for predictive skill.\nAs \\(k \\to n\\), this reduces to the prior predictive distribution \\[p(y^{\\text{rep}}) = \\int_{\\theta} p(y^{\\text{rep}} | \\theta) p(\\theta) d\\theta.\\]"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#cross-validation-and-model-tuning",
    "href": "slides/lecture10-1-cross-validation.html#cross-validation-and-model-tuning",
    "title": "Scoring and Cross-Validation",
    "section": "Cross-Validation and Model Tuning",
    "text": "Cross-Validation and Model Tuning\nCan use cross-validation to evaluate overfitting instead of using different model structure.\nWhat happens to CV error with tighter priors/regularization penalty?\nBut remember, prediction is not the same as scientific inference: try to balance both considerations."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#challenges-with-cross-validation",
    "href": "slides/lecture10-1-cross-validation.html#challenges-with-cross-validation",
    "title": "Scoring and Cross-Validation",
    "section": "Challenges with Cross-Validation",
    "text": "Challenges with Cross-Validation\n\nThis can be very computationally expensive!\nWe often don’t have a lot of data for calibration, so holding some back can be a problem.\nCan have a negative bias for future prediction.\nHow to divide data with spatial or temporal structure? This can be addressed by partitioning the data more cleverly: \\[y = \\{y_{1:t}, y_{-((t+1):T)}\\}\\] but this makes the data problem worse."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#key-points-scoring-rules",
    "href": "slides/lecture10-1-cross-validation.html#key-points-scoring-rules",
    "title": "Scoring and Cross-Validation",
    "section": "Key Points (Scoring Rules)",
    "text": "Key Points (Scoring Rules)\n\nProbabilistic forecasts should be assessed based on both calibration and sharpness.\nScoring rules as measures of probabilistic forecast skill.\nLogarithmic score (negative log-probability) is the unique locally proper scoring rule."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#key-points-cross-validation",
    "href": "slides/lecture10-1-cross-validation.html#key-points-cross-validation",
    "title": "Scoring and Cross-Validation",
    "section": "Key Points (Cross-Validation)",
    "text": "Key Points (Cross-Validation)\n\nGold standard for predictive skill assessment.\nHold out data (one or more points) randomly, refit model, and quantify predictive skill.\nLOO-CV maximizes use of data but can be computationally expensive."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#next-classes",
    "href": "slides/lecture10-1-cross-validation.html#next-classes",
    "title": "Scoring and Cross-Validation",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Entropy and Information Criteria"
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#assessments",
    "href": "slides/lecture10-1-cross-validation.html#assessments",
    "title": "Scoring and Cross-Validation",
    "section": "Assessments",
    "text": "Assessments\nHW4: Due on 4/11 at 9pm."
  },
  {
    "objectID": "slides/lecture10-1-cross-validation.html#references-scroll-for-full-list",
    "href": "slides/lecture10-1-cross-validation.html#references-scroll-for-full-list",
    "title": "Scoring and Cross-Validation",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nGneiting, T., Fadoua Balabdaoui, & Raftery, A. E. (2007). Probabilistic Forecasts, Calibration and Sharpness. J. R. Stat. Soc. Series B Stat. Methodol., 69, 243–268. Retrieved from http://www.jstor.org/stable/4623266"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#the-bootstrap",
    "href": "slides/lecture08-2-bayesian-computation.html#the-bootstrap",
    "title": "Bayesian Computing and MCMC",
    "section": "The Bootstrap",
    "text": "The Bootstrap\n\n\nEfron (1979) suggested combining estimation with simulation: the bootstrap.\nKey idea: use the data to simulate a data-generating mechanism.\n\n\n\n\n\nBaron von Munchhausen Pulling Himself By His Hair\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#why-does-the-bootstrap-work",
    "href": "slides/lecture08-2-bayesian-computation.html#why-does-the-bootstrap-work",
    "title": "Bayesian Computing and MCMC",
    "section": "Why Does The Bootstrap Work?",
    "text": "Why Does The Bootstrap Work?\nLet \\(t_0\\) the “true” value of a statistic, \\(\\hat{t}\\) the estimate of the statistic from the sample, and \\((\\tilde{t}_i)\\) the bootstrap estimates.\n\nVariance: \\(\\text{Var}[\\hat{t}] \\approx \\text{Var}[\\tilde{t}]\\)\nThen the bootstrap error distribution approximates the sampling distribution \\[(\\tilde{t}_i - \\hat{t}) \\overset{\\mathcal{D}}{\\sim} \\hat{t} - t_0\\]"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#bootstrap-variants",
    "href": "slides/lecture08-2-bayesian-computation.html#bootstrap-variants",
    "title": "Bayesian Computing and MCMC",
    "section": "Bootstrap Variants",
    "text": "Bootstrap Variants\n\nResample Cases (Non-Parametric)\nResample Residuals (from fitted model trend)\nSimulate from Fitted Model (Parametric)"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#which-bootstrap-to-use",
    "href": "slides/lecture08-2-bayesian-computation.html#which-bootstrap-to-use",
    "title": "Bayesian Computing and MCMC",
    "section": "Which Bootstrap To Use?",
    "text": "Which Bootstrap To Use?\nDepends on trust in model “correctness”: - Do we trust the model specification to be reasonably correct? - Do we trust that we have enough samples to recover the empirical CDF? - Do we trust the data-generating process?"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#sampling-so-far",
    "href": "slides/lecture08-2-bayesian-computation.html#sampling-so-far",
    "title": "Bayesian Computing and MCMC",
    "section": "Sampling So Far",
    "text": "Sampling So Far\n\n\nRejection sampling (or importance sampling) to draw i.i.d. samples from a proposal density and reject/re-weight based on target.\nBoth require \\(f(x) \\leq M g(x)\\) for some \\(1 &lt; M &lt; \\infty\\).\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Example of rejection sampling for a Normal distribution"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#bayesian-computing-challenges",
    "href": "slides/lecture08-2-bayesian-computation.html#bayesian-computing-challenges",
    "title": "Bayesian Computing and MCMC",
    "section": "Bayesian Computing Challenges",
    "text": "Bayesian Computing Challenges\n\nSamples needed to compute posterior quantities (credible intervals, posterior predictive distributions, model skill estimates, etc.) with Monte Carlo.\nPosteriors often highly correlated.\nGrid approximation can help us visualize the posteriors in low dimensions.\nRejection sampling scales poorly to higher dimensions.\nConjugate priors only are appropriate in limited cases."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#what-would-make-a-good-algorithm",
    "href": "slides/lecture08-2-bayesian-computation.html#what-would-make-a-good-algorithm",
    "title": "Bayesian Computing and MCMC",
    "section": "What Would Make A Good Algorithm?",
    "text": "What Would Make A Good Algorithm?\nA wishlist:\n\nDon’t need to know the characteristics of the distribution.\nSamples will eventually be correctly distributed (given enough time).\nIdeally fast and requiring minimal tuning."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#how-can-we-do-this",
    "href": "slides/lecture08-2-bayesian-computation.html#how-can-we-do-this",
    "title": "Bayesian Computing and MCMC",
    "section": "How Can We Do This?",
    "text": "How Can We Do This?\nSuppose we want to sample a probability distribution \\(f(\\cdot)\\) and are at a parameter vector \\(x\\).\nWhat if we had a method that would let us stochastically jump from \\(x\\) to a new vector \\(y\\) in such a way that, eventually, we would visit any given vector wiith probability \\(f\\)?\nThis would let us trade convenience/flexibility for dependent samples."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#markov-chain-monte-carlo-mcmc",
    "href": "slides/lecture08-2-bayesian-computation.html#markov-chain-monte-carlo-mcmc",
    "title": "Bayesian Computing and MCMC",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\nThere is a mathematical process that has these properties: Markov Chains.\nThese methods are called Markov chain Monte Carlo (MCMC)."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#what-is-a-markov-chain",
    "href": "slides/lecture08-2-bayesian-computation.html#what-is-a-markov-chain",
    "title": "Bayesian Computing and MCMC",
    "section": "What Is A Markov Chain?",
    "text": "What Is A Markov Chain?\n\n\nConsider a stochastic process \\(\\{X_t\\}_{t \\in \\mathcal{T}}\\), where\n\n\\(X_t \\in \\mathcal{S}\\) is the state at time \\(t\\), and\n\\(\\mathcal{T}\\) is a time-index set (can be discrete or continuous)\n\\(\\mathbb{P}(s_i \\to s_j) = p_{ij}\\).\n\n\n\n\n\nMarkov State Space"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#markovian-property",
    "href": "slides/lecture08-2-bayesian-computation.html#markovian-property",
    "title": "Bayesian Computing and MCMC",
    "section": "Markovian Property",
    "text": "Markovian Property\nThis stochastic process is a Markov chain if it satisfies the Markovian (or memoryless) property: \\[\\begin{align*}\n\\mathbb{P}(X_{T+1} = s_i &| X_1=x_1, \\ldots, X_T=x_T) = \\\\ &\\qquad\\mathbb{P}(X_{T+1} = s_i| X_T=x_T)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#example-drunkards-walk",
    "href": "slides/lecture08-2-bayesian-computation.html#example-drunkards-walk",
    "title": "Bayesian Computing and MCMC",
    "section": "Example: “Drunkard’s Walk”",
    "text": "Example: “Drunkard’s Walk”\n\n\n\n\n:img Random Walk, 80%\n\n\n\n\nHow can we model the unconditional probability \\(\\mathbb{P}(X_T = s_i)\\)?\nHow about the conditional probability \\(\\mathbb{P}(X_T = s_i | X_{T-1} = x_{T-1})\\)?"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#example-weather",
    "href": "slides/lecture08-2-bayesian-computation.html#example-weather",
    "title": "Bayesian Computing and MCMC",
    "section": "Example: Weather",
    "text": "Example: Weather\nSuppose the weather can be foggy, sunny, or rainy.\nBased on past experience, we know that:\n\nThere are never two sunny days in a row;\nEven chance of two foggy or two rainy days in a row;\nA sunny day occurs 1/4 of the time after a foggy or rainy day."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#aside-higher-order-markov-chains",
    "href": "slides/lecture08-2-bayesian-computation.html#aside-higher-order-markov-chains",
    "title": "Bayesian Computing and MCMC",
    "section": "Aside: Higher Order Markov Chains",
    "text": "Aside: Higher Order Markov Chains\nSuppose that today’s weather depends on the prior two days.\n\nCan we write this as a Markov chain?\nWhat are the states?"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#weather-transition-matrix",
    "href": "slides/lecture08-2-bayesian-computation.html#weather-transition-matrix",
    "title": "Bayesian Computing and MCMC",
    "section": "Weather Transition Matrix",
    "text": "Weather Transition Matrix\nWe can summarize these probabilities in a transition matrix \\(P\\): \\[\nP =\n\\begin{array}{cc}\n\\begin{array}{ccc}\n\\phantom{i}\\color{red}{F}\\phantom{i} & \\phantom{i}\\color{red}{S}\\phantom{i} & \\phantom{i}\\color{red}{R}\\phantom{i}\n\\end{array}\n\\\\\n\\begin{pmatrix}\n      1/2 & 1/4 & 1/4 \\\\\n      1/2 & 0 & 1/2 \\\\\n      1/4 & 1/4 & 1/2\n      \\end{pmatrix}\n&\n\\begin{array}{ccc}\n\\color{red}F  \\\\ \\color{red}S  \\\\ \\color{red}R\n\\end{array}   \n\\end{array}\n\\]\nRows are the current state, columns are the next step, so \\(\\sum_i p_{ij} = 1\\)."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#weather-example-state-probabilities",
    "href": "slides/lecture08-2-bayesian-computation.html#weather-example-state-probabilities",
    "title": "Bayesian Computing and MCMC",
    "section": "Weather Example: State Probabilities",
    "text": "Weather Example: State Probabilities\nDenote by \\(\\lambda^t\\) a probability distribution over the states at time \\(t\\).\nThen \\(\\lambda^t = \\lambda^{t-1}P\\):\n\\[\\begin{pmatrix}\\lambda^t_F & \\lambda^t_S & \\lambda^t_R \\end{pmatrix} =  \n\\begin{pmatrix}\\lambda^{t-1}_F & \\lambda^{t-1}_S & \\lambda^{t-1}_R \\end{pmatrix}\n      \\begin{pmatrix}\n      1/2 & 1/4 & 1/4 \\\\\n      1/2 & 0 & 1/2 \\\\\n      1/4 & 1/4 & 1/2\n      \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#multi-transition-probabilities",
    "href": "slides/lecture08-2-bayesian-computation.html#multi-transition-probabilities",
    "title": "Bayesian Computing and MCMC",
    "section": "Multi-Transition Probabilities",
    "text": "Multi-Transition Probabilities\nNotice that \\[\\lambda^{t+i} = \\lambda^t P^i,\\] so multiple transition probabilities are \\(P\\)-exponentials.\n\\[P^3 =\n\\begin{array}{cc}\n\\begin{array}{ccc}\n\\phantom{iii}\\color{red}{F}\\phantom{ii} & \\phantom{iii}\\color{red}{S}\\phantom{iii} & \\phantom{ii}\\color{red}{R}\\phantom{iii}\n\\end{array}\n\\\\\n\\begin{pmatrix}\n      26/64 & 13/64 & 25/64 \\\\\n      26/64 & 12/64 & 26/64 \\\\\n      26/64 & 13/64 & 26/64\n      \\end{pmatrix}\n&\n\\begin{array}{ccc}\n\\color{red}F  \\\\ \\color{red}S  \\\\ \\color{red}R\n\\end{array}   \n\\end{array}\n\\]"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#long-run-probabilities",
    "href": "slides/lecture08-2-bayesian-computation.html#long-run-probabilities",
    "title": "Bayesian Computing and MCMC",
    "section": "Long Run Probabilities",
    "text": "Long Run Probabilities\nWhat happens if we let the system run for a while starting from an initial sunny day?\n\n\nCode\ncurrent = [1.0, 0.0, 0.0]\nP = [1/2 1/4 1/4\n    1/2 0 1/2\n    1/4 1/4 1/2]   \n\nT = 21\n\nstate_probs = zeros(T, 3)\nstate_probs[1,:] = current\nfor t=1:T-1\n    state_probs[t+1, :] = state_probs[t:t, :] * P\nend\n\n\np = plot(0:T-1, state_probs, label=[\"Foggy\" \"Sunny\" \"Rainy\"], palette=:mk_8, linewidth=3)\nxlabel!(\"Time\")\nylabel!(\"State Probability\")\nplot!(p, size=(1000, 350))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: State probabilities for the weather examples."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#stationary-distributions",
    "href": "slides/lecture08-2-bayesian-computation.html#stationary-distributions",
    "title": "Bayesian Computing and MCMC",
    "section": "Stationary Distributions",
    "text": "Stationary Distributions\nThis stabilization always occurs when the probability distribution is an eigenvector of \\(P\\) with eigenvalue 1:\n\\[\\pi = \\pi P.\\]\nThis is called an invariant or a stationary distribution."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#which-markov-chains-have-stationary-distributions",
    "href": "slides/lecture08-2-bayesian-computation.html#which-markov-chains-have-stationary-distributions",
    "title": "Bayesian Computing and MCMC",
    "section": "Which Markov Chains Have Stationary Distributions?",
    "text": "Which Markov Chains Have Stationary Distributions?\nThis is a property called ergodicity (or the chain is ergodic). Ergodic Markov chains always have a limiting distribution which is the limit of the time-evolution of the chain dynamics, e.g. \\[\\pi_j = \\lim_{t \\to \\infty} \\mathbb{P}(X_t = s_j).\\]\nKey: The limiting distribution is independent of the initial state probability."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#limiting-distributions-are-stationary",
    "href": "slides/lecture08-2-bayesian-computation.html#limiting-distributions-are-stationary",
    "title": "Bayesian Computing and MCMC",
    "section": "Limiting Distributions are Stationary",
    "text": "Limiting Distributions are Stationary\nFor an ergodic chain, the limiting distribution is the unique stationary distribution (we won’t prove uniqueness):\n\\[\n\\begin{align}\n\\pi_j &= \\lim_{t \\to \\infty} \\mathbb{P}(X_t = s_j | X_0 = s_i) \\\\\n&= \\lim_{t \\to \\infty} (P^{t+1})_{ij} = \\lim_{t \\to \\infty} (P^tP)_{ij} \\\\\n&= \\lim_{t \\to \\infty} \\sum_d (P^t)_{id} P_{dj} \\\\\n&= \\sum_d \\pi_d P_{dj}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#mcmc-and-ergodic-chains",
    "href": "slides/lecture08-2-bayesian-computation.html#mcmc-and-ergodic-chains",
    "title": "Bayesian Computing and MCMC",
    "section": "MCMC and Ergodic Chains",
    "text": "MCMC and Ergodic Chains\nProving that a chain is ergodic is getting into the mathematical weeds a bit (and is outside the scope of this class).\nThe good news: The goal of any MCMC algorithm is to construct an ergodic chain where the stationary distribution \\(\\pi(\\cdot)\\) is the target \\(f(\\cdot)\\).\nThis means that if you’re using a “standard” algorithm, the existence of a stationary distribution for the produced Markov chain is mathematically guaranteed."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#transient-portion-of-the-chain",
    "href": "slides/lecture08-2-bayesian-computation.html#transient-portion-of-the-chain",
    "title": "Bayesian Computing and MCMC",
    "section": "Transient Portion of the Chain",
    "text": "Transient Portion of the Chain\nThe portion of the chain prior to convergence to the stationary distribution is called the transient portion.\n\n\nCode\nvspan!(p, [0, 4], color=:red, alpha=0.3, label=\"Transient Portion\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Transient portion of the weather Markov chain."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#key-points-bayesian-computing",
    "href": "slides/lecture08-2-bayesian-computation.html#key-points-bayesian-computing",
    "title": "Bayesian Computing and MCMC",
    "section": "Key Points (Bayesian Computing)",
    "text": "Key Points (Bayesian Computing)\n\nBayesian computation is difficult because we need to sample from effectively arbitrary distributions.\nMarkov chains provide a path forward if we can construct a chain satisfying detailed balance whose stationary distribution is the target distribution.\nThen a post-convergence chain of samples is the same as a dependent Monte Carlo set of samples."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#key-points-markov-chains",
    "href": "slides/lecture08-2-bayesian-computation.html#key-points-markov-chains",
    "title": "Bayesian Computing and MCMC",
    "section": "Key Points (Markov Chains)",
    "text": "Key Points (Markov Chains)\n\nStochastic process with memoryless property.\nSome chains have a stationary distribution (eigenvector of transition matrix with eigenvalue 1).\nErgodic Markov chains: dynamics converge to a limiting distribution, which is also stationary."
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#next-classes",
    "href": "slides/lecture08-2-bayesian-computation.html#next-classes",
    "title": "Bayesian Computing and MCMC",
    "section": "Next Classes",
    "text": "Next Classes\nMonday: MCMC\nWednesday: Cross-Validation and Model Skill"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#assessments",
    "href": "slides/lecture08-2-bayesian-computation.html#assessments",
    "title": "Bayesian Computing and MCMC",
    "section": "Assessments",
    "text": "Assessments\n\nHomework 3: Due Friday (3/14)\nProject Proposal: Due 3/21"
  },
  {
    "objectID": "slides/lecture08-2-bayesian-computation.html#references-scroll-for-full-list",
    "href": "slides/lecture08-2-bayesian-computation.html#references-scroll-for-full-list",
    "title": "Bayesian Computing and MCMC",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nEfron, B. (1979). Bootstrap methods: Another look at the jackknife. Ann. Stat., 7, 1–26. https://doi.org/10.1214/aos/1176344552"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#bayes-rule",
    "href": "slides/lecture06-1-bayes-workflow.html#bayes-rule",
    "title": "Bayesian Workflow Example",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\\[\n\\underbrace{{p(\\theta | y)}}_{\\text{posterior}} = \\frac{\\overbrace{p(y | \\theta)}^{\\text{likelihood}}}{\\underbrace{p(y)}_\\text{normalization}} \\overbrace{p(\\theta)}^\\text{prior}\n\\]"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#bayesian-model-components",
    "href": "slides/lecture06-1-bayes-workflow.html#bayesian-model-components",
    "title": "Bayesian Workflow Example",
    "section": "Bayesian Model Components",
    "text": "Bayesian Model Components\nA fully specified Bayesian model includes:\n\nPrior distributions over the parameters, \\(p(\\theta)\\)\nProbability model for the data given the parameters (the likelihood), \\(p(y | \\theta)\\)t\n\nThink: Prior provides proposed explanations, likelihood re-weights based on ability to produce the data."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#bayes-and-parametric-uncertainty",
    "href": "slides/lecture06-1-bayes-workflow.html#bayes-and-parametric-uncertainty",
    "title": "Bayesian Workflow Example",
    "section": "Bayes and Parametric Uncertainty",
    "text": "Bayes and Parametric Uncertainty\nFrequentist: Parametric uncertainty is purely the result of sampling variability\nBayesian: Parameters have probabilities based on consistency with data and priors.\nThink: how “likely” is a set of parameters to have produced the data given the specified data generating process?"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#bayesian-updating",
    "href": "slides/lecture06-1-bayes-workflow.html#bayesian-updating",
    "title": "Bayesian Workflow Example",
    "section": "Bayesian Updating",
    "text": "Bayesian Updating\n\nThe posterior is a “compromise” between the prior and the data.\nThe posterior mean is a weighted combination of the data and the prior mean.\nThe weights depend on the prior and the likelihood variances.\nMore data usually makes the posterior more confident."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#san-francisco-tide-gauge-data",
    "href": "slides/lecture06-1-bayes-workflow.html#san-francisco-tide-gauge-data",
    "title": "Bayesian Workflow Example",
    "section": "San Francisco Tide Gauge Data",
    "text": "San Francisco Tide Gauge Data\n\n\nCode\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(DataFrames.transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18\n)\np2 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(1, 1.7), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#proposed-probability-model",
    "href": "slides/lecture06-1-bayes-workflow.html#proposed-probability-model",
    "title": "Bayesian Workflow Example",
    "section": "Proposed Probability Model",
    "text": "Proposed Probability Model\n\\[\n\\begin{align*}\n& y \\sim LogNormal(\\mu, \\sigma) \\tag{likelihood}\\\\\n& \\left. \\begin{aligned}\n& \\mu \\sim Normal(0, 1) \\\\\n& \\sigma \\sim HalfNormal(0, 5)\n\\end{aligned} \\right\\} \\tag{priors}\n\\end{align*}\n\\]\nWant to find:\n\\[p(\\mu, \\sigma | y) \\propto p(y | \\mu, \\sigma) p(\\mu)p(\\sigma)\\]"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#are-our-priors-reasonable",
    "href": "slides/lecture06-1-bayes-workflow.html#are-our-priors-reasonable",
    "title": "Bayesian Workflow Example",
    "section": "Are Our Priors Reasonable?",
    "text": "Are Our Priors Reasonable?\nKey idea: what do the priors imply for observable variables?\nLet’s simulate data from the prior predictive distribution to see we get plausible outcomes.\n\\[y \\sim p(\\tilde{y}) = \\int_{\\Theta} p(\\tilde{y} | \\theta) p(\\theta) d\\theta\\]"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#prior-predictive-check",
    "href": "slides/lecture06-1-bayes-workflow.html#prior-predictive-check",
    "title": "Bayesian Workflow Example",
    "section": "Prior Predictive Check",
    "text": "Prior Predictive Check\n\n\nCode\n# sample from priors\nμ_sample = rand(Normal(0, 1), 1_000)\nσ_sample = rand(truncated(Normal(0, 5), 0, +Inf), 1_000)\n\n# define return periods and cmopute return levels for parameters\nreturn_periods = 2:100\nreturn_levels = zeros(1_000, length(return_periods))\nfor i in 1:1_000\n    return_levels[i, :] = quantile.(LogNormal(μ_sample[i], σ_sample[i]), 1 .- (1 ./ return_periods))\nend\n\nplt_prior_1 = plot(; yscale=:log10, yticks=10 .^ collect(0:2:16), ylabel=\"Return Level (m)\", xlabel=\"Return Period (yrs)\",\n    tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm, legend=:topleft)\nfor idx in 1:1_000\n    label = idx == 1 ? \"Prior\" : false\n    plot!(plt_prior_1, return_periods, return_levels[idx, :]; color=:black, alpha=0.1, label=label)\nend\nplt_prior_1\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Prior predictive check of return periods with revised model"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#lets-revise-the-prior",
    "href": "slides/lecture06-1-bayes-workflow.html#lets-revise-the-prior",
    "title": "Bayesian Workflow Example",
    "section": "Let’s Revise the Prior",
    "text": "Let’s Revise the Prior\n\\[\n\\begin{align*}\n& y \\sim LogNormal(\\mu, \\sigma) \\tag{likelihood}\\\\\n& \\left. \\begin{aligned}\n& \\mu \\sim Normal(0, 0.5) \\\\\n& \\sigma \\sim HalfNormal(0, 0.1)\n\\end{aligned} \\right\\} \\tag{priors}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#prior-predictive-check-2",
    "href": "slides/lecture06-1-bayes-workflow.html#prior-predictive-check-2",
    "title": "Bayesian Workflow Example",
    "section": "Prior Predictive Check 2",
    "text": "Prior Predictive Check 2\n\n\nCode\n# sample from priors\nμ_sample = rand(Normal(0, 0.5), 1_000)\nσ_sample = rand(truncated(Normal(0, 0.1), 0, +Inf), 1_000)\n\nreturn_periods = 2:100\nreturn_levels = zeros(1_000, length(return_periods))\nfor i in 1:1_000\n    return_levels[i, :] = quantile.(LogNormal(μ_sample[i], σ_sample[i]), 1 .- (1 ./ return_periods))\nend\n\nplt_prior_2 = plot(; ylabel=\"Return Level (m)\", xlabel=\"Return Period (yrs)\", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm)\nfor idx in 1:1_000\n    label = idx == 1 ? \"Prior\" : false\n    plot!(plt_prior_2, return_periods, return_levels[idx, :]; color=:black, alpha=0.1, label=label)\nend\nplt_prior_2\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Prior predictive check of return periods with revised model"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#compute-posterior",
    "href": "slides/lecture06-1-bayes-workflow.html#compute-posterior",
    "title": "Bayesian Workflow Example",
    "section": "Compute Posterior",
    "text": "Compute Posterior\n\nCode\nll(μ, σ) = sum(logpdf(LogNormal(μ, σ), dat_annmax.residual))\nlprior1(μ, σ) = logpdf(Normal(0, 1), μ) + logpdf(truncated(Normal(0, 5), 0, Inf), σ)\nlprior2(μ, σ) = logpdf(Normal(0, 0.5), μ) + logpdf(truncated(Normal(0, 0.1), 0, Inf), σ)\nlposterior1(μ, σ) = ll(μ, σ) + lprior1(μ, σ)\nlposterior2(μ, σ) = ll(μ, σ) + lprior2(μ, σ)\n\np_map1 = optimize(p -&gt; -lposterior1(p[1], p[2]), [0.0, 0.0], [1.0, 1.0], [0.5, 0.5]).minimizer\np_map2 = optimize(p -&gt; -lposterior2(p[1], p[2]), [0.0, 0.0], [1.0, 1.0], [0.5, 0.5]).minimizer\n\nμ = 0.15:0.005:0.35\nσ = 0.04:0.01:0.1\nposterior1_vals = @. lposterior1(μ', σ)\nposterior2_vals = @. lposterior2(μ', σ)\n\np_post1 = contour(μ, σ, posterior1_vals, \n    levels=100, \n    clabels=false, \n    cbar=false, lw=1, \n    fill=(true,cgrad(:grays,[0,0.1,1.0])),\n    title = \"Diffuse Prior\"\n)\nscatter!(p_post1, [p_map1[1]], [p_map1[2]], label=\"MLE\", markersize=10, marker=:star)\nxlabel!(p_post1, L\"$\\mu$\")\nylabel!(p_post1, L\"$\\sigma$\")\nplot!(p_post1, size=(600, 500))\n\np_post2 = contour(μ, σ, posterior2_vals, \n    levels=100, \n    clabels=false, \n    cbar=false, lw=1, \n    fill=(true,cgrad(:grays,[0,0.1,1.0])),\n    title = \"More Informed Priors\"\n)\nscatter!(p_post2, [p_map2[1]], [p_map2[2]], label=\"MAP\", markersize=10, marker=:star)\nxlabel!(p_post2, L\"$\\mu$\")\nylabel!(p_post2, L\"$\\sigma$\")\nplot!(p_post2, size=(600, 500))\n\ndisplay(p_post1)\ndisplay(p_post2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Posterior samples from surge model.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\np_map1 = [0.25470601822948175, 0.05548961712923218]\np_map2 = [0.2546874075933288, 0.05542213686160764]"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#assess-map-fit",
    "href": "slides/lecture06-1-bayes-workflow.html#assess-map-fit",
    "title": "Bayesian Workflow Example",
    "section": "Assess MAP Fit",
    "text": "Assess MAP Fit\n\nCode\np1 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    legend=:false,\n    ylabel=\"PDF\",\n    xlabel=\"Annual Max Tide Level (m)\",\n    tickfontsize=16,\n    guidefontsize=18,\n    bottom_margin=5mm, left_margin=5mm\n)\nplot!(p1, LogNormal(p_map2[1], p_map2[2]),\n    linewidth=3,\n    color=:red)\nxlims!(p1, (1, 1.7))\nplot!(p1, size=(600, 450))\n\nreturn_periods = 2:500\nreturn_levels = quantile.(LogNormal(p_map2[1], p_map2[2]), 1 .- (1 ./ return_periods))\n\n# function to calculate exceedance probability and plot positions based on data quantile\nfunction exceedance_plot_pos(y)\n    N = length(y)\n    ys = sort(y; rev=false) # sorted values of y\n    nxp = xp = [r / (N + 1) for r in 1:N] # exceedance probability\n    xp = 1 .- nxp\n    return xp, ys\nend\nxp, ys = exceedance_plot_pos(dat_annmax.residual)\n\np2 = plot(return_periods, return_levels, linewidth=3, color=:blue, label=\"Model Fit\", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=5mm, left_margin=5mm, right_margin=10mm, legend=:bottomright)\nscatter!(p2, 1 ./ xp, ys, label=\"Observations\", color=:black, markersize=5)\nxlabel!(p2, \"Return Period (yrs)\")\nylabel!(p2, \"Return Level (m)\")\nxlims!(-1, 300)\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Checks for model fit.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#what-about-the-posterior-distribution",
    "href": "slides/lecture06-1-bayes-workflow.html#what-about-the-posterior-distribution",
    "title": "Bayesian Workflow Example",
    "section": "What About The Posterior Distribution?",
    "text": "What About The Posterior Distribution?\nOne of the points of Bayesian statistics is we get a distribution over parameters.\nSampling from this distribution is often more involved."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#exception-conjugate-priors",
    "href": "slides/lecture06-1-bayes-workflow.html#exception-conjugate-priors",
    "title": "Bayesian Workflow Example",
    "section": "Exception: Conjugate Priors",
    "text": "Exception: Conjugate Priors\nWhen the mathematical forms of the likelihood and the prior(s) are conjugate, the posterior is a nice closed-form distribution.\nExamples:\n\nNormal \\(p(y | \\mu)\\), Normal \\(p(\\mu)\\) ⇒ Normal \\(p(\\mu | y)\\)\nBinomial \\(p(y | \\theta)\\), Beta \\(p(\\theta)\\), ⇒ Beta \\(p(\\theta | y)\\)\n\nSampling using conjugate priors is called Gibbs sampling."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#when-does-the-prior-matter",
    "href": "slides/lecture06-1-bayes-workflow.html#when-does-the-prior-matter",
    "title": "Bayesian Workflow Example",
    "section": "When Does The Prior Matter?",
    "text": "When Does The Prior Matter?\nIn general, priors matter more for:\n\nLess data (likelihood less informative);\nMore complex models (more degrees of freedom).\n\nAlways justify and test your priors. Explicitly compare the prior to the posterior to see whether your inferences are driven by the prior or by the data (probability model)."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#overview-of-ppls",
    "href": "slides/lecture06-1-bayes-workflow.html#overview-of-ppls",
    "title": "Bayesian Workflow Example",
    "section": "Overview of PPLs",
    "text": "Overview of PPLs\n\nSpeciality “languages” for specifying probability models.\nRely on automatic differentiation to compile likelihood/posterior functions.\nMany frameworks developing over the last few years:\n\nPython: pyMC3\nJulia: Turing.jl\nCross-platform: Stan"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#specifying-extreme-example-with-turing.jl",
    "href": "slides/lecture06-1-bayes-workflow.html#specifying-extreme-example-with-turing.jl",
    "title": "Bayesian Workflow Example",
    "section": "Specifying Extreme Example with Turing.jl",
    "text": "Specifying Extreme Example with Turing.jl\n\n\nCode\nusing Turing\n## y: observed data\n## can also specify covariates or auxiliary data in the function if used\n@model function tide_model(y)\n    # specify priors\n    μ ~ Normal(0, 0.5)\n    σ ~ truncated(Normal(0, 0.1), 0, Inf)\n    # specify likelihood\n    y ~ LogNormal(μ, σ)\n    # returning y allows us (later) to generate predictive simulations\n    return y \nend"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#finding-mle-and-map",
    "href": "slides/lecture06-1-bayes-workflow.html#finding-mle-and-map",
    "title": "Bayesian Workflow Example",
    "section": "Finding MLE and MAP",
    "text": "Finding MLE and MAP\n\n\nCode\nm = tide_model(dat_annmax.residual)\nθ_mle = maximum_likelihood(m)\nθ_map = maximum_a_posteriori(m)\n\n@show θ_mle;\n@show θ_map;\n@show p_map2;\n\n\nθ_mle = [0.25471224255244257, 0.055489643349750276]\nθ_map = [0.2546874075936701, 0.05542213631132044]\np_map2 = [0.2546874075933288, 0.05542213686160764]"
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#more-ppl-tips",
    "href": "slides/lecture06-1-bayes-workflow.html#more-ppl-tips",
    "title": "Bayesian Workflow Example",
    "section": "More PPL Tips",
    "text": "More PPL Tips\n\nParameterization can matter (more when we talk about simulation from posterior than MLE/MAP); read documentation and tips and don’t feel shy about checking Reddit/forums\nSometimes can use external models: easy in Turing, more difficult in Stan and not sure of current status in pyMC3.\nPackages rely on a lot of dependencies which may not be trivial to install."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#more-ppl-tips-1",
    "href": "slides/lecture06-1-bayes-workflow.html#more-ppl-tips-1",
    "title": "Bayesian Workflow Example",
    "section": "More PPL Tips",
    "text": "More PPL Tips\nWhen are PPLs useful?\n\nReadable model code;\nComplex models (hierarchical models, external models with infeasible parameters, etc)\n“Full Bayes” (haven’t discussed yet, but generating samples from the posterior distribution)."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#key-points-bayesian-workflow",
    "href": "slides/lecture06-1-bayes-workflow.html#key-points-bayesian-workflow",
    "title": "Bayesian Workflow Example",
    "section": "Key Points: Bayesian Workflow",
    "text": "Key Points: Bayesian Workflow\n\nUse prior predictive simulations to refine priors.\nPriors matter less when likelihood is highly informative.\nCan use PPLs to specify models without formally writing out likelihoods."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#next-classes",
    "href": "slides/lecture06-1-bayes-workflow.html#next-classes",
    "title": "Bayesian Workflow Example",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Random variate generation and sampling from distributions.\nNext Week: Monte Carlo and the Bootstrap."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#term-project",
    "href": "slides/lecture06-1-bayes-workflow.html#term-project",
    "title": "Bayesian Workflow Example",
    "section": "Term Project",
    "text": "Term Project\n\nCan work in groups of 1-2\nProposal due 3/21.\nMax three pages, should include background and problem statement, data overview, proposed probability models, and research plan.\nDeliverables include presentations (in class, last 2-3 sessions) and written report (during finals week)."
  },
  {
    "objectID": "slides/lecture06-1-bayes-workflow.html#references-scroll-for-full-list",
    "href": "slides/lecture06-1-bayes-workflow.html#references-scroll-for-full-list",
    "title": "Bayesian Workflow Example",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#probability-fundamentals",
    "href": "slides/lecture02-1-nhst.html#probability-fundamentals",
    "title": "Statistics as Decision-Making",
    "section": "Probability Fundamentals",
    "text": "Probability Fundamentals\n\nBayesian vs. Frequentist Interpretations\nDistributions reflect assumptions on probability of data.\nPDFs, CDFs, quantiles\nLikelihood\nCommunicating uncertainty: confidence vs. predictive intervals."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#science-as-decision-making-under-uncertainty",
    "href": "slides/lecture02-1-nhst.html#science-as-decision-making-under-uncertainty",
    "title": "Statistics as Decision-Making",
    "section": "Science as Decision-Making Under Uncertainty",
    "text": "Science as Decision-Making Under Uncertainty\n\n\nGoal is to draw insights:\n\nAbout causes and effects;\nAbout interventions.\n\n\n\n\n\nXKCD 2440\n\n\n\nSource: XKCD 2440"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#data-generation-approximates-reality",
    "href": "slides/lecture02-1-nhst.html#data-generation-approximates-reality",
    "title": "Statistics as Decision-Making",
    "section": "Data Generation Approximates Reality",
    "text": "Data Generation Approximates Reality\n\n\n\n\n\nEstimand Estimator Cake\n\n\n\n\n\n\n\nEstimand Estimator Cake\n\n\n\n\n\n\n\n\nEstimate Cake\n\n\n\n\n\nSource: Richard McElreath\n\n\nGoal is to start with some “true” process, then apply a procedure (experimental/observational + statistical) and recover what is hopefully a good estimate.\nBut lots can go wrong in this process!"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#questions-we-might-like-to-answer",
    "href": "slides/lecture02-1-nhst.html#questions-we-might-like-to-answer",
    "title": "Statistics as Decision-Making",
    "section": "Questions We Might Like To Answer",
    "text": "Questions We Might Like To Answer\n\nAre high water levels influenced by environmental change?\nDoes some environmental condition have an effect on water quality/etc?\nDoes a drug or treatment have some effect?"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#onus-probandi-incumbit-ei-qui-dicit-non-ei-qui-negat",
    "href": "slides/lecture02-1-nhst.html#onus-probandi-incumbit-ei-qui-dicit-non-ei-qui-negat",
    "title": "Statistics as Decision-Making",
    "section": "Onus probandi incumbit ei qui dicit, non ei qui negat",
    "text": "Onus probandi incumbit ei qui dicit, non ei qui negat\n\n\nCore assumption: Burden of proof is on someone claiming an effect (or a similar hypothesis).\n\n\n\n\nNull Hypothesis Meme\n\n\n\n\nThe title of this slide is a reference to the burden of proof is on the person who affirms, not one who denies. Can think of this as similar to Ockham’s razor or someone mantras: we want to propose hypotheses about scientific phenomena and then see if the evidence supports it."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#null-hypothesis-significance-testing",
    "href": "slides/lecture02-1-nhst.html#null-hypothesis-significance-testing",
    "title": "Statistics as Decision-Making",
    "section": "Null Hypothesis Significance Testing",
    "text": "Null Hypothesis Significance Testing\n\n\n\nCheck if the data is consistent with a “null” model;\nIf the data is unlikely from the null model (to some level of significance), this is evidence for the alternative.\nIf the data is consistent with the null, there is no need for an alternative hypothesis.\n\n\n\n\n\nAlternative Hypothesis Meme\n\n\n\n\nFor scientific hypotheses, this has been encoded in the NHST paradigm:\n\nThink of a “null” hypothesis and look for evidence that it is reasonably consistent with the data.\nIf the data can be explained by the null, then we have no clear evidence for the alternative hypothesis.\nIf the data is highly unlikely under the null hypothesis, then that gives us reason to reject the null and favor the alternative."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#from-null-hypothesis-to-null-model",
    "href": "slides/lecture02-1-nhst.html#from-null-hypothesis-to-null-model",
    "title": "Statistics as Decision-Making",
    "section": "From Null Hypothesis to Null Model",
    "text": "From Null Hypothesis to Null Model\n\n\n…the null hypothesis must be exact, that is free of vagueness and ambiguity, because it must supply the basis of the ‘problem of distribution,’ of which the test of significance is the solution.\n\n\n— R. A. Fisher, The Design of Experiments, 1935.\n\n\n\nThe trick is to go from a null scientific hypothesis to a null statistical model, hence Fisher’s comment about the need for a null hypothesis to be “exact”."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#example-high-water-nonstationarity",
    "href": "slides/lecture02-1-nhst.html#example-high-water-nonstationarity",
    "title": "Statistics as Decision-Making",
    "section": "Example: High Water Nonstationarity",
    "text": "Example: High Water Nonstationarity\n\n\nCode\n# load SF tide gauge data\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18,\n    left_margin=5mm, \n    bottom_margin=5mm\n)\n\nn = nrow(dat_annmax)\nlinfit = lm(@formula(residual ~ Year), dat_annmax)\npred = coef(linfit)[1] .+ coef(linfit)[2] * dat_annmax.Year\n\nplot!(p1, dat_annmax.Year, pred, linewidth=3, label=\"Linear Trend\")\n\n\n\n\nFigure 1: Annual maxima surge data from the San Francisco, CA tide gauge.\n\nFor example, consider this annual extreme high water dataset from San Francisco from 1897 through 2022. A linear fit gives us an observed trend of 0.4 mm/yr. Is that meaningful?\nNote that we didn’t do anything yet to justify whether linear regression is a non-stupid thing to do (hint: it’s a stupid thing to do): we’ll talk about this more later."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#the-null-is-the-trend-real",
    "href": "slides/lecture02-1-nhst.html#the-null-is-the-trend-real",
    "title": "Statistics as Decision-Making",
    "section": "The Null: Is The Trend Real?",
    "text": "The Null: Is The Trend Real?\n\\(\\mathcal{H}_0\\) (Null Hypothesis):\n\nThe “trend” is just due to chance, there is no “true” long-term trend in the data.\n\n\n\nStatistically:\n\n\\[y = \\underbrace{b}_{\\text{constant}} + \\underbrace{\\varepsilon}_{\\text{residuals}}, \\qquad \\varepsilon \\underbrace{\\sim}_{\\substack{\\text{distributed} \\\\ {\\text{according to}}}} \\mathcal{N}(0, \\sigma^2) \\]"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#an-alternative-hypothesis",
    "href": "slides/lecture02-1-nhst.html#an-alternative-hypothesis",
    "title": "Statistics as Decision-Making",
    "section": "An Alternative Hypothesis",
    "text": "An Alternative Hypothesis\n\\(\\mathcal{H}\\):\n\nThe trend is likely non-zero in time.\n\n\n\nStatistically:\n\n\\[y = a \\times t + b + \\varepsilon, \\qquad \\varepsilon \\sim Normal(0, \\sigma^2) \\]"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#null-test",
    "href": "slides/lecture02-1-nhst.html#null-test",
    "title": "Statistics as Decision-Making",
    "section": "Null Test",
    "text": "Null Test\nComparing \\(\\mathcal{H}\\) with \\(\\mathcal{H}_0\\):\n\n\\(\\mathcal{H}\\): \\(a \\neq 0\\)\n\\(\\mathcal{H}_0\\): \\(a = 0\\)\n\n\nIn this example, our null is an example of a point-null hypothesis."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#computing-the-test-statistic",
    "href": "slides/lecture02-1-nhst.html#computing-the-test-statistic",
    "title": "Statistics as Decision-Making",
    "section": "Computing the Test Statistic",
    "text": "Computing the Test Statistic\nFor this type of null hypothesis test, our test statistic is the slope of the linear fit. OLS estimate: \\[\\hat{a} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{(x_i - \\bar{x})^2}.\\]\nIdea is that even assuming the null hypothesis, we could obtain many different datasets, some of which will have a non-zero slope by chance."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#sampling-distribution-of-test-statistic",
    "href": "slides/lecture02-1-nhst.html#sampling-distribution-of-test-statistic",
    "title": "Statistics as Decision-Making",
    "section": "Sampling Distribution of Test Statistic",
    "text": "Sampling Distribution of Test Statistic\nThe distribution of all of these slopes is the sampling distribution.\nStandard result (don’t worry if this isn’t familiar to you):\nAssuming the null, the sampling distribution of the slope statistic is given by a t-distribution:\n\\[\\frac{\\hat{a}}{SE_{\\hat{a}}} \\sim t_{n-2}.\\]"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#statistical-significance",
    "href": "slides/lecture02-1-nhst.html#statistical-significance",
    "title": "Statistics as Decision-Making",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nIs the value of the test statistic consistent with the null hypothesis?\n\n\nMore formally, could the test statistic have been reasonably observed from a random sample given the null hypothesis?"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#p-values-quantification-of-surprise",
    "href": "slides/lecture02-1-nhst.html#p-values-quantification-of-surprise",
    "title": "Statistics as Decision-Making",
    "section": "p-Values: Quantification of “Surprise”",
    "text": "p-Values: Quantification of “Surprise”\n\n\nOne-Tailed Test:\n\n\n\n\n\n\n\nFigure 2: Illustration of a p-value\n\n\n\n\n\nTwo-Tailed Test:\n\n\n\n\n\n\n\nFigure 3: Illustration of a two-tailed p-value"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#error-types",
    "href": "slides/lecture02-1-nhst.html#error-types",
    "title": "Statistics as Decision-Making",
    "section": "Error Types",
    "text": "Error Types\n\n\n\n\n\n\n\nNull Hypothesis Is\n\n\n\n\n\n\n\n\nTrue\n\n\nFalse\n\n\n\n\nDecision About Null Hypothesis\n\n\nDon’t reject\n\n\nTrue negative (probability \\(1-\\alpha\\))\n\n\nType II error (probability \\(\\beta\\))\n\n\n\n\nReject\n\n\nType I Error (probability \\(\\alpha\\))\n\n\nTrue positive (probability \\(1-\\beta\\))\n\n\n\n\nThe general testing framework is built around Type I (false positive) and Type II (false negative) errors."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#navigating-type-i-and-ii-errors",
    "href": "slides/lecture02-1-nhst.html#navigating-type-i-and-ii-errors",
    "title": "Statistics as Decision-Making",
    "section": "Navigating Type I and II Errors",
    "text": "Navigating Type I and II Errors\nThe standard null hypothesis significance framework is based on balancing the chance of making Type I (false positive) and Type II (false negative) errors.\nIdea: Set a significance level \\(\\alpha\\) which is an “acceptable” probability of making a Type I error.\nAside: The probability \\(1-\\beta\\) of correctly rejecting \\(H_0\\) is the power.\n\nNote that these are frequentist concepts, not applicable to a single dataset (which give rise to p-values, which are random values).\nIf we only run a single experiment all we can claim is that if we had run a long series of experiments we would have had 100α% false positives had H0 been true and 100β% false negatives had H1 been true provided we got the power calculations right. Note the conditionals."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#p-value-and-significance",
    "href": "slides/lecture02-1-nhst.html#p-value-and-significance",
    "title": "Statistics as Decision-Making",
    "section": "p-Value and Significance",
    "text": "p-Value and Significance\nCommon practice: If the p-value is sufficiently small (below \\(\\alpha\\)), reject the null hypothesis with \\(1-\\alpha\\) confidence, or declare that the alternative hypothesis is statistically significant at the \\(1-\\alpha\\) level.\nThis can mean:\n\n\nThe null hypothesis is not true for that data-generating process;\nThe null hypothesis is true but the data is an outlying sample.\n\n\n\nThis is a strange hybrid of two schools of frequentist statistics, that of Fisher and of Neyman-Pearson. Fisher viewed p-values as weak evidence which was part of an inductive process (even when assuming unbiased sampling and accurate measurement), while the Neyman-Pearson significance framework was based on quantitative specifications of alternative hypotheses with explicitly calculated power."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#what-p-values-are-not",
    "href": "slides/lecture02-1-nhst.html#what-p-values-are-not",
    "title": "Statistics as Decision-Making",
    "section": "What p-Values Are Not",
    "text": "What p-Values Are Not\n\n\n\nProbability that the null hypothesis is true (this is never computed);\nAn indication of the effect size (or the stakes of that effect).\n\n\n\\[ \\underbrace{p(S \\geq \\hat{S}) | \\mathcal{H}_0)}_{\\text{p-value}} \\neq \\underbrace{p(\\mathcal{H}_0 | S \\geq \\hat{S})}_{\\substack{\\text{probability of} \\\\ \\text{null}}}!\\]\n\n\nA p-value is a random variable which depends on the data. It’s evidence which can be collected from a single experiment but can not establish .\nOver repeated experiments, reasoning about validity of the null should have the right properties assuming the experiments are conducted faithfully."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#statistical-significance-scientific-significance",
    "href": "slides/lecture02-1-nhst.html#statistical-significance-scientific-significance",
    "title": "Statistics as Decision-Making",
    "section": "Statistical Significance ≠ Scientific Significance",
    "text": "Statistical Significance ≠ Scientific Significance\n\n\nStatistical significance does not mean anything about whether the alternative hypothesis is:\n\n“true”;\nan accurate reflection of the data-generating process.\n\n\n\n\n\nHypothesis vs. Causal Meme"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#what-is-any-statistical-test-doing",
    "href": "slides/lecture02-1-nhst.html#what-is-any-statistical-test-doing",
    "title": "Statistics as Decision-Making",
    "section": "What is Any Statistical Test Doing?",
    "text": "What is Any Statistical Test Doing?\n\nAssume the null hypothesis \\(\\mathcal{H}_0\\).\nCompute the test statistic \\(\\hat{S}\\) for the sample.\nObtain the sampling distribution of the test statistic \\(S\\) under \\(H_0\\).\nCalculate \\(\\mathbb{P}(S &gt; \\hat{S})\\) (the p-value)."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#non-uniqueness-of-null-models",
    "href": "slides/lecture02-1-nhst.html#non-uniqueness-of-null-models",
    "title": "Statistics as Decision-Making",
    "section": "Non-Uniqueness of “Null” Models",
    "text": "Non-Uniqueness of “Null” Models\n\n\nIs there a trend in the SF tide gauge trend data?\n\nTrend as regression (\\(p\\text{-value} \\approx 0.02\\))\nMann-Kendall test for monotonic trend (\\(p\\text{-value} \\approx 0.5\\))\n\n\n\n\n\nNon-Uniqueness of Null Models\n\n\n\nSource: McElreath (2020, fig. 1.2)"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#multiple-comparisons",
    "href": "slides/lecture02-1-nhst.html#multiple-comparisons",
    "title": "Statistics as Decision-Making",
    "section": "Multiple Comparisons",
    "text": "Multiple Comparisons\n\n\nIf you conduct multiple statistical tests, you must account for all of these in the p-value computation and assessment of significance.\nImportant: This includes model selection!\n\n\n\n\nMultiple Comparisons Meme\n\n\n\n\nThe core issue is the standard test statistics have the right Type I/Type II properties for each individual test, but multiple tests distort these frequencies, sometimes quite dramatically.\nFor example, suppose each individual test has a 5% Type I error rate. If you test 100 different models, and the errors are independent, you would expect 5 false positives, one of which would be selected by minimizing the p-value. The probability of at least one type I error is &gt;99%.\nThere are a number of corrections (Bonferroni being the most common), but sometimes stepwise tests are subtle, including cases of model selection followed by model fitting. You can also use simulation to estimate the false-positive rate for the procedure under a null data-generating process, which ties into the broader methods we’ll discuss."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#results-are-flashy-but-meaningless-without-methods",
    "href": "slides/lecture02-1-nhst.html#results-are-flashy-but-meaningless-without-methods",
    "title": "Statistics as Decision-Making",
    "section": "Results Are Flashy, But Meaningless Without Methods",
    "text": "Results Are Flashy, But Meaningless Without Methods\n\nElton John Results Section Meme\nSource: Richard McElreath"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#note-this-does-not-mean-null-hypothesis-testing-is-useless",
    "href": "slides/lecture02-1-nhst.html#note-this-does-not-mean-null-hypothesis-testing-is-useless",
    "title": "Statistics as Decision-Making",
    "section": "Note: This Does Not Mean Null Hypothesis Testing Is Useless!",
    "text": "Note: This Does Not Mean Null Hypothesis Testing Is Useless!\n\n\nExamining and testing the implications of competing models is important, including “null” models!\n\n\n\n\nNull Hypothesis Selection Good Vs. Bad\n\n\n\n\nDeborah Mayo discusses this interpretation as “severe testing”: apply different levels of scrutiny to a scientific model to see what level of severity breaks the model.\nThe idea of a p-value (not necessarily “significance”) being a piece of inductive evidence rather than a threshold for validity (especially for an individual experiment) reflects the original views of Fisher."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#what-might-be-more-satisfying",
    "href": "slides/lecture02-1-nhst.html#what-might-be-more-satisfying",
    "title": "Statistics as Decision-Making",
    "section": "What Might Be More Satisfying?",
    "text": "What Might Be More Satisfying?\n\nConsideration of multiple plausible (possibly more nuanced) hypotheses.\nAssessment/quantification of evidence consistent with different hypotheses.\nIdentification of opportunities to design experiments/learn.\nInsight into the effect size."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#hypothesis-testing",
    "href": "slides/lecture02-1-nhst.html#hypothesis-testing",
    "title": "Statistics as Decision-Making",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nClassical framework: Compare a null hypothesis (no effect) to an alternative (some effect)\n\\(p\\)-value: probability (under \\(H_0\\)) of more extreme test statistic than observed.\n“Significant” if \\(p\\)-value is below a significance level reflecting acceptable Type I error rate."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#problems-with-nhst-framework",
    "href": "slides/lecture02-1-nhst.html#problems-with-nhst-framework",
    "title": "Statistics as Decision-Making",
    "section": "Problems with NHST framework",
    "text": "Problems with NHST framework\n\nReal “null” hypotheses are often more nuanced than in typical tests (which were often developed for controlled experiments or for computational convenience).\nDecisions are often not binary (“significant/not significant”).\n\\(p\\)-values are often over-interpreted and are often be incorrectly calculated, with negative outcomes!\nImportant: “Big” data can make things worse, as NHST is highly sensitive to small but evidence effects."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#next-classes",
    "href": "slides/lecture02-1-nhst.html#next-classes",
    "title": "Statistics as Decision-Making",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Probability Models and Exploratory (Graphical) Analysis\nFriday: Multiple Linear Regression."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#assessments",
    "href": "slides/lecture02-1-nhst.html#assessments",
    "title": "Statistics as Decision-Making",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 available; due next Friday (2/6)."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#references-scroll-for-full-list",
    "href": "slides/lecture02-1-nhst.html#references-scroll-for-full-list",
    "title": "Statistics as Decision-Making",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nMcElreath, R. (2020). Statistical rethinking : A bayesian course with examples in R and Stan (Second). Boca Raton, Florida: CRC. Retrieved from https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#random-variable-generation",
    "href": "slides/lecture07-1-monte-carlo.html#random-variable-generation",
    "title": "Monte Carlo",
    "section": "Random Variable Generation",
    "text": "Random Variable Generation\n\nQuantile transform method turns uniforms into known distributions.\nRejection sampling can be used when quantiles are difficult to compute (but requires proposal covering target distribution).\nPseudorandom number generation and importance of seeds."
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#stochastic-simulation",
    "href": "slides/lecture07-1-monte-carlo.html#stochastic-simulation",
    "title": "Monte Carlo",
    "section": "Stochastic Simulation",
    "text": "Stochastic Simulation\nGoal: Estimate \\(\\mathbb{E}_p\\left[h(x)\\right]\\), \\(x \\sim f(x)\\)\nMonte Carlo principle:\n\nSample \\(x^1, x^2, \\ldots, x^N \\sim f(x)\\)\nEstimate \\(\\mathbb{E}_p\\left[h(x)\\right] \\approx \\sum_{n=1}^N h(x^n)\\) / N\n\nIn other words: replace calculus with data summaries!"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-process-schematic",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-process-schematic",
    "title": "Monte Carlo",
    "section": "Monte Carlo Process Schematic",
    "text": "Monte Carlo Process Schematic\n\n\n\n\n\n\n\nG\n\n\n\na\n\nProbability\n Distribution\n\n\n\nb\n\nRandom\n Samples\n\n\n\na-&gt;b\n\n\nSample\n\n\n\nc\n\nModel\n\n\n\nb-&gt;c\n\n\nInput\n\n\n\nd\n\nOutputs\n\n\n\nc-&gt;d\n\n\nSimulate"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#goals-of-monte-carlo",
    "href": "slides/lecture07-1-monte-carlo.html#goals-of-monte-carlo",
    "title": "Monte Carlo",
    "section": "Goals of Monte Carlo",
    "text": "Goals of Monte Carlo\nMonte Carlo is a broad method, which can be used to:\n\nObtain probability distributions of outputs;\nEstimate deterministic quantities (Monte Carlo estimation)."
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#mc-example-finding-pi",
    "href": "slides/lecture07-1-monte-carlo.html#mc-example-finding-pi",
    "title": "Monte Carlo",
    "section": "MC Example: Finding \\(\\pi\\)",
    "text": "MC Example: Finding \\(\\pi\\)\nHow can we use MC to estimate \\(\\pi\\)?\nHint: Think of \\(\\pi\\) as an expected value…"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#mc-example-finding-pi-1",
    "href": "slides/lecture07-1-monte-carlo.html#mc-example-finding-pi-1",
    "title": "Monte Carlo",
    "section": "MC Example: Finding \\(\\pi\\)",
    "text": "MC Example: Finding \\(\\pi\\)\n\n\n\nFinding \\(\\pi\\) by sampling random values from the unit square and computing the fraction in the unit circle. This is an example of Monte Carlo integration.\n\\[\\frac{\\text{Area of Circle}}{\\text{Area of Square}} = \\frac{\\pi}{4}\\]\n\n\n\n\n\nCode\nLogging.disable_logging(Logging.Info)\n\nfunction circleShape(r)\n    θ = LinRange(0, 2 * π, 500)\n    r * sin.(θ), r * cos.(θ)\nend\n\nnsamp = 3000\nunif = Uniform(-1, 1)\nx = rand(unif, (nsamp, 2))\nl = mapslices(v -&gt; sum(v.^2), x, dims=2)\nin_circ = l .&lt; 1\npi_est = [4 * mean(in_circ[1:i]) for i in 1:nsamp]\n\nplt1 = plot(\n    1,\n    xlim = (-1, 1),\n    ylim = (-1, 1),\n    legend = false,\n    markersize = 4,\n    framestyle = :origin,\n    tickfontsize=16,\n    grid=:false\n    )\nplt2 = plot(\n    1,\n    xlim = (1, nsamp),\n    ylim = (3, 3.5),\n    legend = :false,\n    linewidth=3, \n    color=:black,\n    tickfontsize=16,\n    guidefontsize=16,\n    xlabel=\"Iteration\",\n    ylabel=\"Estimate\",\n    right_margin=5mm\n)\nhline!(plt2, [π], color=:red, linestyle=:dash)\nplt = plot(plt1, plt2, layout=Plots.grid(2, 1, heights=[2/3, 1/3]), size=(600, 500))\n\nplot!(plt, circleShape(1), linecolor=:blue, lw=1, aspectratio=1, subplot=1)\n\n\nmc_anim = @animate for i = 1:nsamp\n    if l[i] &lt; 1\n        scatter!(plt[1], Tuple(x[i, :]), color=:blue, markershape=:x, subplot=1)\n    else\n        scatter!(plt[1], Tuple(x[i, :]), color=:red, markershape=:x, subplot=1)\n    end\n    push!(plt, 2, i, pi_est[i])\nend every 100\n\ngif(mc_anim, \"figures/mc_pi.gif\", fps=3)\n\n\n\n\n\n\n\nFigure 1: MCMC Estimation of pi"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#mc-example-dice",
    "href": "slides/lecture07-1-monte-carlo.html#mc-example-dice",
    "title": "Monte Carlo",
    "section": "MC Example: Dice",
    "text": "MC Example: Dice\n\n\nWhat is the probability of rolling 4 dice for a total of 19?\n\nCan simulate dice rolls and find the frequency of 19s among the samples.\n\n\n\n\n\nCode\nfunction dice_roll_repeated(n_trials, n_dice)\n    dice_dist = DiscreteUniform(1, 6) \n    roll_results = zeros(n_trials)\n    for i=1:n_trials\n        roll_results[i] = sum(rand(dice_dist, n_dice))\n    end\n    return roll_results\nend\n\nnsamp = 10000\n# roll four dice 10000 times\nrolls = dice_roll_repeated(nsamp, 4) \n\n# calculate probability of 19\nsum(rolls .== 19) / length(rolls)\n\n# initialize storage for frequencies by sample length\navg_freq = zeros(length(rolls)) \nstd_freq = zeros(length(rolls)) \n\n# compute average frequencies of 19\navg_freq[1] = (rolls[1] == 19)\ncount = 1\nfor i=2:length(rolls)\n    avg_freq[i] = (avg_freq[i-1] * (i-1) + (rolls[i] == 19)) / i\n    std_freq[i] = 1/sqrt(i-1) * std(rolls[1:i] .== 19)\nend\n\nplt = plot(\n    1,\n    xlim = (1, nsamp),\n    ylim = (0, 0.1),\n    legend = :false,\n    tickfontsize=16,\n    guidefontsize=16,\n    xlabel=\"Iteration\",\n    ylabel=\"Estimate\",\n    right_margin=8mm,\n    color=:black,\n    linewidth=3,\n    size=(600, 400)\n)\nhline!(plt, [0.0432], color=\"red\", \n    linestyle=:dash) \n\nmc_anim = @animate for i = 1:nsamp\n    push!(plt, 1, i, avg_freq[i])\nend every 100\n\ngif(mc_anim, \"figures/mc_dice.gif\", fps=10)\n\n\n\n\n\n\n\nFigure 2: MCMC Estimation of pi"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-and-uncertainty-propagation",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-and-uncertainty-propagation",
    "title": "Monte Carlo",
    "section": "Monte Carlo and Uncertainty Propagation",
    "text": "Monte Carlo and Uncertainty Propagation\nMonte Carlo simulation: propagate uncertainties from inputs through a model to outputs.\nThis is an example of uncertainty propagation: draw samples from some distribution, and run them through one or more models to find the (conditional) probability of outcomes of interest (for good or bad)."
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-formal-approach",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-formal-approach",
    "title": "Monte Carlo",
    "section": "Monte Carlo: Formal Approach",
    "text": "Monte Carlo: Formal Approach\nFormally: Monte Carlo estimation as the computation of the expected value of a random quantity \\(Y = f(X)\\), \\(\\mu = \\mathbb{E}[Y]\\).\nTo do this, generate \\(n\\) independent and identically distributed values \\(Y_1, \\ldots, Y_n\\). Then the sample estimate is\n\\[\\tilde{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#what-makes-a-good-statistical-estimator",
    "href": "slides/lecture07-1-monte-carlo.html#what-makes-a-good-statistical-estimator",
    "title": "Monte Carlo",
    "section": "What Makes a Good Statistical Estimator?",
    "text": "What Makes a Good Statistical Estimator?\nStatistical estimators are random, which means we can’t ever guarantee that we get back the “true” value\n\nNo bias (\\(\\text{Bias} = \\mathbb{E}_g[\\tilde{\\mu}] - \\mu\\))\nWell-characterized, ideally small, variance (\\(\\text{Var}(\\tilde{\\mu})\\))"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#the-law-of-large-numbers",
    "href": "slides/lecture07-1-monte-carlo.html#the-law-of-large-numbers",
    "title": "Monte Carlo",
    "section": "The Law of Large Numbers",
    "text": "The Law of Large Numbers\nIf\n\n\\(Y\\) is a random variable and its expectation exists and\n\\(Y_1, \\ldots, Y_n\\) are independently and identically distributed\n\nThen by the weak law of large numbers:\n\\[\\lim_{n \\to \\infty} \\mathbb{P}\\left(\\left|\\tilde{\\mu}_n - \\mu\\right| \\leq \\varepsilon \\right) = 1\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#the-law-of-large-numbers-1",
    "href": "slides/lecture07-1-monte-carlo.html#the-law-of-large-numbers-1",
    "title": "Monte Carlo",
    "section": "The Law of Large Numbers",
    "text": "The Law of Large Numbers\nIn other words, eventually Monte Carlo estimates will get within an arbitrary error of the true expectation.\nBut how large is large enough?"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-sample-mean",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-sample-mean",
    "title": "Monte Carlo",
    "section": "Monte Carlo Sample Mean",
    "text": "Monte Carlo Sample Mean\nThe sample mean \\(\\tilde{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i\\) is itself a random variable.\n\nWith some assumptions (the mean of \\(Y\\) exists and \\(Y\\) has finite variance), the expected Monte Carlo sample mean \\(\\mathbb{E}[\\tilde{\\mu}_n]\\) is\n\\[\\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}[Y_i] = \\frac{1}{n} n \\mu = \\mu\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-error",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-error",
    "title": "Monte Carlo",
    "section": "Monte Carlo Error",
    "text": "Monte Carlo Error\nWe’d like to know more about the error of this estimate for a given sample size. The variance of this estimator is\n\\[\\tilde{\\sigma}_n^2 = \\text{Var}\\left(\\tilde{\\mu}_n\\right) = \\mathbb{E}\\left((\\tilde{\\mu}_n - \\mu)^2\\right) = \\frac{\\sigma_Y^2}{n}\\]\n\nSo as \\(n\\) increases, the standard error decreases:\n\\[\\tilde{\\sigma}_n = \\frac{\\sigma_Y}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-error-1",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-error-1",
    "title": "Monte Carlo",
    "section": "Monte Carlo Error",
    "text": "Monte Carlo Error\nIn other words, if we want to decrease the Monte Carlo error by 10x, we need 100x additional samples. This is not an ideal method for high levels of accuracy.\n\n\n\nMonte Carlo is an extremely bad method. It should only be used when all alternative methods are worse.\n\n\n— Sokal, Monte Carlo Methods in Statistical Mechanics, 1996\n\n\n\n\nBut…often most alternatives are worse!"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#when-might-we-want-to-use-monte-carlo",
    "href": "slides/lecture07-1-monte-carlo.html#when-might-we-want-to-use-monte-carlo",
    "title": "Monte Carlo",
    "section": "When Might We Want to Use Monte Carlo?",
    "text": "When Might We Want to Use Monte Carlo?\nIf you can compute your integrals analytically or through quadrature, you probably should.\nBut for many “real” problems, this is either\n\nNot possible (or computationally intractable);\nRequires a lot of stylization and simplification."
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-confidence-intervals",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-confidence-intervals",
    "title": "Monte Carlo",
    "section": "Monte Carlo Confidence Intervals",
    "text": "Monte Carlo Confidence Intervals\nBasic Idea: The Central Limit Theorem says that with enough samples, the errors are normally distributed:\n\\[\\left\\|\\tilde{\\mu}_n - \\mu\\right\\| \\to \\mathcal{N}\\left(0, \\frac{\\sigma_Y^2}{n}\\right)\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-confidence-intervals-1",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-confidence-intervals-1",
    "title": "Monte Carlo",
    "section": "Monte Carlo Confidence Intervals",
    "text": "Monte Carlo Confidence Intervals\nThe \\(\\alpha\\)-confidence interval is: \\[\\tilde{\\mu}_n \\pm \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right) \\frac{\\sigma_Y}{\\sqrt{n}}\\]\nFor example, the 95% confidence interval is \\[\\tilde{\\mu}_n \\pm 1.96 \\frac{\\sigma_Y}{\\sqrt{n}}.\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#sidebar-estimating-sigma_y",
    "href": "slides/lecture07-1-monte-carlo.html#sidebar-estimating-sigma_y",
    "title": "Monte Carlo",
    "section": "Sidebar: Estimating \\(\\sigma_Y\\)",
    "text": "Sidebar: Estimating \\(\\sigma_Y\\)\nWe don’t know the standard deviation \\(\\sigma_Y\\).\nBut we can estimate it using the simulation standard deviation:"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#implications-of-monte-carlo-error",
    "href": "slides/lecture07-1-monte-carlo.html#implications-of-monte-carlo-error",
    "title": "Monte Carlo",
    "section": "Implications of Monte Carlo Error",
    "text": "Implications of Monte Carlo Error\nConverging at a rate of \\(1/\\sqrt{n}\\) is not great. But:\n\nAll models are wrong, and so there always exists some irreducible model error.\nWe often need a lot of simulations. Do we have enough computational power?"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#airshed-model",
    "href": "slides/lecture07-1-monte-carlo.html#airshed-model",
    "title": "Monte Carlo",
    "section": "Airshed Model",
    "text": "Airshed Model\n\n\n\n\n\n\n\n\nFigure 3: Illustration of the airshed, including notation.\n\n\n\n\nGoal: Find the probability of exceeding the 1-hour SO2 average exposure concentration standard, which is 0.14 ppm.\n\n\n\\[\\mathbb{P}[\\text{SO}_2(\\theta) &gt; 0.14] = \\int \\mathbb{I}(\\text{SO}_2(\\theta) &gt; 0.14) p(\\theta) d\\theta\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#airshed-model-1",
    "href": "slides/lecture07-1-monte-carlo.html#airshed-model-1",
    "title": "Monte Carlo",
    "section": "Airshed Model",
    "text": "Airshed Model\n\n\nFigure 4: Illustration of the airshed, including notation.\n\\[\\frac{dC}{dt} = \\frac{u}{L} C_\\text{in} + \\frac{S-D}{WHL} - \\left(\\frac{u}{L} + k\\right)C\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#forward-euler-discretization",
    "href": "slides/lecture07-1-monte-carlo.html#forward-euler-discretization",
    "title": "Monte Carlo",
    "section": "Forward Euler Discretization",
    "text": "Forward Euler Discretization\n\\[\n\\frac{dC}{dt} = \\frac{u}{L} C_\\text{in}(t) + \\frac{S-D}{WHL} - \\left(\\frac{u}{L} + k\\right)C\\]\n\n\\[\\Rightarrow \\frac{C(t+1) - C(t)}{\\Delta t} = \\frac{u}{L} C_\\text{in}(t) + \\frac{R}{WHL} - \\left(\\frac{u}{L} + k\\right)C(t)\\]\n\n\n\\[\\bbox[yellow, 10px, border:5px solid red]{C(t+1) = \\left(1 - \\Delta t\\left(\\frac{u}{L} + k\\right)\\right)C(t) + \\Delta t \\left(\\frac{u}{L} C_\\text{in}(t) + \\frac{R}{WHL}\\right)}\n\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-samples",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-samples",
    "title": "Monte Carlo",
    "section": "Monte Carlo Samples",
    "text": "Monte Carlo Samples\n\nCode\nnsamp = 1000\nu = rand(LogNormal(log(2), 1), nsamp)\nCin = rand(LogNormal(log(0.16), 0.12), nsamp)\nR = rand(Normal(0.5, 0.5), nsamp)\n\np1 = histogram(u, ylabel=\"count\", xlabel=L\"$u$ (m/s)\", label=false, tickfontsize=16, guidefontsize=18, size=(400, 450))\np2 = histogram(Cin, ylabel=\"count\", xlabel=L\"$C_{in}$ (ppm)\", label=false, tickfontsize=16, guidefontsize=18, size=(400, 450))\np3 = histogram(R, ylabel=\"count\", xlabel=L\"$R$ (ppm/hr)\", label=false, tickfontsize=16, guidefontsize=18, size=(400, 450))\ndisplay(p1)\ndisplay(p2)\ndisplay(p3)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Monte Carlo samples for the airshed model.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#simulation-results",
    "href": "slides/lecture07-1-monte-carlo.html#simulation-results",
    "title": "Monte Carlo",
    "section": "Simulation Results",
    "text": "Simulation Results\n\nCode\n# other parameters\nC₀ = 0.07\nT = 60\nk = 0.3\nW = 4\nH = 5\nL = 4\n# conduct simulation\nP = u / L .* Cin\nl = u / L .+ k\nC2 = zeros(T*100 + 1, nsamp)\nS = 0:0.01:T\nfor (i, t) in pairs(S)\n    if i == 1\n        C2[i, :] .= C₀\n    else\n        C2[i, :] = (1 .- 0.01*l) .* C2[i-1, :] .+ 0.01 * P .+ 0.01 * R / (H * W * L)\n    end\nend\nmean_SO2 = map(mean, eachcol(C2)) # calculate means\n# plot histogram\np1 = histogram(mean_SO2, xlabel=\"1-Hour Average Exposure (ppm)\", ylabel=\"Count\", legend=false, tickfontsize=16, guidefontsize=18)\nvline!(p1, [0.14], color=:red, linestyle=:dash, linewidth=3)\nxticks!(p1, 0:0.04:0.3)\nxaxis!(p1, xminorticks=2)\nplot!(p1, size=(600, 450))\n# plot cdf\np2 = plot(sort(mean_SO2), (1:nsamp) ./ nsamp, xlabel=\"1-Hour Average Exposure (ppm)\", ylabel=\"Cumulative Probability\", legend=false, tickfontsize=17, guidefontsize=18, linewidth=3)\nvline!(p2, [0.14], linestyle=:dash, color=:red, linewidth=3, minorgrid=true)\nxticks!(p2, 0:0.04:0.3)\nxaxis!(p2, xminorticks=2)\nyaxis!(p2, yminorticks=5)\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Monte Carlo samples for the airshed model.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 6"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-estimation",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-estimation",
    "title": "Monte Carlo",
    "section": "Monte Carlo Estimation",
    "text": "Monte Carlo Estimation\n\n\n\\[\\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{I}[x_i &gt; 0.14]\\]\n\\[\\hat{\\sigma}_n = \\sqrt{\\frac{\\text{Var}(\\mathbb{I}[x_{1:n} &gt; 0.14])}{n}}\\]\n\n\n\nCode\n# show Monte Carlo estimate stabilization\navg_mc_out = zeros(nsamp)\navg_mc_out[1] = mean_SO2[1] &gt; 0.14\nstd_mc_out = zeros(nsamp)\nstd_mc_out[1] = 0\nfor i = 2:nsamp\n    avg_mc_out[i] = (avg_mc_out[i-1] * (i-1) + (mean_SO2[i] &gt; 0.14)) / i\n    std_mc_out[i] = 1/sqrt(i) * std(mean_SO2[1:i] .&gt; 0.14)\nend\np = plot(avg_mc_out, xlabel=\"Monte Carlo Iteration\", ylabel=\"Probability\", left_margin=3mm, legend=:false, ribbon=1.96*std_mc_out, fillalpha=0.3, linewidth=2, tickfontsize=16, guidefontsize=18, fillcolor=:red, right_margin=5mm, minorgrid=true)\nylims!(p, (0, 0.3))\nyaxis!(p, yminorticks=5)\nplot!(p, size=(600, 450))\ndisplay(p)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Monte Carlo estimation for the airshed model."
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-optimization",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-optimization",
    "title": "Monte Carlo",
    "section": "Monte Carlo Optimization",
    "text": "Monte Carlo Optimization\nCan also use Monte Carlo to estimate expected values for optimization problems\nFor example: in previous problem, might try to optimize a control strategy with an objective of minimizing violations. But no closed form representation of the distribution, so use MC."
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#mc-estimate-of-the-cdf",
    "href": "slides/lecture07-1-monte-carlo.html#mc-estimate-of-the-cdf",
    "title": "Monte Carlo",
    "section": "MC Estimate of the CDF",
    "text": "MC Estimate of the CDF\nWould like to estimate the CDF \\(F\\) with some approximation \\(\\hat{F}_n\\), then compute \\(\\hat{z}^\\alpha_n = \\hat{F}_n^{-1}(\\alpha)\\) as an estimator of the \\(\\alpha\\)-quantile \\(z^\\alpha\\).\nGiven samples \\(\\hat{\\mathbf{y}} = y_1, \\ldots, y_n \\sim F\\), define \\[\\hat{F}_(y) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(y_i \\leq y).\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#is-this-an-unbiased-estimator",
    "href": "slides/lecture07-1-monte-carlo.html#is-this-an-unbiased-estimator",
    "title": "Monte Carlo",
    "section": "Is This An Unbiased Estimator?",
    "text": "Is This An Unbiased Estimator?\n\\[\n\\begin{align*}\n\\mathbb{E}[\\hat{F}_(y)] &= \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(y_i \\leq y) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\mathbb{P}(y_i \\leq y) \\\\\n&= F(y)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#monte-carlo-quantile-estimation-error",
    "href": "slides/lecture07-1-monte-carlo.html#monte-carlo-quantile-estimation-error",
    "title": "Monte Carlo",
    "section": "Monte Carlo Quantile Estimation Error",
    "text": "Monte Carlo Quantile Estimation Error\nFrom the CLT and some (not super important) theory about order statistics: \\[\\text{Var}(\\hat{z}^\\alpha_n) \\to \\frac{\\sigma^2_y}{n}\\frac{\\alpha (1 - \\alpha)}{f^2(z^\\alpha)}\\]\nIn other words, the smaller the density at the “true” quantile \\(z^\\alpha\\), the greater the error."
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#what-if-we-cant-sample-from-the-distribution",
    "href": "slides/lecture07-1-monte-carlo.html#what-if-we-cant-sample-from-the-distribution",
    "title": "Monte Carlo",
    "section": "What If We Can’t Sample From The Distribution?",
    "text": "What If We Can’t Sample From The Distribution?\n\nMay not be able to generate samples \\(X \\sim f(x)\\) efficiently\nThink of sampling from tails:\n\n\\[P(X &gt; k) \\approx \\frac{1}{M} \\sum_{i=1}^M \\mathbb{I}(X_i &gt; k)\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#importance-sampling",
    "href": "slides/lecture07-1-monte-carlo.html#importance-sampling",
    "title": "Monte Carlo",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nExtension of rejection sampling without requiring “rejection”:\n\nDraw samples from importance distribution g(x);\nReweight samples: \\[\n\\begin{align*}\n\\mathbb{E}_f[h(x)] &= \\int_x \\frac{f(x)}{g(x)} g(x)h(x) dx \\\\\n&\\approx \\frac{1}{M} \\sum_{i=1}^M \\frac{f(x)}{g(x)}h(x)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#importance-sampling-needs",
    "href": "slides/lecture07-1-monte-carlo.html#importance-sampling-needs",
    "title": "Monte Carlo",
    "section": "Importance Sampling Needs",
    "text": "Importance Sampling Needs\nTechnically works with any proposal \\(g\\), but more efficient if \\(g\\) “covers” \\(f\\) (like with rejection sampling):\n\\[f(x)/g(x) &lt; M  &lt; \\infty\\]"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#antithetic-variates",
    "href": "slides/lecture07-1-monte-carlo.html#antithetic-variates",
    "title": "Monte Carlo",
    "section": "Antithetic Variates",
    "text": "Antithetic Variates\nIf target values of pairs of samples \\(h(X_i\\)) and \\(h(Y_i)\\) are negatively correlated, can increase rate of convergence of \\[\\frac{1}{2M} \\sum_{i=1}^M [h(X_i) + h(Y_i)]\\] relative to \\(\\frac{1}{2M} h(X_i)\\) alone."
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#antithetic-variate-generation-can-be-difficult-in-practice",
    "href": "slides/lecture07-1-monte-carlo.html#antithetic-variate-generation-can-be-difficult-in-practice",
    "title": "Monte Carlo",
    "section": "Antithetic Variate Generation Can Be Difficult in Practice",
    "text": "Antithetic Variate Generation Can Be Difficult in Practice\n\nEnsuring anti-correlation can be difficult to verify in general;\nGains in efficiency are dependent on effectiveness of antithetical variate generation and shape of \\(h(x)\\)"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#key-points-1",
    "href": "slides/lecture07-1-monte-carlo.html#key-points-1",
    "title": "Monte Carlo",
    "section": "Key Points",
    "text": "Key Points\n\nMonte Carlo: stochastic simulation instead of integration to estimate expected values\nMonte Carlo is an unbiased estimator; confidence intervals given by CLT.\nBe mindful of Monte Carlo standard error for “naive” MC with iid samples.\nAdvanced: Variance reduction techniques to improve convergence, see e.g. https://artowen.su.domains/mc/Ch-var-basic.pdf."
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#perhaps-most-importantly",
    "href": "slides/lecture07-1-monte-carlo.html#perhaps-most-importantly",
    "title": "Monte Carlo",
    "section": "Perhaps Most Importantly…",
    "text": "Perhaps Most Importantly…\nAlways report Monte Carlo error!"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#next-classes",
    "href": "slides/lecture07-1-monte-carlo.html#next-classes",
    "title": "Monte Carlo",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: The Bootstrap"
  },
  {
    "objectID": "slides/lecture07-1-monte-carlo.html#references-scroll-for-full-list",
    "href": "slides/lecture07-1-monte-carlo.html#references-scroll-for-full-list",
    "title": "Monte Carlo",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#probability-fundamentals",
    "href": "slides/lecture02-1-eda.html#probability-fundamentals",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Probability Fundamentals",
    "text": "Probability Fundamentals\n\nBayesian vs. Frequentist Interpretations\nDistributions reflect assumptions on probability of data.\nNormal distributions: “least informative” distribution for a given mean/variance.\nFit distributions by maximizing likelihood.\nCommunicating uncertainty: confidence vs. predictive intervals."
  },
  {
    "objectID": "slides/lecture02-1-eda.html#you-can-always-fit-models",
    "href": "slides/lecture02-1-eda.html#you-can-always-fit-models",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "You Can Always Fit Models…",
    "text": "You Can Always Fit Models…\n\n\nBut not all models are theoretically justifiable.\n\n\n\n\nIan Malcolm meme"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#some-implications-of-mis-specification",
    "href": "slides/lecture02-1-eda.html#some-implications-of-mis-specification",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Some Implications of Mis-Specification",
    "text": "Some Implications of Mis-Specification\n\nBiased estimates (expected value from the model does not match “true” expected value);\nIncorrect inferences or inappropriate understandings of relationships/causality.\nOver/under-confident risk assessments."
  },
  {
    "objectID": "slides/lecture02-1-eda.html#data-generation-approximates-reality",
    "href": "slides/lecture02-1-eda.html#data-generation-approximates-reality",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Data Generation Approximates Reality",
    "text": "Data Generation Approximates Reality\n\n\n\n\n\nEstimand Estimator Cake\n\n\n\n\n\n\n\nEstimand Estimator Cake\n\n\n\n\n\n\n\n\nEstimate Cake\n\n\n\n\n\nSource: Richard McElreath\n\n\nGoal is to start with some “true” process, then apply a procedure (experimental/observational + statistical) and recover what is hopefully a good estimate.\nBut lots can go wrong in this process!"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#how-do-we-choose-what-to-model",
    "href": "slides/lecture02-1-eda.html#how-do-we-choose-what-to-model",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "How Do We Choose What To Model?",
    "text": "How Do We Choose What To Model?\n\nXKCD 2620\nSource: XKCD 2620"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#how-do-we-choose-what-to-model-1",
    "href": "slides/lecture02-1-eda.html#how-do-we-choose-what-to-model-1",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "How Do We Choose What To Model?",
    "text": "How Do We Choose What To Model?\nDeveloping suitable models often starts with both exploratory data analysis (EDA) and theoretical reasoning.\n\nEDA: examining patterns/relationships with visual (plots, clustering) or quantitative (correlations).\nTheory: Avoids “data dredging” or “mining,” which can find spurious correlations or patterns which are misleading without broader context about data-generating mechanisms."
  },
  {
    "objectID": "slides/lecture02-1-eda.html#exploratory-data-analysis-1",
    "href": "slides/lecture02-1-eda.html#exploratory-data-analysis-1",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nWill see many examples of this throughout the semester.\nTry to impose as few assumptions as possible.\nGoal of exploratory analysis is to get a high-level view of the data and formulate hypotheses/check if data is fit for purpose."
  },
  {
    "objectID": "slides/lecture02-1-eda.html#eda-questions",
    "href": "slides/lecture02-1-eda.html#eda-questions",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "EDA Questions",
    "text": "EDA Questions\nEDA is about generating questions and identifying if the data quality is sufficient to address them.\nCommon questions to guide EDA:\n\nWhat type of variability occurs within my variables?\nWhat type of covariation occurs between my variables?"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#common-eda-questions",
    "href": "slides/lecture02-1-eda.html#common-eda-questions",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Common EDA Questions",
    "text": "Common EDA Questions\n\nWhat is the range of the data? What are “typical” values?\nWhat values are rare? Do they make sense?\nAre there missing data or strange outliers? What might explain them?\nIs a particular relationship or distribution supported by the data?"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#common-eda-approaches",
    "href": "slides/lecture02-1-eda.html#common-eda-approaches",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Common EDA Approaches",
    "text": "Common EDA Approaches\n\nData summaries (quantiles, mean/median, max/min, etc.)\nCorrelations\nPlot data (from multiple perspectives)\nClustering (do you need multiple models?)"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#example-ny-air-quality-dataset",
    "href": "slides/lecture02-1-eda.html#example-ny-air-quality-dataset",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Example: NY Air Quality Dataset",
    "text": "Example: NY Air Quality Dataset\n\n\nCode\naq = DataFrame(CSV.File(\"data/airquality/airquality.csv\"))\nrename!(aq, :\"Solar.R\" =&gt; :Solar) # rename solar radiation column to get rid of period\naq[1:5, :]\n\n\n5×7 DataFrame\n\n\n\nRow\nrownames\nOzone\nSolar\nWind\nTemp\nMonth\nDay\n\n\n\nInt64\nInt64?\nInt64?\nFloat64\nInt64\nInt64\nInt64\n\n\n\n\n1\n1\n41\n190\n7.4\n67\n5\n1\n\n\n2\n2\n36\n118\n8.0\n72\n5\n2\n\n\n3\n3\n12\n149\n12.6\n74\n5\n3\n\n\n4\n4\n18\n313\n11.5\n62\n5\n4\n\n\n5\n5\nmissing\nmissing\n14.3\n56\n5\n5"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#quantitative-eda-summaries",
    "href": "slides/lecture02-1-eda.html#quantitative-eda-summaries",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Quantitative EDA: Summaries",
    "text": "Quantitative EDA: Summaries\n\n\nCode\naq_stack = stack(aq, 2:5)\naq_gp = @groupby(aq_stack, :variable)\n@combine(aq_gp, $AsTable = (\n        min=minimum(skipmissing(:value)), \n        median=median(skipmissing(:value)),\n        mean=mean(skipmissing(:value)),\n        max=maximum(skipmissing(:value)),\n        missings=sum(ismissing.(:value))\n    )\n)\n\n\n4×6 DataFrame\n\n\n\nRow\nvariable\nmin\nmedian\nmean\nmax\nmissings\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nOzone\n1.0\n31.5\n42.1293\n168.0\n37\n\n\n2\nSolar\n7.0\n205.0\n185.932\n334.0\n7\n\n\n3\nWind\n1.7\n9.7\n9.95752\n20.7\n0\n\n\n4\nTemp\n56.0\n79.0\n77.8824\n97.0\n0"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#quantitative-eda-correlations",
    "href": "slides/lecture02-1-eda.html#quantitative-eda-correlations",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Quantitative EDA: Correlations",
    "text": "Quantitative EDA: Correlations\n\n\nCode\naq_cor = cor(Matrix(dropmissing(aq[:, 2:5])))\nDataFrame(aq_cor, names(aq[:, 2:5]))\n\n\n4×4 DataFrame\n\n\n\nRow\nOzone\nSolar\nWind\nTemp\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1.0\n0.348342\n-0.612497\n0.698541\n\n\n2\n0.348342\n1.0\n-0.127183\n0.294088\n\n\n3\n-0.612497\n-0.127183\n1.0\n-0.49719\n\n\n4\n0.698541\n0.294088\n-0.49719\n1.0"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#anscombes-quartet",
    "href": "slides/lecture02-1-eda.html#anscombes-quartet",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\n\n\nFour datasets, all with the same means, variances, correlations, and regression lines.\nShows the importance of visualization!\n\n\n\n\nAnscombe’s Quartet\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#visualizations-for-eda",
    "href": "slides/lecture02-1-eda.html#visualizations-for-eda",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Visualizations for EDA",
    "text": "Visualizations for EDA\n\nOften useful to start with scatterplots: one variable (typically independent) on the \\(x\\)-axis, another (typically dependent) on the \\(y\\)-axis.\nTime series: time always goes on the \\(x\\)-axis.\nBoxplots for comparison quantiles/ranges of variables or groups of data.\nQ-Q Plots to look for mismatches between distributions and data."
  },
  {
    "objectID": "slides/lecture02-1-eda.html#q-q-plots",
    "href": "slides/lecture02-1-eda.html#q-q-plots",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Q-Q Plots",
    "text": "Q-Q Plots\n\n\nOne exploratory method to see if your data is reasonably described by a theoretical distribution is a Q-Q plot.\n\n\n\nCode\nsamps = rand(Normal(0, 3), 20)\nqqplot(Normal, samps, tickfontsize=16, guidefontsize=18, linewidth=3, markersize=6)\nxlabel!(\"Theoretical Quantiles\")\nylabel!(\"Empirical Quantiles\")\nplot!(size=(500, 450))\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#fat-tailed-data-and-q-q-plots",
    "href": "slides/lecture02-1-eda.html#fat-tailed-data-and-q-q-plots",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Fat-Tailed Data and Q-Q Plots",
    "text": "Fat-Tailed Data and Q-Q Plots\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normal vs Cauchy Distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\nFigure 2: Q-Q Plot for Cauchy Data and Normal Distribution"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#multi-modal-data-and-q-q-plots",
    "href": "slides/lecture02-1-eda.html#multi-modal-data-and-q-q-plots",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Multi-Modal Data and Q-Q Plots",
    "text": "Multi-Modal Data and Q-Q Plots\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normal vs Cauchy Distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\nFigure 3: Q-Q Plot for Cauchy Data and Normal Distribution"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#boxplots-vs-histograms",
    "href": "slides/lecture02-1-eda.html#boxplots-vs-histograms",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Boxplots vs Histograms",
    "text": "Boxplots vs Histograms\n\n\nBoxplots: Show quantiles (usually median + 1.5 $IQR)\nHistogram: See distribution of data.\n\n\n\nCode\np1 = boxplot(skipmissing(aq[!, :Ozone]), ylabel=\"Ozone (ppb)\")\n\np2 = histogram(skipmissing(aq[!, :Ozone]), ylabel=\"Ozone (ppb)\")\n\np = plot(p1, p2, layout=(2, 1), size=(1100, 400))\n\n\n\n\n\n\n\nFigure 4: Paired plots of air quality dataset"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#scatterplots",
    "href": "slides/lecture02-1-eda.html#scatterplots",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\nCode\n# this uses the dataframe plotting syntax from StatsPlots.jl\np1 = @df aq scatter(:Ozone, :Solar, markersize=5, color=:blue, xlabel=\"Ozone (ppb)\", ylabel=\"Solar Radiation (lang)\", leftmargin=10mm)\np2 = @df aq scatter(:Ozone, :Wind, markersize=5, color=:blue, xlabel=\"Ozone (ppb)\", ylabel=\"Wind Speed (mph)\", leftmargin=10mm)\np3 = @df aq scatter(:Ozone, :Temp, markersize=5, color=:blue, xlabel=\"Ozone (ppb)\", ylabel=\"Max Temperature (°F) \", leftmargin=10mm)\n\np = plot(p1, p2, p3, layout=(1, 3), size=(1100, 400))\n\n\n\n\nFigure 5: Paired plots of air quality dataset"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#eda",
    "href": "slides/lecture02-1-eda.html#eda",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "EDA",
    "text": "EDA\n\nEDA: Understand structure of data while making as few assumptions as possible (e.g. look at the raw data, not “smoothed” versions).\nNeed to be willing to ask many questions of your data: most won’t make sense once you look at the data.\nYou might be asking the “right” questions but have the “wrong” data: be open-minded and rely on domain knowledge.\nQuantitative EDA can be a starting point, but EDA also requires visualization."
  },
  {
    "objectID": "slides/lecture02-1-eda.html#next-classes",
    "href": "slides/lecture02-1-eda.html#next-classes",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Data Visualization\nFriday: Data Visualization Discussion"
  },
  {
    "objectID": "slides/lecture02-1-eda.html#to-be-prepared-for-friday",
    "href": "slides/lecture02-1-eda.html#to-be-prepared-for-friday",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "To Be Prepared for Friday",
    "text": "To Be Prepared for Friday\n\nFind a visualization and its underlying dataset you find interesting (article, web repository, news story).\nCritique the visualization: What story is it trying to tell? What does it do well? What’s confusing or limited about it?\nTry reconstructing it to tell the story “better”.\nCome to class with the original visual and your attempt at improving it."
  },
  {
    "objectID": "slides/lecture02-1-eda.html#assessments",
    "href": "slides/lecture02-1-eda.html#assessments",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 Due Friday, 2/6."
  },
  {
    "objectID": "slides/lecture02-1-eda.html#references-scroll-for-full-list",
    "href": "slides/lecture02-1-eda.html#references-scroll-for-full-list",
    "title": "Exploratory Data Analysis and Visualization",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#eda",
    "href": "slides/lecture02-2-data-viz.html#eda",
    "title": "Data Visualization",
    "section": "EDA",
    "text": "EDA\n\nEDA: Understand structure of data while making as few assumptions as possible (e.g. look at the raw data, not “smoothed” versions).\nNeed to be willing to ask many questions of your data: most won’t make sense once you look at the data.\nYou might be asking the “right” questions but have the “wrong” data: be open-minded and rely on domain knowledge.\nQuantitative EDA can be a starting point, but EDA also requires visualization."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#considerations-when-plotting",
    "href": "slides/lecture02-2-data-viz.html#considerations-when-plotting",
    "title": "Data Visualization",
    "section": "Considerations When Plotting",
    "text": "Considerations When Plotting\n\nWorry about the vertical scale\n\nAre you exaggerating small changes? Downplaying meaningful changes?\n“Adjust vertical scale of data” to reflect meaningful changes and “Vertical scale should always include zero” are conflicting advice.\n\nNeed to be thoughtful: there is no such thing as objectively “correct” visualizations, just honest visualizations."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#avoid-using-two-vertical-axes",
    "href": "slides/lecture02-2-data-viz.html#avoid-using-two-vertical-axes",
    "title": "Data Visualization",
    "section": "Avoid Using Two Vertical Axes",
    "text": "Avoid Using Two Vertical Axes\n\nTwo axis plot\nSource: Datawrapper.de"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#avoid-using-two-vertical-axes-1",
    "href": "slides/lecture02-2-data-viz.html#avoid-using-two-vertical-axes-1",
    "title": "Data Visualization",
    "section": "Avoid Using Two Vertical Axes",
    "text": "Avoid Using Two Vertical Axes\n\n\n\n\n\nTwo axis plot\n\n\n\nSource: Datawrapper.de\n\n\n\n\n\nIllustration of how rescaling axes manipulates impression\n\n\n\n\nSource: Datawrapper.de"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#example-spurious-correlations",
    "href": "slides/lecture02-2-data-viz.html#example-spurious-correlations",
    "title": "Data Visualization",
    "section": "Example: Spurious Correlations",
    "text": "Example: Spurious Correlations\n\nSpurious Correlation 5901\nSource: Spurious Correlations"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#encode-information-with-marks-and-channels",
    "href": "slides/lecture02-2-data-viz.html#encode-information-with-marks-and-channels",
    "title": "Data Visualization",
    "section": "Encode Information With Marks and Channels",
    "text": "Encode Information With Marks and Channels\n\n\nMarks: Geometric primitives\n\nPoints\nSegments\nPaths\nPolygons\n\n\nChannels: Mark appearance\n\nColor (Hue/Saturation/Luminescence)\nPosition (1D/2D/3D)\nSize (Length/Area/Volume)\nAngle"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#ordered-vs.-categorical-attributes",
    "href": "slides/lecture02-2-data-viz.html#ordered-vs.-categorical-attributes",
    "title": "Data Visualization",
    "section": "Ordered vs. Categorical Attributes",
    "text": "Ordered vs. Categorical Attributes\nThe channels available depend on the type of attribute:\n\nOrdered attributes can be\n\nOrdinal: Ranking, no meaning to distance;\nQuantitative: Measure of magnitude which supports arithmetic comparison;\n\nCategorical attributes are unordered."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#channel-effectiveness-ordered-data",
    "href": "slides/lecture02-2-data-viz.html#channel-effectiveness-ordered-data",
    "title": "Data Visualization",
    "section": "Channel Effectiveness: Ordered Data",
    "text": "Channel Effectiveness: Ordered Data\n\n\n\n\n\n\n\n\n\nChannels for ordered data, arranged top-to-bottom from more to less effective (channels in the right column are less effective than those in the left). Modified from Healy (2018) after Munzner (2014)."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#channel-effectiveness-categorical-data",
    "href": "slides/lecture02-2-data-viz.html#channel-effectiveness-categorical-data",
    "title": "Data Visualization",
    "section": "Channel Effectiveness: Categorical Data",
    "text": "Channel Effectiveness: Categorical Data\n\n\n\n\n\n\n\n\n\nChannels for categorical data, arranged top-to-bottom from more to less effective. Modified from Healy (2018) after Munzer (2014)."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#preattentive-popout",
    "href": "slides/lecture02-2-data-viz.html#preattentive-popout",
    "title": "Data Visualization",
    "section": "Preattentive Popout",
    "text": "Preattentive Popout\n\n\nTry to make your key features “pop out” to the viewer during the pre-attentive scan.\n\nSearching for the blue circle becomes harder. Adapted from Healy (2018).\n\n\n\n\nCode\nnpt = 20\ndist = Distributions.Product(Uniform.([0, 0], [1, 1]))\npts = Tuple.(eachcol(rand(dist, npt)))\nblueidx = rand(1:npt)\np1 = scatter(pts[1:end .!= blueidx], color=:red, xticks=:false, yticks=:false, legend=:false, markersize=5, title=\"Color Only, N=20\", framestyle=:box)\nscatter!(p1, pts[blueidx, :], color=:blue, markersize=5)\n\nnpt = 100\npts = Tuple.(eachcol(rand(dist, npt)))\nblueidx = rand(1:npt)\np2 = scatter(pts[1:end .!= blueidx], color=:red, xticks=:false, yticks=:false, legend=:false, markersize=5, title=\"Color Only, N=100\", framestyle=:box)\nscatter!(p2, pts[blueidx, :], color=:blue, markersize=5)\n\nnpt = 20\npts = Tuple.(eachcol(rand(dist, npt)))\nblueidx = rand(1:npt)\np3 = scatter(pts[1:end .!= blueidx], color=:blue, markershape=:utriangle, xticks=:false, yticks=:false, legend=:false, markersize=5, title=\"Shape Only, N=20\", framestyle=:box)\nscatter!(p3, pts[blueidx, :], color=:blue, markersize=5, markershape=:circle)\n\nnpt = 100\npts = Tuple.(eachcol(rand(dist, npt)))\nblueidx = rand(1:npt)\np4 = scatter(pts[1:end .!= blueidx], color=:blue, markershape=:utriangle, xticks=:false, yticks=:false, legend=:false, markersize=5, title=\"Shape Only, N=100\", framestyle=:box)\nscatter!(p4, pts[blueidx, :], color=:blue, markersize=5, markershape=:circle)\n\nplot(p1, p2, p3, p4, layout=(2, 2), size=(800, 500))"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#gestalt-principles",
    "href": "slides/lecture02-2-data-viz.html#gestalt-principles",
    "title": "Data Visualization",
    "section": "Gestalt Principles",
    "text": "Gestalt Principles\nThe Gestalt school of psychology identified several principles of perception.\nCore idea: Humans are very good at finding structure.\n\nAs a result, you need to evaluate the totality of a visual field, not just each component."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#gestalt-principles-1",
    "href": "slides/lecture02-2-data-viz.html#gestalt-principles-1",
    "title": "Data Visualization",
    "section": "Gestalt Principles",
    "text": "Gestalt Principles\n\n\n\nProximity\nSimilarity\nParallelism\nCommon Fate\nCommon Region\nContinuity\nClosure\n\n\n\n\n\nIllustration of Gestalt principles\n\n\n\nIllustration of several Gestalt principles. Adapted from Healy (2018)."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#impact-of-continuity",
    "href": "slides/lecture02-2-data-viz.html#impact-of-continuity",
    "title": "Data Visualization",
    "section": "Impact of Continuity",
    "text": "Impact of Continuity"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#principle-of-proportional-ink",
    "href": "slides/lecture02-2-data-viz.html#principle-of-proportional-ink",
    "title": "Data Visualization",
    "section": "Principle of Proportional Ink",
    "text": "Principle of Proportional Ink\n\n\nOn the left: 2014 quantity is ~1.1x 2010 value, but due to vertical scale the 2014 bar uses 2.7x the ink.\nCan mislead viewers into thinking the difference is larger than it truly is!\n\n\n\n\nIllustration of Non-Proportional Ink\n\n\n\nSource: Calling Bullshit"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#color-schemes",
    "href": "slides/lecture02-2-data-viz.html#color-schemes",
    "title": "Data Visualization",
    "section": "Color Schemes",
    "text": "Color Schemes\nDifferent color schemes are appropriate depending on whether the data is sequential, divergent, or unordered.\n\n\n\n\n\n\nAppropriate Color Schemes\n\n\nColor schemes should be perceptually uniform to preserve a mapping between changes in perceived colors and changes in attribute values.\nTry to also choose color schemes which avoid confusing people who are color blind."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#color-schemes-1",
    "href": "slides/lecture02-2-data-viz.html#color-schemes-1",
    "title": "Data Visualization",
    "section": "Color Schemes",
    "text": "Color Schemes\nGood news: Most plotting libraries include a wide variety of perceptually uniform, color-blind safe color schemes.\nBad news: These are not usually the defaults (in particular, avoid “rainbow” color schemes)."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#sequential-color-schemes",
    "href": "slides/lecture02-2-data-viz.html#sequential-color-schemes",
    "title": "Data Visualization",
    "section": "Sequential Color Schemes",
    "text": "Sequential Color Schemes\nSequential schemes change in intensity from low to high as the value changes."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#divergent-color-schemes",
    "href": "slides/lecture02-2-data-viz.html#divergent-color-schemes",
    "title": "Data Visualization",
    "section": "Divergent Color Schemes",
    "text": "Divergent Color Schemes\nDivergent schemes intensify in two directions from a zero or mean value."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#unordered-color-schemes",
    "href": "slides/lecture02-2-data-viz.html#unordered-color-schemes",
    "title": "Data Visualization",
    "section": "Unordered Color Schemes",
    "text": "Unordered Color Schemes\nUnordered schemes are appropriate for categorical data."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#some-caveats",
    "href": "slides/lecture02-2-data-viz.html#some-caveats",
    "title": "Data Visualization",
    "section": "Some Caveats",
    "text": "Some Caveats\n\nThere is no recipe to effective visualization. Everything depends on your data and the story you want to tell.\nThis also means that defaults from data visualization packages are usually bad.\nPrinciples are largely based on Western (American/European) norms and may not translate perfectly.\nA lot of these guidelines are based on average outcomes, there is likely to be a lot of individual variation."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#first-attempt-paired-scatterplots",
    "href": "slides/lecture02-2-data-viz.html#first-attempt-paired-scatterplots",
    "title": "Data Visualization",
    "section": "First Attempt: Paired Scatterplots",
    "text": "First Attempt: Paired Scatterplots\n\n\nCode\naq = DataFrame(CSV.File(\"data/airquality/airquality.csv\"))\nrename!(aq, :\"Solar.R\" =&gt; :Solar) # rename solar radiation column to get rid of period\n\n# this uses the dataframe plotting syntax from StatsPlots.jl\np1 = @df aq scatter(:Ozone, :Solar, markersize=5, color=:blue, xlabel=\"Ozone (ppb)\", ylabel=\"Solar Radiation (lang)\", leftmargin=10mm)\np2 = @df aq scatter(:Ozone, :Wind, markersize=5, color=:blue, xlabel=\"Ozone (ppb)\", ylabel=\"Wind Speed (mph)\", leftmargin=10mm)\np3 = @df aq scatter(:Ozone, :Temp, markersize=5, color=:blue, xlabel=\"Ozone (ppb)\", ylabel=\"Max Temperature (°F) \", leftmargin=10mm)\n\np = plot(p1, p2, p3, layout=(1, 3), size=(1100, 400))\n\n\n\n\nFigure 1: Paired plots of air quality dataset"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#do-multiple-channels-help",
    "href": "slides/lecture02-2-data-viz.html#do-multiple-channels-help",
    "title": "Data Visualization",
    "section": "Do Multiple Channels Help?",
    "text": "Do Multiple Channels Help?\n\n\nCode\n# this uses the dataframe plotting syntax from StatsPlots.jl\naq2 = dropmissing(aq)\np = @df aq2 scatter(:Ozone, :Wind, marker_z=:Solar,\n    xlabel=\"Ozone (ppb)\", \n    ylabel=\"Wind Speed (mph)\", \n    colorbar_title=\"Solar Radiation (lang)\",\n    c=:reds,\n    markersize=5\n)\n\n\n\n\nFigure 2: Scatterplot of air quality dataset with solar color"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#do-multiple-channels-help-1",
    "href": "slides/lecture02-2-data-viz.html#do-multiple-channels-help-1",
    "title": "Data Visualization",
    "section": "Do Multiple Channels Help?",
    "text": "Do Multiple Channels Help?\n\n\nCode\n# this uses the dataframe plotting syntax from StatsPlots.jl\naq2 = dropmissing(aq)\np = @df aq2 scatter(:Ozone, :Wind, marker_z=:Temp,\n    xlabel=\"Ozone (ppb)\", \n    ylabel=\"Wind Speed (mph)\", \n    colorbar_title=\"Max Temperature (°F)\",\n    c=:reds,\n    markersize=5\n)\n\n\n\n\nFigure 3: Scatterplot of air quality dataset with temperature color"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#next-classes",
    "href": "slides/lecture02-2-data-viz.html#next-classes",
    "title": "Data Visualization",
    "section": "Next Classes",
    "text": "Next Classes\nFriday: Data Visualization Discussion\nNext Week: Probability Models and Linear Regression"
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#to-be-prepared-for-friday",
    "href": "slides/lecture02-2-data-viz.html#to-be-prepared-for-friday",
    "title": "Data Visualization",
    "section": "To Be Prepared for Friday",
    "text": "To Be Prepared for Friday\n\nFind a visualization and its underlying dataset you find interesting (article, web repository, news story).\nCritique the visualization: What story is it trying to tell? What does it do well? What’s confusing or limited about it?\nTry reconstructing it to tell the story “better”.\nCome to class with the original visual and your attempt at improving it."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#assessments",
    "href": "slides/lecture02-2-data-viz.html#assessments",
    "title": "Data Visualization",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 Due next Friday (2/6).\nExercises: Due before Monday’s class.\nReading: Do annotation before next Monday."
  },
  {
    "objectID": "slides/lecture02-2-data-viz.html#references-scroll-for-full-list",
    "href": "slides/lecture02-2-data-viz.html#references-scroll-for-full-list",
    "title": "Data Visualization",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nHealy, K. (2018). Data visualization: A practical introduction. Princeton University Press.\n\n\nMunzner, T. (2014). Visualization analysis and design. CRC Press."
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#last-class",
    "href": "slides/lecture01-2-probreview.html#last-class",
    "title": "Probability Review",
    "section": "Last Class",
    "text": "Last Class\n\nClass Motivation\nPolicies (see syllabus if class missed)"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#what-is-uncertainty",
    "href": "slides/lecture01-2-probreview.html#what-is-uncertainty",
    "title": "Probability Review",
    "section": "What is Uncertainty?",
    "text": "What is Uncertainty?\n\n\n\n…A departure from the (unachievable) ideal of complete determinism…\n\n\n— Walker et al. (2003)"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#types-of-uncertainty",
    "href": "slides/lecture01-2-probreview.html#types-of-uncertainty",
    "title": "Probability Review",
    "section": "Types of Uncertainty",
    "text": "Types of Uncertainty\n\n\n\n\n\n\n\n\n\n\nUncertainty Type\nSource\nExample(s)\n\n\n\n\nAleatory uncertainty\nRandomness\nDice rolls, Instrument imprecision\n\n\nEpistemic uncertainty\nLack of knowledge\nClimate sensitivity, Premier League champion\n\n\n\n\n\n\n\nWhich Uncertainty Type Meme\n\n\n\n\nNote that the distinction between aleatory and epistemic uncertainty is somewhat arbitrary (aside from maybe some quantum effects). For example, we often think of coin tosses as aleatory, but if we had perfect information about the toss, we might be able to predict the outcome with less uncertainty. There’s a famous paper by Persi Diaconis where he collaborated with engineers to build a device which could arbitrary bias a “fair” coin toss.\nBut in practice, this doesn’t really matter: the key thing is whether for a given model we’re treating the uncertainty as entirely random (e.g. white noise) versus being interested in the impacts of that uncertainty on the outcome of interest. And there’s a representation theorem by the Bayesian actuary Bruno de Finetti which shows that, under a condition called exchangeability, we can think of any random sequence as arising from an independent and identically distributed process, so the practical difference can collapse further."
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#sources-of-uncertainty",
    "href": "slides/lecture01-2-probreview.html#sources-of-uncertainty",
    "title": "Probability Review",
    "section": "Sources of Uncertainty",
    "text": "Sources of Uncertainty\n\nModel structures\nParameters\nData collection (measurement/observation errors)"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#probability",
    "href": "slides/lecture01-2-probreview.html#probability",
    "title": "Probability Review",
    "section": "Probability",
    "text": "Probability\nProbability is a language for expressing uncertainty.\nThe axioms of probability are straightforward:\n\n\\(\\mathcal{P}(E) \\geq 0\\);\n\\(\\mathcal{P}(\\Omega) = 1\\);\n\\(\\mathcal{P}(\\cup_{i=1}^\\infty E_i) = \\sum_{i=1}^\\infty \\mathcal{P}(E_i)\\) for disjoint \\(E_i\\).\n\n\nThe third is a generalization of the definition of independent events to sets of outcomes."
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#probability-distributions",
    "href": "slides/lecture01-2-probreview.html#probability-distributions",
    "title": "Probability Review",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nDistributions are mathematical representations of probabilities over a range of possible outcomes.\n\\[x \\to \\mathbb{P}_{\\color{green}\\mathcal{D}}[x] = p_{\\color{green}\\mathcal{D}}\\left(x | {\\color{purple}\\theta}\\right)\\]\n\n\\({\\color{green}\\mathcal{D}}\\): probability distribution (often implicit);\n\\({\\color{purple}\\theta}\\): distribution parameters"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#sampling-notation",
    "href": "slides/lecture01-2-probreview.html#sampling-notation",
    "title": "Probability Review",
    "section": "Sampling Notation",
    "text": "Sampling Notation\nTo write \\(x\\) is sampled from \\(\\mathcal{D}(\\theta)\\): \\[x \\sim \\mathcal{D}(\\theta)\\]\nFor example, for a normal distribution: \\[x \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(\\mu, \\sigma)\\]\n\n“i.i.d.” means “identically and independently distributed.””"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#probability-density-function",
    "href": "slides/lecture01-2-probreview.html#probability-density-function",
    "title": "Probability Review",
    "section": "Probability Density Function",
    "text": "Probability Density Function\nA continuous distribution \\(\\mathcal{D}\\) has a probability density function (PDF) \\(f_\\mathcal{D}(x) = p(x | \\theta)\\).\nThe probability of \\(x\\) occurring in an interval \\((a, b)\\) is \\[\\mathbb{P}[a \\leq x \\leq b] = \\int_a^b f_\\mathcal{D}(x)dx.\\]\nImportant: \\(\\mathbb{P}(x = x^*)\\) is zero!"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#probability-mass-functions",
    "href": "slides/lecture01-2-probreview.html#probability-mass-functions",
    "title": "Probability Review",
    "section": "Probability Mass Functions",
    "text": "Probability Mass Functions\nDiscrete distributions have probability mass functions (PMFs) which are defined at point values, e.g. \\(p(x = x^*) \\neq 0\\).\n\nUnlike continuous distributions, we can talk about the probability of individual values for discrete distributions, which a PMF provides versus a PDF. But in general these are the same things."
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#cumulative-density-functions",
    "href": "slides/lecture01-2-probreview.html#cumulative-density-functions",
    "title": "Probability Review",
    "section": "Cumulative Density Functions",
    "text": "Cumulative Density Functions\n\n\nIf \\(\\mathcal{D}\\) is a distribution with PDF \\(f_\\mathcal{D}(x)\\), the cumulative density function (CDF) of \\(\\mathcal{D}\\) is \\(F_\\mathcal{D}(x)\\):\n\\[F_\\mathcal{D}(x) = \\int_{-\\infty}^x f_\\mathcal{D}(u)du.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Relationship of CDF and PDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#relationship-between-pdfs-and-cdfs",
    "href": "slides/lecture01-2-probreview.html#relationship-between-pdfs-and-cdfs",
    "title": "Probability Review",
    "section": "Relationship Between PDFs and CDFs",
    "text": "Relationship Between PDFs and CDFs\nSince \\[F_\\mathcal{D}(x) = \\int_{-\\infty}^x f_\\mathcal{D}(u)du,\\]\nif \\(f_\\mathcal{D}\\) is continuous at \\(x\\), the Fundamental Theorem of Calculus gives: \\[f_\\mathcal{D}(x) = \\frac{d}{dx}F_\\mathcal{D}(x).\\]\n\nThe value of the CDF is the amount of probability “below” the value. So e.g. for a one-sided statistical test, the p-value is the complement of the CDF at the value of the test statistic."
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#quantiles",
    "href": "slides/lecture01-2-probreview.html#quantiles",
    "title": "Probability Review",
    "section": "Quantiles",
    "text": "Quantiles\n\n\nThe quantile function is the inverse of the CDF:\n\\[q(\\alpha) = F^{-1}_\\mathcal{D}(\\alpha)\\]\nSo \\[x_0 = q(\\alpha) \\iff \\mathbb{P}_\\mathcal{D}(X &lt; x_0) = \\alpha.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Relationship of CDF and PDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#measures-of-central-tendency",
    "href": "slides/lecture01-2-probreview.html#measures-of-central-tendency",
    "title": "Probability Review",
    "section": "Measures of Central Tendency",
    "text": "Measures of Central Tendency\nCommon measures of “typical” values of a function \\(f\\) of a random variable \\(Y \\sim p_{\\mathcal{D}}(y)\\):\n\nMean (or expected value): \\[\\mathbb{E}[f(Y)] = \\int_Y f(y) p(y) dy\\]\nMedian: \\(q(0.50)\\)\nMode: \\(\\max_{Y} p(f(Y))\\)"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#distributions-are-assumptions",
    "href": "slides/lecture01-2-probreview.html#distributions-are-assumptions",
    "title": "Probability Review",
    "section": "Distributions Are Assumptions",
    "text": "Distributions Are Assumptions\nSpecifying a distribution is making an assumption about observations and any applicable constraints.\nExamples: If your observations are, then the most common choices are:\n\nContinuous and fat-tailed? Cauchy distribution\nContinuous and bounded? Beta distribution\nSums of positive random variables? Gamma or Normal distribution."
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#statistics-of-random-variables-are-random-variables",
    "href": "slides/lecture01-2-probreview.html#statistics-of-random-variables-are-random-variables",
    "title": "Probability Review",
    "section": "Statistics of Random Variables are Random Variables",
    "text": "Statistics of Random Variables are Random Variables\nThe sum or mean of a random sample is itself a random variable:\n\\[\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i \\sim \\mathcal{D}_n\\]\n\n\\(\\mathcal{D}_n\\): The sampling distribution of the mean (or sum, or other estimate of interest)."
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#sampling-distributions",
    "href": "slides/lecture01-2-probreview.html#sampling-distributions",
    "title": "Probability Review",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\nIllustration of the Sampling Distribution"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#likelihood-1",
    "href": "slides/lecture01-2-probreview.html#likelihood-1",
    "title": "Probability Review",
    "section": "Likelihood",
    "text": "Likelihood\nHow do we “fit” distributions to a dataset?\nLikelihood of data to have come from distribution \\(\\mathcal{D}\\) with pdf \\(f(\\mathbf{x} | \\theta)\\):\n\\[\\mathcal{L}(\\theta | \\mathbf{x}) = \\underbrace{f(\\mathbf{x} | \\theta)}_{\\text{PDF}}\\]\nIn other words: likelihood evaluates parameters conditional on data, PDF evaluates data conditional on parameters.\n\nThe likelihood gives us a measure of how probable a dataset is from a given distribution. It’s the PDF of the distribution at the data.\nBut the perspective is flipped: instead of fixing a distribution and calculating the probability of some data, we fix the data and look at how the probability of observing that data changes as the distribution changes."
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#normal-distribution-pdf",
    "href": "slides/lecture01-2-probreview.html#normal-distribution-pdf",
    "title": "Probability Review",
    "section": "Normal Distribution PDF",
    "text": "Normal Distribution PDF\n\\[f_\\mathcal{D}(x) = p(x | \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}^2\\right)\\right)\\]\n\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#likelihood-of-multiple-samples",
    "href": "slides/lecture01-2-probreview.html#likelihood-of-multiple-samples",
    "title": "Probability Review",
    "section": "Likelihood of Multiple Samples",
    "text": "Likelihood of Multiple Samples\nFor multiple (independent) samples \\(\\mathbf{x} = \\{x_1, \\ldots, x_n\\}\\):\n\\[\\mathcal{L}(\\theta | \\mathbf{x}) = \\prod_{i=1}^n \\mathcal{L}(\\theta | x_i).\\]"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#likelihood-example",
    "href": "slides/lecture01-2-probreview.html#likelihood-example",
    "title": "Probability Review",
    "section": "Likelihood Example",
    "text": "Likelihood Example\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nLikelihood\n\n\n\n\n\\(N(0, 1)\\)\n3.7e-11"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#likelihood-example-1",
    "href": "slides/lecture01-2-probreview.html#likelihood-example-1",
    "title": "Probability Review",
    "section": "Likelihood Example",
    "text": "Likelihood Example\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nLikelihood\n\n\n\n\n\\(N(0, 1)\\)\n3.7e-11\n\n\n\\(N(-1, 2)\\)\n5.9e-10"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#likelihood-example-2",
    "href": "slides/lecture01-2-probreview.html#likelihood-example-2",
    "title": "Probability Review",
    "section": "Likelihood Example",
    "text": "Likelihood Example\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nLikelihood\n\n\n\n\n\\(N(0, 1)\\)\n3.7e-11\n\n\n\\(N(-1, 2)\\)\n5.9e-10\n\n\n\\(N(-1, 1)\\)\n1.2e-13"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#log-likelihood",
    "href": "slides/lecture01-2-probreview.html#log-likelihood",
    "title": "Probability Review",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\n\n\nLikelihoods get very small very fast due to multiplying small numbers.\nThis is a computational problem due to underflow.\nWe use logarithms to avoid these issues: compute \\(\\log \\mathcal{L}(\\theta | x)\\).\n\n\n\n\nLogarithms for probability calculations"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#confidence-intervals",
    "href": "slides/lecture01-2-probreview.html#confidence-intervals",
    "title": "Probability Review",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\nFrequentist estimates have confidence intervals, which will contain the “true” parameter value for \\(\\alpha\\)% of data samples.\nNo guarantee that an individual CI contains the true value (with any “probability”)!\n\n\n\n\nHorseshoe Illustration\n\n\n\nSource: https://www.wikihow.com/Throw-a-Horseshoe\n\n\n\nConfidence intervals only capture uncertainty in parameter inferences due to data uncertainty, though this language sometimes gets misused to also refer to data/estimand uncertainty."
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#example-95-cis-for-n0.4-2",
    "href": "slides/lecture01-2-probreview.html#example-95-cis-for-n0.4-2",
    "title": "Probability Review",
    "section": "Example: 95% CIs for N(0.4, 2)",
    "text": "Example: 95% CIs for N(0.4, 2)\n\nCode\n# set up distribution\nmean_true = 0.4\nn_cis = 100 # number of CIs to compute\ndist = Normal(mean_true, 2)\n\n# use sample size of 100\nsamples = rand(dist, (100, n_cis))\n# mapslices broadcasts over a matrix dimension, could also use a loop\nsample_means = mapslices(mean, samples; dims=1)\nsample_sd = mapslices(std, samples; dims=1) \nmc_sd = 1.96 * sample_sd / sqrt(100)\nmc_ci = zeros(n_cis, 2) # preallocate\nfor i = 1:n_cis\n    mc_ci[i, 1] = sample_means[i] - mc_sd[i]\n    mc_ci[i, 2] = sample_means[i] + mc_sd[i]\nend\n# find which CIs contain the true value\nci_true = (mc_ci[:, 1] .&lt; mean_true) .&& (mc_ci[:, 2] .&gt; mean_true)\n# compute percentage of CIs which contain the true value\nci_frac1 = 100 * sum(ci_true) ./ n_cis\n\n# plot CIs\np1 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:deepskyblue, label=\"95% Confidence Interval\", title=\"Sample Size 100\", yticks=:false, legend=:false)\nfor i = 2:n_cis\n    if ci_true[i]\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:deepskyblue, label=:false)\n    else\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)\n    end\nend\nvline!(p1, [mean_true], color=:black, linewidth=2, linestyle=:dash, label=\"True Value\") # plot true value as a vertical line\nxaxis!(p1, \"Estimate\")\nplot!(p1, size=(500, 350)) # resize to fit slide\n\n# use sample size of 1000\nsamples = rand(dist, (1000, n_cis))\n# mapslices broadcasts over a matrix dimension, could also use a loop\nsample_means = mapslices(mean, samples; dims=1)\nsample_sd = mapslices(std, samples; dims=1) \nmc_sd = 1.96 * sample_sd / sqrt(1000)\nmc_ci = zeros(n_cis, 2) # preallocate\nfor i = 1:n_cis\n    mc_ci[i, 1] = sample_means[i] - mc_sd[i]\n    mc_ci[i, 2] = sample_means[i] + mc_sd[i]\nend\n# find which CIs contain the true value\nci_true = (mc_ci[:, 1] .&lt; mean_true) .&& (mc_ci[:, 2] .&gt; mean_true)\n# compute percentage of CIs which contain the true value\nci_frac2 = 100 * sum(ci_true) ./ n_cis\n\n# plot CIs\np2 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:deepskyblue, label=\"95% Confidence Interval\", title=\"Sample Size 1,000\", yticks=:false, legend=:false)\nfor i = 2:n_cis\n    if ci_true[i]\n        plot!(p2, [mc_ci[i, :]], [i, i], linewidth=2, color=:deepskyblue, label=:false)\n    else\n        plot!(p2, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)\n    end\nend\nvline!(p2, [mean_true], color=:black, linewidth=2, linestyle=:dash, label=\"True Value\") # plot true value as a vertical line\nxaxis!(p2, \"Estimate\")\nplot!(p2, size=(500, 350)) # resize to fit slide\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sample Size 100\n\n\n\n\n\n\n\n\n\n\n\n(b) Sample Size 1,000\n\n\n\n\n\n\n\nFigure 4: Display of 95% confidence intervals\n\n\n\n96% of the CIs contain the true value (left) vs. 96% (right)"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#predictive-intervals",
    "href": "slides/lecture01-2-probreview.html#predictive-intervals",
    "title": "Probability Review",
    "section": "Predictive Intervals",
    "text": "Predictive Intervals\n\n\nPredictive intervals capture uncertainty in an estimand.\nWith what probability would I see a particular outcome in the future?\nOften need to construct these using simulation.\n\n\n\n\n\n\n\n\nFigure 5: Two different 95% credible intervals.\n\n\n\n\n\n\nDue to this non-uniqueness, the typical convention is to use the “equal tailed” interval based on quantiles."
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#next-classes",
    "href": "slides/lecture01-2-probreview.html#next-classes",
    "title": "Probability Review",
    "section": "Next Classes",
    "text": "Next Classes\nNext Week: Exploratory Data Analysis"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#assessments",
    "href": "slides/lecture01-2-probreview.html#assessments",
    "title": "Probability Review",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 due next Friday (2/6).\nReading:"
  },
  {
    "objectID": "slides/lecture01-2-probreview.html#references-scroll-for-full-list",
    "href": "slides/lecture01-2-probreview.html#references-scroll-for-full-list",
    "title": "Probability Review",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nWalker, W. E., Harremoës, P., Rotmans, J., Sluijs, J. P. van der, Asselt, M. B. A. van, Janssen, P., & Krayer von Krauss, M. P. (2003). Defining uncertainty: A conceptual basis for uncertainty management in model-based decision support. Integrated Assessment, 4, 5–17. https://doi.org/10.1076/iaij.4.1.5.16466"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#extreme-values",
    "href": "slides/lecture12-1-peaks-thresholds.html#extreme-values",
    "title": "Peaks Over Thresholds",
    "section": "Extreme Values",
    "text": "Extreme Values\nValues with a very low probability of occurring, not necessarily high-impact events (which don’t have to be rare!).\n\n“Block” extremes, e.g. annual maxima (block maxima)\nValues which exceed a certain threshold (peaks over threshold)"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#generalized-extreme-values",
    "href": "slides/lecture12-1-peaks-thresholds.html#generalized-extreme-values",
    "title": "Peaks Over Thresholds",
    "section": "Generalized Extreme Values",
    "text": "Generalized Extreme Values\n\n\nBlock maxima \\(Y_n = \\max \\{X_1, \\ldots, X_n \\}\\) are modeled using Generalized Extreme Value distributions: \\(GEV(\\mu, \\sigma, \\xi)\\).\n\n\n\nCode\np1 = plot(-2:0.1:6, GeneralizedExtremeValue(0, 1, 0.5), linewidth=3, color=:red, label=L\"$\\xi = 1/2$\", lw=3)\nplot!(-4:0.1:6, GeneralizedExtremeValue(0, 1, 0), linewidth=3, color=:green, label=L\"$\\xi = 0$\", lw=3)\nplot!(-4:0.1:2, GeneralizedExtremeValue(0, 1, -0.5), linewidth=3, color=:blue, label=L\"$\\xi = -1/2$\", lw=3)\nscatter!((-2, 0), color=:red, label=:false)\nscatter!((2, 0), color=:blue, label=:false)\nylabel!(\"Density\")\nxlabel!(L\"$x$\")\nplot!(size=(600, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Shape of the GEV distribution with different choices of \\(\\xi\\)."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#return-levels",
    "href": "slides/lecture12-1-peaks-thresholds.html#return-levels",
    "title": "Peaks Over Thresholds",
    "section": "Return Levels",
    "text": "Return Levels\nReturn Levels are a central (if poorly named) concept in risk analysis.\nThe \\(T\\)-year return level is the value expected to be observed on average once every \\(T\\) years.\nFrom a GEV fit to annual maxima: \\(T\\)-year return level is the \\(1-1/T\\) quantile."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#return-periods",
    "href": "slides/lecture12-1-peaks-thresholds.html#return-periods",
    "title": "Peaks Over Thresholds",
    "section": "Return Periods",
    "text": "Return Periods\nThe return period of an extreme value is the inverse of the exceedance probability.\nExample: The 100-year return period has an exceedance probability of 1%, e.g. the 0.99 quantile.\nReturn levels are associated with the analogous return period."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#drawbacks-of-block-maxima",
    "href": "slides/lecture12-1-peaks-thresholds.html#drawbacks-of-block-maxima",
    "title": "Peaks Over Thresholds",
    "section": "Drawbacks of Block Maxima",
    "text": "Drawbacks of Block Maxima\nThe block-maxima approach has two potential drawbacks:\n\nUses a limited amount of data;\nDoesn’t capture the potential for multiple extremes within a block."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#peaks-over-thresholds-1",
    "href": "slides/lecture12-1-peaks-thresholds.html#peaks-over-thresholds-1",
    "title": "Peaks Over Thresholds",
    "section": "Peaks Over Thresholds",
    "text": "Peaks Over Thresholds\nConsider the conditional excess distribution function\n\\[F_u(y) = \\mathbb{P}(X - u &gt; y  |  X &gt; u)\\]\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of the Conditional Excess Function\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Histogram of conditional excesses"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#generalized-pareto-distribution-gpd",
    "href": "slides/lecture12-1-peaks-thresholds.html#generalized-pareto-distribution-gpd",
    "title": "Peaks Over Thresholds",
    "section": "Generalized Pareto Distribution (GPD)",
    "text": "Generalized Pareto Distribution (GPD)\nFor a large number of underlying distributions of \\(X\\), \\(F_u(y)\\) is well-approximated by a Generalized Pareto Distribution (GPD):\n\\[F_u(y) \\to G(y) = 1 - \\left[1 + \\xi\\left(\\frac{y-\\mu}{\\sigma}\\right)^{-1/\\xi}\\right],\\] defined for \\(y\\) such that \\(1 + \\xi(y-\\mu)/\\sigma &gt; 0\\)."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#generalized-pareto-distribution-gpd-1",
    "href": "slides/lecture12-1-peaks-thresholds.html#generalized-pareto-distribution-gpd-1",
    "title": "Peaks Over Thresholds",
    "section": "Generalized Pareto Distribution (GPD)",
    "text": "Generalized Pareto Distribution (GPD)\nSimilarly to the GEV distribution, the GPD distribution has three parameters:\n\nlocation \\(\\mu\\);\nscale \\(\\sigma &gt; 0\\);\nshape \\(\\xi\\)."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#gpd-types",
    "href": "slides/lecture12-1-peaks-thresholds.html#gpd-types",
    "title": "Peaks Over Thresholds",
    "section": "GPD Types",
    "text": "GPD Types\n\n\n\n\\(\\xi &gt; 0\\): heavy-tailed\n\\(\\xi = 0\\): light-tailed\n\\(\\xi &lt; 0\\): bounded\n\n\n\n\nCode\np1 = plot(-2:0.1:6, GeneralizedPareto(0, 1, 0.5), linewidth=3, color=:red, label=L\"$\\xi = 1/2$\", left_margin=5mm, bottom_margin=10mm)\nplot!(-4:0.1:6, GeneralizedPareto(0, 1, 0), linewidth=3, color=:green, label=L\"$\\xi = 0$\")\nplot!(-4:0.1:2, GeneralizedPareto(0, 1, -0.5), linewidth=3, color=:blue, label=L\"$\\xi = -1/2$\")\nscatter!((-2, 0), color=:red, label=:false)\nscatter!((2, 0), color=:blue, label=:false)\nylabel!(\"Density\")\nxlabel!(L\"$x$\")\nplot!(size=(600, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Shape of the GPD distribution with different choices of \\(\\xi\\)."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#exceedances-can-occur-in-clusters",
    "href": "slides/lecture12-1-peaks-thresholds.html#exceedances-can-occur-in-clusters",
    "title": "Peaks Over Thresholds",
    "section": "Exceedances Can Occur In Clusters",
    "text": "Exceedances Can Occur In Clusters\n\n\nCode\n# load SF tide gauge data\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\nd_sf = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=d_sf.datetime[ma_offset+1:end-ma_offset], residual=d_sf.gauge[ma_offset+1:end-ma_offset] .- moving_average(d_sf.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\n\n\nthresh = 1.0\ndat_ma_plot = @subset(dat_ma, year.(:datetime) .&gt; 2020)\ndat_ma_plot.residual = dat_ma_plot.residual ./ 1000\np1 = plot(dat_ma_plot.datetime, dat_ma_plot.residual; linewidth=2, ylabel=\"Gauge Weather Variability (m)\", label=\"Observations\", legend=:bottom, xlabel=\"Date/Time\", right_margin=10mm, left_margin=5mm, bottom_margin=5mm)\nhline!([thresh], color=:red, linestyle=:dash, label=\"Threshold\")\nscatter!(dat_ma_plot.datetime[dat_ma_plot.residual .&gt; thresh], dat_ma_plot.residual[dat_ma_plot.residual .&gt; thresh], markershape=:x, color=:black, markersize=3, label=\"Exceedances\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Peaks Over Thresholds for the SF Tide Gauge Data"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#declustering",
    "href": "slides/lecture12-1-peaks-thresholds.html#declustering",
    "title": "Peaks Over Thresholds",
    "section": "Declustering",
    "text": "Declustering\nArns et al. (2013) note: there is no clear declustering time period to use: need to rely on physical understanding of events and “typical” durations.\nIf we have prior knowledge about the duration of physical processes leading to clustered extremes (e.g. storm durations), can use this. Otherwise, need some way to estimate cluster duration from the data."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#extremal-index",
    "href": "slides/lecture12-1-peaks-thresholds.html#extremal-index",
    "title": "Peaks Over Thresholds",
    "section": "Extremal Index",
    "text": "Extremal Index\nThe most common is the extremal index \\(\\theta(u)\\), which measures the inter-exceedance time for a given threshold \\(u\\).\n\\[0 \\leq \\theta(u) \\leq 1,\\]\nwhere \\(\\theta(u) = 1\\) means independence and \\(\\theta(u) = 0\\) means the entire dataset is one cluster."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#extremal-index-1",
    "href": "slides/lecture12-1-peaks-thresholds.html#extremal-index-1",
    "title": "Peaks Over Thresholds",
    "section": "Extremal Index",
    "text": "Extremal Index\n\\(\\theta(u)\\) has two meanings:\n\nThe “propensity to cluster”: \\(\\theta\\) is the probability that the process has left one exceedance cluster;\nThe “reciprocal of the clustering duration”: \\(1/\\theta\\) is the mean time between clusters."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#computing-the-extremal-index",
    "href": "slides/lecture12-1-peaks-thresholds.html#computing-the-extremal-index",
    "title": "Peaks Over Thresholds",
    "section": "Computing the Extremal Index",
    "text": "Computing the Extremal Index\nThis estimator is taken from Ferro & Segers (2003).\nLet \\(N = \\sum_{i=1}^n \\mathbb{I}(X_i &gt; u)\\) be the total number of exceedances.\nDenote by \\(1 \\leq S_1 &lt; \\ldots &lt; S_N \\leq n\\) the exceedance times.\nThen the inter-exceedance times are \\[T_i = S_{i+1} - S_i, \\quad 1 \\leq i \\leq N-1.\\]"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#computing-the-extremal-index-1",
    "href": "slides/lecture12-1-peaks-thresholds.html#computing-the-extremal-index-1",
    "title": "Peaks Over Thresholds",
    "section": "Computing the Extremal Index",
    "text": "Computing the Extremal Index\n\\[\\hat{\\theta}(u) = \\frac{2\\left(\\sum_{i=1}^{N-1} T_i\\right)^2}{(N-1)\\sum_{i=1}^{N-1}T_i^2}\\]\n\n\nCode\n# find total number of exceedances and exceedance times\ndat_ma.residual = dat_ma.residual ./ 1000 # convert to m\nS = findall(dat_ma.residual .&gt; thresh)\nN = length(S)\nT = diff(S) # get difference between adjacent exceedances\nθ = 2 * sum(T)^2 / ((N-1) * sum(T.^2)) # extremal index\n\n\nFor the SF tide gauge data and \\(u=1.0 \\text{m}\\), we get the an extremal index of 0.24 and a declustering time of 4.0 hours."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#mapping-data-to-clusters",
    "href": "slides/lecture12-1-peaks-thresholds.html#mapping-data-to-clusters",
    "title": "Peaks Over Thresholds",
    "section": "Mapping Data To Clusters",
    "text": "Mapping Data To Clusters\n\n\nCode\n# cluster data points which occur within period\nfunction assign_cluster(dat, period)\n    cluster_index = 1\n    clusters = zeros(Int, length(dat))\n    for i in 1:length(dat)\n        if clusters[i] == 0\n            clusters[findall(abs.(dat .- dat[i]) .&lt;= period)] .= cluster_index\n            cluster_index += 1\n        end\n    end\n    return clusters\nend\n\n# cluster exceedances that occur within a four-hour window\n# @transform is a macro from DataFramesMeta.jl which adds a new column based on a data transformation\ndat_exceed = dat_ma[dat_ma.residual .&gt; thresh, :]\ndat_exceed = @transform dat_exceed :cluster = assign_cluster(:datetime, Dates.Hour(4))\n# find maximum value within cluster\ndat_decluster = combine(dat_exceed -&gt; dat_exceed[argmax(dat_exceed.residual), :], \n    groupby(dat_exceed, :cluster))\ndat_decluster"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#declustered-distribution",
    "href": "slides/lecture12-1-peaks-thresholds.html#declustered-distribution",
    "title": "Peaks Over Thresholds",
    "section": "Declustered Distribution",
    "text": "Declustered Distribution\n\n\nCode\np = histogram(dat_decluster.residual .- thresh,\n    normalize = :pdf,\n    label=\"Data\",\n    xlabel=\"Threshold Exceedance (m)\",\n    ylabel=\"PDF\",\n    yticks=[],\n    left_margin=10mm, \n    right_margin=10mm,\n    bottom_margin=5mm\n    )\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Histogram of clustered exceedances for SF tide gauge data."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#gpd-fit",
    "href": "slides/lecture12-1-peaks-thresholds.html#gpd-fit",
    "title": "Peaks Over Thresholds",
    "section": "GPD Fit",
    "text": "GPD Fit\n\nCode\n# fit GPD\ninit_θ = [1.0, 1.0]\nlow_bds = [0.0, -Inf]\nup_bds = [Inf, Inf]\ngpd_lik(θ) = -sum(logpdf(GeneralizedPareto(0.0, θ[1], θ[2]), dat_decluster.residual .- thresh))\nθ_mle = Optim.optimize(gpd_lik, low_bds, up_bds, init_θ).minimizer\np1 = plot!(p, GeneralizedPareto(0.0, θ_mle[1], θ_mle[2]), linewidth=3, label=\"GPD Fit\")\nplot!(size=(600, 450))\n\n# Q-Q Plot\np2 = qqplot(GeneralizedPareto(0.0, θ_mle[1], θ_mle[2]), dat_decluster.residual .- thresh, \n    xlabel=\"Theoretical Quantile\",\n    ylabel=\"Empirical Quantile\",\n    linewidth=3,\n    left_margin=5mm, \n    right_margin=10mm,\n    bottom_margin=5mm)\nplot!(size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) GPD fit to tide gauge readings over 1m of San Francisco Tide Gauge Data\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 7"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#but-what-about-exceedance-frequency",
    "href": "slides/lecture12-1-peaks-thresholds.html#but-what-about-exceedance-frequency",
    "title": "Peaks Over Thresholds",
    "section": "But What About Exceedance Frequency?",
    "text": "But What About Exceedance Frequency?\n\n\nThe GPD fit gives a distribution for how extreme threshold exceedances are when they occur.\nBut how often do they occur?\n\n\n\nCode\n# add column with years of occurrence\ndat_decluster = @transform dat_decluster :year = Dates.year.(dat_decluster.datetime)\n# group by year and add up occurrences\nexceed_counts = combine(groupby(dat_decluster, :year), nrow =&gt; :count)\ndelete!(exceed_counts, nrow(exceed_counts)) # including 2023 will bias the count estimate\np = histogram(exceed_counts.count, legend=:false, \n    xlabel=\"Yearly Exceedances\",\n    ylabel=\"Count\",\n    left_margin=5mm,\n    bottom_margin=10mm\n)\nplot!(size=(600, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: histogram of number of exceedances in each year"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#poisson---generalized-pareto-process",
    "href": "slides/lecture12-1-peaks-thresholds.html#poisson---generalized-pareto-process",
    "title": "Peaks Over Thresholds",
    "section": "Poisson - Generalized Pareto Process",
    "text": "Poisson - Generalized Pareto Process\nModel the number of new exceedances with a Poisson distribution\n\\[n \\sim \\text{Poisson}(\\lambda_u),\\]\nThe MLE for \\(\\lambda_u\\) is the mean of the count data, in this case 49.5.\nThen, for each \\(i=1, \\ldots, n\\), sample \\[X_i \\sim \\text{GeneralizedPareto}(u, \\sigma, \\xi).\\]"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#poisson---generalized-pareto-process-return-levels",
    "href": "slides/lecture12-1-peaks-thresholds.html#poisson---generalized-pareto-process-return-levels",
    "title": "Peaks Over Thresholds",
    "section": "Poisson - Generalized Pareto Process Return Levels",
    "text": "Poisson - Generalized Pareto Process Return Levels\nThen the return level for return period \\(m\\) years can be obtained by solving the quantile equation (see Coles (2001) for details):\n\\[\\text{RL}_m = \\begin{cases}u + \\frac{\\sigma}{\\xi} \\left((m\\lambda_u)^\\xi - 1\\right) & \\text{if}\\  \\xi \\neq 0 \\\\ u + \\sigma \\log(m\\lambda_u) & \\text{if}\\  \\xi = 0.\\end{cases}\\]"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#poisson-gpp-example",
    "href": "slides/lecture12-1-peaks-thresholds.html#poisson-gpp-example",
    "title": "Peaks Over Thresholds",
    "section": "Poisson-GPP Example",
    "text": "Poisson-GPP Example\n\n\nQuestion: What is the 100-year return period of the SF data?\n\n\n\nCode\ncount_dist = fit(Poisson, exceed_counts.count)\nλ = params(count_dist)[1]\n\np = histogram(exceed_counts.count, legend=:false, \n    xlabel=\"Yearly Exceedances\",\n    ylabel=\"Count\",\n    normalize=:pdf,\n    left_margin=5mm,\n    bottom_margin=10mm\n)\nplot!(size=(600, 500))\nplot!(p, count_dist)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Histogram of number of exceedances in each year with Poisson fit"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#poisson-gpp-example-1",
    "href": "slides/lecture12-1-peaks-thresholds.html#poisson-gpp-example-1",
    "title": "Peaks Over Thresholds",
    "section": "Poisson-GPP Example",
    "text": "Poisson-GPP Example\n\n\nPoisson: \\(\\lambda = 50\\.0\\)\nGPD parameters:\n\n\\(\\mu = 0\\) (fixed)\n\\(\\sigma = 0\\.11\\)\n\\(\\xi = \\-0\\.17\\)\n\n\n\\[\n\\begin{align*}\n\\text{RL}_{\\text{100}} &= 1 + \\frac{\\sigma}{\\xi} \\left((m\\lambda)^\\xi - 1\\right) \\\\\n&= 1 - \\frac{0.11}{0.17}\\left((100 * 50)^{-0.17} - 1\\right) \\\\\n&\\approx 1.5 \\text{m}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#nonstationary-pot",
    "href": "slides/lecture12-1-peaks-thresholds.html#nonstationary-pot",
    "title": "Peaks Over Thresholds",
    "section": "Nonstationary POT",
    "text": "Nonstationary POT\nNon-stationarity could influence either the Poisson (how many exceedances occur each year) or GPD (when exceedances occur how are they distributed)?\nOften simplest to use a Poisson GLM:\n\\[\\lambda_t = \\text{Poisson}(\\lambda_0 + \\lambda_1 x_t)\\]\nand let the GPD be stationary."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#nonstationary-return-levels",
    "href": "slides/lecture12-1-peaks-thresholds.html#nonstationary-return-levels",
    "title": "Peaks Over Thresholds",
    "section": "Nonstationary Return Levels",
    "text": "Nonstationary Return Levels\nThen the \\(m\\)-year return level associated with covariate \\(x_t\\) becomes\n\\[\\text{RL}_m = \\begin{cases}u + \\frac{\\sigma}{\\xi} \\left((m(\\lambda_0 + \\lambda_1 x_t))^\\xi - 1\\right) & \\text{if}\\  \\xi \\neq 0 \\\\ u + \\sigma \\log(m(\\lambda_0 + \\lambda_1 x_t)) & \\text{if}\\  \\xi = 0.\\end{cases}\\]"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#upcoming-schedule",
    "href": "slides/lecture12-1-peaks-thresholds.html#upcoming-schedule",
    "title": "Peaks Over Thresholds",
    "section": "Upcoming Schedule",
    "text": "Upcoming Schedule\nWednesday: Missing Data and E-M Algorithm\nMonday: Mixture Models and Model-Based Clustering (maybe)\nNext Wednesday (4/23): No Class"
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#assessments",
    "href": "slides/lecture12-1-peaks-thresholds.html#assessments",
    "title": "Peaks Over Thresholds",
    "section": "Assessments",
    "text": "Assessments\nHW5 released, due 5/2."
  },
  {
    "objectID": "slides/lecture12-1-peaks-thresholds.html#references",
    "href": "slides/lecture12-1-peaks-thresholds.html#references",
    "title": "Peaks Over Thresholds",
    "section": "References",
    "text": "References\n\n\n\n\nArns, A., Wahl, T., Haigh, I. D., Jensen, J., & Pattiaratchi, C. (2013). Estimating extreme water level probabilities: A comparison of the direct methods and recommendations for best practise. Coast. Eng., 81, 51–66. https://doi.org/10.1016/j.coastaleng.2013.07.003\n\n\nColes, S. (2001). An Introduction to Statistical Modeling of Extreme Values. London: Springer-Verlag London.\n\n\nFerro, C. A. T., & Segers, J. (2003). Inference for clusters of extreme values. J. R. Stat. Soc. Series B Stat. Methodol., 65, 545–556. https://doi.org/10.1111/1467-9868.00401"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#the-bootstrap",
    "href": "slides/lecture08-1-parametric-bootstrap.html#the-bootstrap",
    "title": "The Parametric Bootstrap",
    "section": "The Bootstrap",
    "text": "The Bootstrap\n\n\nEfron (1979) suggested combining estimation with simulation: the bootstrap.\nKey idea: use the data to simulate a data-generating mechanism.\n\n\n\n\n\nBaron von Munchhausen Pulling Himself By His Hair\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#why-does-the-bootstrap-work",
    "href": "slides/lecture08-1-parametric-bootstrap.html#why-does-the-bootstrap-work",
    "title": "The Parametric Bootstrap",
    "section": "Why Does The Bootstrap Work?",
    "text": "Why Does The Bootstrap Work?\nLet \\(t_0\\) the “true” value of a statistic, \\(\\hat{t}\\) the estimate of the statistic from the sample, and \\((\\tilde{t}_i)\\) the bootstrap estimates.\n\nVariance: \\(\\text{Var}[\\hat{t}] \\approx \\text{Var}[\\tilde{t}]\\)\nThen the bootstrap error distribution approximates the sampling distribution \\[(\\tilde{t}_i - \\hat{t}) \\overset{\\mathcal{D}}{\\sim} \\hat{t} - t_0\\]"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#the-non-parametric-bootstrap",
    "href": "slides/lecture08-1-parametric-bootstrap.html#the-non-parametric-bootstrap",
    "title": "The Parametric Bootstrap",
    "section": "The Non-Parametric Bootstrap",
    "text": "The Non-Parametric Bootstrap\n\n\nThe non-parametric bootstrap is the most “naive” approach to the bootstrap: resample-then-estimate.\n\n\n\n\nNon-Parametric Bootstrap"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#approaches-to-bootstrapping-structured-data",
    "href": "slides/lecture08-1-parametric-bootstrap.html#approaches-to-bootstrapping-structured-data",
    "title": "The Parametric Bootstrap",
    "section": "Approaches to Bootstrapping Structured Data",
    "text": "Approaches to Bootstrapping Structured Data\n\nCorrelations: Transform to uncorrelated data (principal components, etc.), sample, transform back.\nTime Series: Block bootstrap"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#sources-of-non-parametric-bootstrap-error",
    "href": "slides/lecture08-1-parametric-bootstrap.html#sources-of-non-parametric-bootstrap-error",
    "title": "The Parametric Bootstrap",
    "section": "Sources of Non-Parametric Bootstrap Error",
    "text": "Sources of Non-Parametric Bootstrap Error\n\nSampling error: error from using finitely many replications\nStatistical error: error in the bootstrap sampling distribution approximation"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#the-parametric-bootstrap-1",
    "href": "slides/lecture08-1-parametric-bootstrap.html#the-parametric-bootstrap-1",
    "title": "The Parametric Bootstrap",
    "section": "The Parametric Bootstrap",
    "text": "The Parametric Bootstrap\n\nNon-Parametric Bootstrap: Resample directly from the data.\nParametric Bootstrap: Fit a model to the original data and simulate new samples, then calculate bootstrap estimates.\n\nThis lets us use additional information, such as a simulation or statistical model, instead of relying only on the empirical CDF."
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#parametric-bootstrap-scheme",
    "href": "slides/lecture08-1-parametric-bootstrap.html#parametric-bootstrap-scheme",
    "title": "The Parametric Bootstrap",
    "section": "Parametric Bootstrap Scheme",
    "text": "Parametric Bootstrap Scheme\n\n\nThe parametric bootstrap generates pseudodata using simulations from a fitted model.\n\n\n\n\nParametric Bootstrap"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#benefits-of-the-parametric-bootstrap",
    "href": "slides/lecture08-1-parametric-bootstrap.html#benefits-of-the-parametric-bootstrap",
    "title": "The Parametric Bootstrap",
    "section": "Benefits of the Parametric Bootstrap",
    "text": "Benefits of the Parametric Bootstrap\n\nCan quantify uncertainties in parameter values\nDeals better with structured data (model accounts for structure)\nCan look at statistics which are limited by resimulating from empirical CDF."
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#potential-drawbacks",
    "href": "slides/lecture08-1-parametric-bootstrap.html#potential-drawbacks",
    "title": "The Parametric Bootstrap",
    "section": "Potential Drawbacks",
    "text": "Potential Drawbacks\n\nNew source of error: model specification\nMisspecified models can completely distort estimates."
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#tide-gauge-data",
    "href": "slides/lecture08-1-parametric-bootstrap.html#tide-gauge-data",
    "title": "The Parametric Bootstrap",
    "section": "Tide Gauge Data",
    "text": "Tide Gauge Data\nDetrended San Francisco Tide Gauge Data:\n\n\nCode\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5\n)\np2 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[]\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(1, 1.7), bottom_margin=10mm, left_margin=5mm)\nplot!(size=(1000, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#parametric-bootstrap-strategy",
    "href": "slides/lecture08-1-parametric-bootstrap.html#parametric-bootstrap-strategy",
    "title": "The Parametric Bootstrap",
    "section": "Parametric Bootstrap Strategy",
    "text": "Parametric Bootstrap Strategy\n\nFit/calibrate model\nCompute statistic of interest\nRepeat \\(N\\) times:\n\nResample values from fitted model\nCalculate statistic.\n\nCompute mean/confidence intervals from distribution of bootstrapped statistics."
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#parametric-bootstrap-results",
    "href": "slides/lecture08-1-parametric-bootstrap.html#parametric-bootstrap-results",
    "title": "The Parametric Bootstrap",
    "section": "Parametric Bootstrap Results",
    "text": "Parametric Bootstrap Results\n\n\nCode\n# function to fit GEV model for each data set\ninit_θ = [1.0, 1.0, 0.0]\nlb = [0.0, 0.0, -2.0]\nub = [5.0, 10.0, 2.0]\nloglik_gev(θ) = -sum(logpdf(GeneralizedExtremeValue(θ[1], θ[2], θ[3]), dat_annmax.residual))\n\n# get estimates from observations\nrp_emp = quantile(dat_annmax.residual, 0.99)\nθ_gev = Optim.optimize(loglik_gev, lb, ub, init_θ).minimizer\n\np = histogram(dat_annmax.residual,  normalize=:pdf, xlabel=\"Annual Maximum Storm Tide (m)\", ylabel=\"Probability Density\", label=false, right_margin=5mm)\nplot!(p, GeneralizedExtremeValue(θ_gev[1], θ_gev[2], θ_gev[3]), linewidth=3, label=\"Parametric Model\", color=:orange)\nvline!(p, [rp_emp], color=:red, linewidth=3, linestyle=:dash, label=\"Empirical Return Level\")\nvline!(p, [quantile(GeneralizedExtremeValue(θ_gev[1], θ_gev[2], θ_gev[3]), 0.99)], color=:blue, linewidth=3, linestyle=:dash, label=\"Model Return Level\")\nxlims!(p, 1, 2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Fitted GEV Distribution"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#adding-bootstrap-samples",
    "href": "slides/lecture08-1-parametric-bootstrap.html#adding-bootstrap-samples",
    "title": "The Parametric Bootstrap",
    "section": "Adding Bootstrap Samples",
    "text": "Adding Bootstrap Samples\n\n\nCode\nn_boot = 1000\nboot_samp = rand(GeneralizedExtremeValue(θ_gev[1], θ_gev[2], θ_gev[3]), (nrow(dat_annmax), n_boot))\nrp_boot = mapslices(col -&gt; quantile(col, 0.99), boot_samp, dims=1)'\n\npfit = plot(GeneralizedExtremeValue(θ_gev[1], θ_gev[2], θ_gev[3]), color=:darkorange, linewidth=3, label=\"GEV Model\", xlabel=\"Annual Maximum Storm Tide (m)\", ylabel=\"Probability Density\", right_margin=5mm)\nvline!(pfit, [rp_emp], color=:black, linewidth=3, linestyle=:dash, label=\"Empirical Return Level\")\nxlims!(pfit, 1, 1.75)\nscatter!(pfit, rp_boot[:, 1], zeros(nrow(dat_annmax)), color=:orange, label=\"GEV Bootstrap Replicates\", markersize=3, alpha=0.3)\nvline!(pfit, [mean(rp_boot)], color=:orange, linewidth=3, label=\"GEV Bootstrap Estimate\")\nvline!(pfit, [2 * rp_emp - mean(rp_boot)], color=:orange, linewidth=3, linestyle=:dot, label=\"Bias-Corrected Estimate (GEV)\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Initial bootstrap sample"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#bootstrap-confidence-interval",
    "href": "slides/lecture08-1-parametric-bootstrap.html#bootstrap-confidence-interval",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap Confidence Interval",
    "text": "Bootstrap Confidence Interval\n\n\nCode\nphist = histogram(rp_boot, xlabel=\"100-Year Return Period Estimate (m)\", ylabel=\"Count\", right_margin=5mm, label=\"GEV Bootstrap Samples\", color=:darkorange, alpha=0.4)\nvline!(phist, [rp_emp], color=:black, linewidth=3, linestyle=:dash, label=\"Empirical Estimate\")\nq_boot = 2 * rp_emp .- quantile(rp_boot, [0.975, 0.025])\nvline!(phist, [mean(rp_boot)], color=:red, linestyle=:dash, linewidth=3, label=\"GEV Bootstrap Estimate\")\nvline!(phist, [2 * rp_emp - mean(rp_boot)], color=:red, linestyle=:dot, linewidth=3, label=\"Bias-Corrected GEV Estimate\")\nvspan!(phist, q_boot, linecolor=:orange, fillcolor=:orange, lw=3, alpha=0.3, fillalpha=0.3, label=\"95% GEV Bootstrap CI\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Bootstrap histogram"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#when-to-use-the-parametric-bootstrap",
    "href": "slides/lecture08-1-parametric-bootstrap.html#when-to-use-the-parametric-bootstrap",
    "title": "The Parametric Bootstrap",
    "section": "When To Use The Parametric Bootstrap?",
    "text": "When To Use The Parametric Bootstrap?\n\nReasonable to specify model (but uncertain about parameters/statistics);\nInterested in statistics where model provides needed structure (e.g. extremes, dependent data);"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#impact-of-model-choice",
    "href": "slides/lecture08-1-parametric-bootstrap.html#impact-of-model-choice",
    "title": "The Parametric Bootstrap",
    "section": "Impact of Model Choice",
    "text": "Impact of Model Choice\n\nCode\n## re-do bootstrap with lognormal distribution\n# function to fit GEV model for each data set\ninit_θ = [1.0, 1.0]\nlb = [0.0, 0.0]\nub = [5.0, 10.0]\nloglik_ln(θ) = -sum(logpdf(LogNormal(θ[1], θ[2]), dat_annmax.residual))\nθ_ln = Optim.optimize(loglik_ln, lb, ub, init_θ).minimizer\n\nboot_samp_ln = rand(LogNormal(θ_ln[1], θ_ln[2]), (nrow(dat_annmax), n_boot))\nrp_boot_ln = mapslices(col -&gt; quantile(col, 0.99), boot_samp_ln, dims=1)'\nq_boot_ln = 2 * rp_emp .- quantile(rp_boot_ln, [0.975, 0.025])\n\n## plot confidence intervals and estimates\n## GEV fit\npfit = plot(GeneralizedExtremeValue(θ_gev[1], θ_gev[2], θ_gev[3]), color=:darkorange, linewidth=3, label=\"GEV Model\", xlabel=\"Annual Maximum Storm Tide (m)\", ylabel=\"Probability Density\", right_margin=5mm)\nvline!(pfit, [rp_emp], color=:black, linewidth=3, linestyle=:dash, label=\"Empirical Return Level\")\nxlims!(pfit, 1, 1.75)\nvline!(pfit, [mean(rp_boot)], color=:orange, linewidth=3, label=false)\nvline!(pfit, [2 * rp_emp - mean(rp_boot)], color=:orange, linewidth=3, linestyle=:dot, label=false)\n## lognormal fit\nplot!(pfit, LogNormal(θ_ln[1], θ_ln[2]), color=:darkgreen, lw=3, label=\"LogNormal Model\")\nvline!(pfit, [mean(rp_boot_ln)], color=:green, linewidth=3, label=false)\nvline!(pfit, [2 * rp_emp - mean(rp_boot_ln)], color=:green, linewidth=3, linestyle=:dot, label=false)\nplot!(pfit, size=(550, 550))\n\n## GEV histogram\nphist = histogram(rp_boot, xlabel=\"100-Year Return Period Estimate (m)\", ylabel=\"Count\", right_margin=5mm, legend=false, color=:darkorange, alpha=0.4)\nvline!(phist, [2 * rp_emp - mean(rp_boot)], color=:red, linestyle=:dot, linewidth=3)\nvspan!(phist, q_boot, linecolor=:orange, fillcolor=:orange, lw=3, alpha=0.3, fillalpha=0.3)\nhistogram!(phist, rp_boot_ln, fillcolor=:green, alpha=0.3, label=\"LN Bootstrap Samples\")\nvline!(phist, [2 * rp_emp - mean(rp_boot_ln)], color=:purple, linestyle=:dot, linewidth=3)\nvspan!(phist, q_boot_ln, linecolor=:darkgreen, fillcolor=:darkgreen, lw=3, alpha=0.3, fillalpha=0.3)\nplot!(phist, size=(550, 550))\n\ndisplay(pfit)\ndisplay(phist)\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Bootstrap histogram with model mis-specification\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#streamflow-tds-regression-example",
    "href": "slides/lecture08-1-parametric-bootstrap.html#streamflow-tds-regression-example",
    "title": "The Parametric Bootstrap",
    "section": "Streamflow-TDS Regression Example",
    "text": "Streamflow-TDS Regression Example\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[S \\sim \\mathcal{N}(\\beta_0 + \\beta_1 \\log(D), \\sigma^2)\\]"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#parametric-bootstrapping",
    "href": "slides/lecture08-1-parametric-bootstrap.html#parametric-bootstrapping",
    "title": "The Parametric Bootstrap",
    "section": "Parametric Bootstrapping",
    "text": "Parametric Bootstrapping\n\nCode\n# tds_riverflow_loglik: function to compute the log-likelihood for the tds model\n# θ: vector of model parameters (coefficients β₀ and β₁ and stdev σ)\n# tds, flow: vectors of data\nfunction tds_riverflow_loglik(θ, tds, flow)\n    β₀, β₁, σ = θ # unpack parameter vector\n    μ = β₀ .+ β₁ * log.(flow) # find mean\n    ll = sum(logpdf.(Normal.(μ, σ), tds)) # compute log-likelihood\n    return ll\nend\n\nlb = [0.0, -1000.0, 1.0]\nub = [1000.0, 1000.0, 100.0]\nθ₀ = [500.0, 0.0, 50.0]\noptim_out = Optim.optimize(θ -&gt; -tds_riverflow_loglik(θ, tds.tds_mgL, tds.discharge_cms), lb, ub, θ₀)\nθ_mle = round.(optim_out.minimizer; digits=0)\n\nxspan = 0.1:0.1:52\nmodel_fit(x, θ) = θ[1] .+ θ[2] * log.(x)\nplot!(p, xspan, model_fit(xspan, θ_mle), label=\"Fitted Regression\")\nxlims!(p, 3, 55)\nplot!(p, size=(550, 500))\ndisplay(p)\n\n## get bootstrap replicates\nnboot = 1_000\nboot_samples = zeros(nrow(tds), nboot)\nboot_θ = zeros(3, nboot)\nfor i = 1:nboot\n    # resample from model\n    boot_samples[:, i] = model_fit(tds.discharge_cms, θ_mle) + rand(Normal(0, θ_mle[3]), nrow(tds))\n    # refit model\n    optim_out = Optim.optimize(θ -&gt; -tds_riverflow_loglik(θ, boot_samples[:, i], tds.discharge_cms), lb, ub, θ₀)\n    boot_θ[:, i] = optim_out.minimizer\nend\n\n\npboot = scatter(\n    tds.discharge_cms,\n    tds.tds_mgL,\n    xlabel=L\"Discharge (m$^3$/s)\",\n    ylabel=\"Total dissolved solids (mg/L)\",\n    markersize=5,\n    label=\"Observations\",\n    color=:blue,\n    xaxis=:log,\n    alpha=0.4\n)\nscatter!(pboot,\n    tds.discharge_cms,\n    boot_samples[:, 1],\n    markersize=5,\n    color=:grey,\n    label=\"Bootstrap Replicate\",\n    alpha=0.4\n)\nplot!(pboot, xspan, model_fit(xspan, θ_mle), label=\"Original Model Fit\", color=:blue, lw=3)\nplot!(pboot, xspan, model_fit(xspan, boot_θ[:, 1]), label=\"Bootstrap Model Fit\", color=:grey, lw=3)\nxlims!(pboot, 3, 55)\nplot!(pboot, size=(550, 500))\ndisplay(pboot)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Fitted regression model for the parametric model.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 6"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#parametric-bootstrap-samples",
    "href": "slides/lecture08-1-parametric-bootstrap.html#parametric-bootstrap-samples",
    "title": "The Parametric Bootstrap",
    "section": "Parametric Bootstrap Samples",
    "text": "Parametric Bootstrap Samples\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Confidence Intervals for the TDS problem from the parametric bootstrap\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\nFigure 7"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#parametric-vs.-non-parametric-bootstrap-cis",
    "href": "slides/lecture08-1-parametric-bootstrap.html#parametric-vs.-non-parametric-bootstrap-cis",
    "title": "The Parametric Bootstrap",
    "section": "Parametric vs. Non-Parametric Bootstrap CIs",
    "text": "Parametric vs. Non-Parametric Bootstrap CIs\n\nCode\n# need to get non-parametric bootstrap estimates\nnp_boot_θ = zeros(3, nboot)\nfor i = 1:nboot\n    # resample from model\n    idx = sample(1:nrow(tds), nrow(tds); replace=true)\n    # refit model\n    optim_out = Optim.optimize(θ -&gt; -tds_riverflow_loglik(θ, tds.tds_mgL[idx], tds.discharge_cms[idx]), lb, ub, θ₀)\n    np_boot_θ[:, i] = optim_out.minimizer\nend\n\n## find bias-corrected estimates and confidence intervals\nnp_boot_est = 2 * θ_mle - mean(np_boot_θ; dims=2)\nnp_boot_q = mapslices(v -&gt; quantile(v, [0.95, 0.05]), np_boot_θ; dims=2)\nnp_boot_ci = zeros(3, 2)\nfor i in eachindex(θ_mle)\n    np_boot_ci[i, :] = 2 * θ_mle[i] .- np_boot_q[i, :]\nend\n\nparnames = [\"β₀\", \"β₁\", \"σ\"]\nboot_ci_str =  string.(Int64.(round.(boot_est; digits=0)), \" (\", Int64.(round.(boot_ci[:, 1]; digits=0)), \",\", Int64.(round.(boot_ci[:, 2]; digits=0)), \")\")\nnp_boot_ci_str =  string.(Int64.(round.(np_boot_est; digits=0)), \" (\", Int64.(round.(np_boot_ci[:, 1]; digits=0)), \",\", Int64.(round.(np_boot_ci[:, 2]; digits=0)), \")\")\npretty_table(DataFrame(Parameters=parnames, NP=vec(np_boot_ci_str), Par=vec(boot_ci_str)); backend=Val(:markdown), show_subheader=false, show_row_number=false)\n\n\n\n\nTable 1\n\n\n\n\n\nParameters\nNP\nPar\n\n\n\n\nβ₀\n611 (576,643)\n610 (574,646)\n\n\nβ₁\n-112 (-123,-101)\n-112 (-128,-97)\n\n\nσ\n75 (52,97)\n74 (63,83)"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#which-bootstrap-to-use",
    "href": "slides/lecture08-1-parametric-bootstrap.html#which-bootstrap-to-use",
    "title": "The Parametric Bootstrap",
    "section": "Which Bootstrap To Use?",
    "text": "Which Bootstrap To Use?\nParametric bootstrap estimates converge faster than non-parametric estimates.\n\nIf your parametric model is “properly” specified, parametric bootstrap gives more accurate results with the same \\(n\\).\nIf the parametric model is mis-specified, you’re rapidly converging to the wrong distribution."
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#bootstrapping-residuals",
    "href": "slides/lecture08-1-parametric-bootstrap.html#bootstrapping-residuals",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrapping Residuals",
    "text": "Bootstrapping Residuals\nCan bootrap residuals from a model versus “full” parametric bootstrap:\n\nFit model (statistically or numerical).\nCalculate residuals from deterministic/expected values.\nResample residuals.\nAdd bootstrapped residuals back to model trend to create new replicates.\nRefit model to replicates."
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#bootstrap-vs.-monte-carlo",
    "href": "slides/lecture08-1-parametric-bootstrap.html#bootstrap-vs.-monte-carlo",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap vs. Monte Carlo",
    "text": "Bootstrap vs. Monte Carlo\nBootstrap “if I had a different sample (conditional on the bootstrap principle), what could I have inferred”?\nMonte Carlo: Given specification of input uncertainty, what data could we generate?"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#bootstrap-distribution-and-monte-carlo",
    "href": "slides/lecture08-1-parametric-bootstrap.html#bootstrap-distribution-and-monte-carlo",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap Distribution and Monte Carlo",
    "text": "Bootstrap Distribution and Monte Carlo\nCould we use a bootstrap distribution for MC?\n\nSure, that’s just one specification of the data-generating process.\nNothing unique or particularly rigorous in using the bootstrap for this; substituting the bootstrap principle for other assumptions."
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#bootstrap-vs.-bayes",
    "href": "slides/lecture08-1-parametric-bootstrap.html#bootstrap-vs.-bayes",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap vs. Bayes",
    "text": "Bootstrap vs. Bayes\nBootstrap: “if I had a different sample (conditional on the bootstrap principle), what could I have inferred”?\nBayesian Inference: “what different parameters could have produced the observed data”?"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#key-points-1",
    "href": "slides/lecture08-1-parametric-bootstrap.html#key-points-1",
    "title": "The Parametric Bootstrap",
    "section": "Key Points",
    "text": "Key Points\n\nBootstrap: Use the characteristics of the data to simulate new samples.\nBootstrap gives idea of sampling error in statistics (including model parameters)\nDistribution of \\(\\tilde{t} - \\hat{t}\\) approximates distribution around estimate \\(\\hat{t} - t_0\\).\nAllows us to estimate uncertainty of estimates (confidence intervals, bias, etc).\nParametric bootstrap introduces model specification error"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#bootstrap-variants",
    "href": "slides/lecture08-1-parametric-bootstrap.html#bootstrap-variants",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap Variants",
    "text": "Bootstrap Variants\n\nResample Cases (Non-Parametric)\nResample Residuals (from fitted model trend)\nSimulate from Fitted Model (Parametric)"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#which-bootstrap-to-use-1",
    "href": "slides/lecture08-1-parametric-bootstrap.html#which-bootstrap-to-use-1",
    "title": "The Parametric Bootstrap",
    "section": "Which Bootstrap To Use?",
    "text": "Which Bootstrap To Use?\nDepends on trust in model “correctness”: - Do we trust the model specification to be reasonably correct? - Do we trust that we have enough samples to recover the empirical CDF? - Do we trust the data-generating process?"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#questions-to-seed-discussion",
    "href": "slides/lecture08-1-parametric-bootstrap.html#questions-to-seed-discussion",
    "title": "The Parametric Bootstrap",
    "section": "Questions to Seed Discussion",
    "text": "Questions to Seed Discussion\n\nWhat are the practical differences between exploratory and consolidative modeling?\nWhen do you think each approach is more or less appropriate?\nHow do these approaches impact the choice of your methods?"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#next-classes",
    "href": "slides/lecture08-1-parametric-bootstrap.html#next-classes",
    "title": "The Parametric Bootstrap",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Markov Chains and Bayesian Computation\nNext Week (plan): Model Evaluation"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#assessments",
    "href": "slides/lecture08-1-parametric-bootstrap.html#assessments",
    "title": "The Parametric Bootstrap",
    "section": "Assessments",
    "text": "Assessments\n\nHomework 3: Due Friday (3/14)\nProject Proposal: Due 3/21"
  },
  {
    "objectID": "slides/lecture08-1-parametric-bootstrap.html#references-scroll-for-full-list",
    "href": "slides/lecture08-1-parametric-bootstrap.html#references-scroll-for-full-list",
    "title": "The Parametric Bootstrap",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nEfron, B. (1979). Bootstrap methods: Another look at the jackknife. Ann. Stat., 7, 1–26. https://doi.org/10.1214/aos/1176344552"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#information-and-uncertainty",
    "href": "slides/lecture11-1-information-criteria.html#information-and-uncertainty",
    "title": "Information Criteria",
    "section": "Information and Uncertainty",
    "text": "Information and Uncertainty\nKey Idea: “Information” as reduction in uncertainty from projection.\nEntropy: \\(H(p) = \\mathbb{E}[\\log(p)] = -\\sum_{i=1}^n p_i \\log p_i\\)"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#kullback-leibler-divergence",
    "href": "slides/lecture11-1-information-criteria.html#kullback-leibler-divergence",
    "title": "Information Criteria",
    "section": "Kullback-Leibler Divergence",
    "text": "Kullback-Leibler Divergence\nMeasure of “distance” between “true” distribution \\(p\\) and surrogate/projection distribution \\(q\\):\n\\[D_{KL}(p, q) = \\sum_i p_i (\\log(p_i) - \\log(q_i))\\]\nIntuitively: What is the surprise from using \\(q\\) to predict \\(p\\)?"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#entropy-and-log-score",
    "href": "slides/lecture11-1-information-criteria.html#entropy-and-log-score",
    "title": "Information Criteria",
    "section": "Entropy and Log-Score",
    "text": "Entropy and Log-Score\nSuppose we have two projections \\(q\\) and \\(r\\) for a “true” data-generating process \\(p\\):\n\\[\nD_{KL}(p, q) - D_{KL}(p, r) = \\sum_i p_i (\\log(r_i) - \\log(q_i))\n\\]"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#deviance-scale",
    "href": "slides/lecture11-1-information-criteria.html#deviance-scale",
    "title": "Information Criteria",
    "section": "Deviance Scale",
    "text": "Deviance Scale\nThis log-predictive-density score is better when larger.\nCommon to see this converted to deviance, which is \\(-2\\text{lppd}\\):\n\n\\(-1\\) reorients so smaller is better;\nMultiplied by \\(2\\) for historical reasons (cancels out the -1/2 in the Gaussian likelihood)."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#deviance-and-degrees-of-freedom",
    "href": "slides/lecture11-1-information-criteria.html#deviance-and-degrees-of-freedom",
    "title": "Information Criteria",
    "section": "Deviance and Degrees of Freedom",
    "text": "Deviance and Degrees of Freedom\n\nCode\nfunction calculate_deviance(y_gen, x_gen, y_oos, x_oos; degree=2, reg=Inf)\n    # fit model\n    lb = [-5 .+ zeros(degree); 0.01]\n    ub = [5 .+ zeros(degree); 10.0]\n    p0 = [zeros(degree); 5.0]\n\n    function model_loglik(p, y, x, reg)\n        if mean(p[1:end-1]) &lt;= reg\n            ll = 0\n            for i = 1:length(y)\n                μ = sum(x[i, 1:degree] .* p[1:end-1])\n                ll += logpdf(Normal(μ, p[end]), y[i])\n            end\n        else\n            ll = -Inf\n        end\n        return ll\n    end\n\n    result = optimize(p -&gt; -model_loglik(p, y_gen, x_gen, reg), lb, ub, p0)\n    θ = result.minimizer\n\n    function deviance(p, y_pred, x_pred)\n        dev = 0\n        for i = 1:length(y_pred)\n            μ = sum(x_pred[i, 1:degree] .* p[1:end-1])\n            dev += -2 * logpdf(Normal(μ, p[end]), y_pred[i])\n        end\n        return dev\n    end\n\n    dev_is = deviance(θ, y_gen, x_gen)\n    dev_oos = deviance(θ, y_oos, x_oos)\n    return (dev_is, dev_oos)\nend\n\n\nN_sim = 10_000\n\nfunction simulate_deviance(N_sim, N_data; reg=Inf)\n    d_is = zeros(5, 4)\n    for d = 1:5\n        dev_out = zeros(N_sim, 2)\n        for n = 1:N_sim\n            x_gen = rand(Uniform(-2, 2), (N_data, 5))\n            μ_gen = [0.1 * x_gen[i, 1] - 0.3 * x_gen[i, 2] for i in 1:N_data]\n            y_gen = rand.(Normal.(μ_gen, 1))\n\n            x_oos = rand(Uniform(-2, 2), (N_data, 5))\n            μ_oos = [0.1 * x_oos[i, 1] - 0.3 * x_oos[i, 2] for i in 1:N_data]\n            y_oos = rand.(Normal.(μ_oos, 1))\n\n            dev_out[n, :] .= calculate_deviance(y_gen, x_gen, y_oos, x_oos, degree=d, reg=reg)\n        end\n        d_is[d, 1] = mean(dev_out[:, 1])\n        d_is[d, 2] = std(dev_out[:, 1])\n        d_is[d, 3] = mean(dev_out[:, 2])\n        d_is[d, 4] = std(dev_out[:, 2])\n    end\n    return d_is\nend\n\nd_20 = simulate_deviance(N_sim, 20)\np1 = scatter(collect(1:5) .- 0.1, d_20[:, 1], yerr=d_20[:, 2], color=:blue, linecolor=:blue, lw=2, markersize=5, label=\"In Sample\", xlabel=\"Number of Predictors\", ylabel=\"Deviance\", title=L\"$N = 20$\", grid=true)\nscatter!(p1, collect(1:5) .+ 0.1, d_20[:, 3], yerr=d_20[:, 4], lw=2, markersize=5, color=:black, linecolor=:black, label=\"Out of Sample\")\nplot!(p1, size=(600, 550))\n\nd_100 = simulate_deviance(N_sim, 100)\np2 = scatter(collect(1:5) .- 0.1, d_100[:, 1], yerr=d_100[:, 2], color=:blue, linecolor=:blue, lw=2, markersize=5, label=\"In Sample\", xlabel=\"Number of Predictors\", ylabel=\"Deviance\", title=L\"$N = 100$\", grid=true)\nscatter!(p2, collect(1:5) .+ 0.1, d_100[:, 3], yerr=d_100[:, 4], lw=2, markersize=5, color=:black, linecolor=:black, label=\"Out of Sample\")\nplot!(p2, size=(600, 550))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Simulation of in vs. out of sample deviance calculations with increasing number of simulations\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#cross-validation-and-over-under-fitting",
    "href": "slides/lecture11-1-information-criteria.html#cross-validation-and-over-under-fitting",
    "title": "Information Criteria",
    "section": "Cross-Validation and Over-/Under-Fitting",
    "text": "Cross-Validation and Over-/Under-Fitting\nCross-Validation and related measures cannot detect overfitting when information leaks from the full dataset to the trained model."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#cross-validation-and-over-under-fitting-1",
    "href": "slides/lecture11-1-information-criteria.html#cross-validation-and-over-under-fitting-1",
    "title": "Information Criteria",
    "section": "Cross-Validation and Over-/Under-Fitting",
    "text": "Cross-Validation and Over-/Under-Fitting\nIn other words:\n\nDo not select features based on the entire dataset, then do C-V on that model.\nMust treat variable/feature selection as part of the C-V process.\nA priori model specification (before looking at the data) avoids this problem."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#expected-out-of-sample-predictive-accuracy",
    "href": "slides/lecture11-1-information-criteria.html#expected-out-of-sample-predictive-accuracy",
    "title": "Information Criteria",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nWe want to compute the expected out-of-sample log-predictive density\n\\[\n\\begin{align}\n\\text{elpd} &= \\text{expected log-predictive density for } \\tilde{y}_i \\\\\n&= \\mathbb{E}_P \\left[\\log p(\\tilde{y}_i)\\right] \\\\\n&= \\int \\log\\left(p(\\tilde{y}_i)\\right) P(\\tilde{y}_i)\\,d\\tilde{y}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#expected-out-of-sample-predictive-accuracy-1",
    "href": "slides/lecture11-1-information-criteria.html#expected-out-of-sample-predictive-accuracy-1",
    "title": "Information Criteria",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nWhat is the challenge?\n\nWe don’t know \\(P\\) (the distribution of new data)!"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#expected-out-of-sample-predictive-accuracy-2",
    "href": "slides/lecture11-1-information-criteria.html#expected-out-of-sample-predictive-accuracy-2",
    "title": "Information Criteria",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nWe need some measure of the error induced by using an approximating distribution \\(Q\\) from some model.\n\\[\\begin{align}\nQ(\\tilde{y}_i) &= \\mathbb{E}_\\theta \\left[p(\\tilde{y}_i | \\theta)\\right] \\\\\n&= \\int p(\\tilde{y_i} | \\theta) p(\\theta)\\,d\\theta.\n\\end{align}\\]\nHere \\(\\theta\\) can be the MLE or integrated over the sampling or posterior."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#information-criteria-1",
    "href": "slides/lecture11-1-information-criteria.html#information-criteria-1",
    "title": "Information Criteria",
    "section": "Information Criteria",
    "text": "Information Criteria\n“Information criteria” refers to a category of estimators of prediction error.\nThe idea: estimate predictive error using the fitted model."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#information-criteria-overview",
    "href": "slides/lecture11-1-information-criteria.html#information-criteria-overview",
    "title": "Information Criteria",
    "section": "Information Criteria Overview",
    "text": "Information Criteria Overview\nThere is a common framework for all of these:\n\\[\\widehat{\\text{elpd}} = \\underbrace{\\log p(y | \\theta, \\mathcal{M})}_{\\text{in-sample log-predictive density}} - \\underbrace{d(\\mathcal{M})}_{\\text{penalty for degrees of freedom}}\\]"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#akaike-information-criterion-aic",
    "href": "slides/lecture11-1-information-criteria.html#akaike-information-criterion-aic",
    "title": "Information Criteria",
    "section": "Akaike Information Criterion (AIC)",
    "text": "Akaike Information Criterion (AIC)\nThe “first” information criterion that most people see.\nUses a point estimate (the maximum-likelihood estimate \\(\\hat{\\theta}_\\text{MLE}\\)) to compute the log-predictive density for the data, corrected by the number of parameters \\(k\\):\n\\[\\widehat{\\text{elpd}}_\\text{AIC} = \\log p(y | \\hat{\\theta}_\\text{MLE}) - k.\\]"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-formula",
    "href": "slides/lecture11-1-information-criteria.html#aic-formula",
    "title": "Information Criteria",
    "section": "AIC Formula",
    "text": "AIC Formula\nThe AIC is defined as \\(-2\\widehat{\\text{elpd}}_\\text{AIC}\\).\nDue to this convention, lower AICs are better (they correspond to a higher predictive skill)."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-correction-term",
    "href": "slides/lecture11-1-information-criteria.html#aic-correction-term",
    "title": "Information Criteria",
    "section": "AIC Correction Term",
    "text": "AIC Correction Term\nIn the case of a model with normal sampling distributions, uniform priors, and sample size \\(N &gt;&gt; k\\), \\(k\\) is the asymptotically “correct” bias term (there are modified corrections for small sample sizes).\nHowever, with more informative priors and/or hierarchical models, the bias correction \\(k\\) is no longer quite right, as there is less “freedom” associated with each parameter."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-and-deviance-example",
    "href": "slides/lecture11-1-information-criteria.html#aic-and-deviance-example",
    "title": "Information Criteria",
    "section": "AIC and Deviance Example",
    "text": "AIC and Deviance Example\n\nCode\nscatter!(p1, collect(1:5) .+ 0.2, d_20[:, 1] .+ 2 * (2:6), yerr=d_20[:, 2], lw=2, markersize=5, color=:red, linecolor=:black, label=\"AIC\")\n\nscatter!(p2, collect(1:5) .+ 0.2, d_100[:, 1] .+ 2 * (2:6), yerr=d_20[:, 2], lw=2, markersize=5, color=:red, linecolor=:black, label=\"AIC\")\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Simulation of in vs. out of sample AIC calculations with increasing number of simulations\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-storm-surge-example",
    "href": "slides/lecture11-1-information-criteria.html#aic-storm-surge-example",
    "title": "Information Criteria",
    "section": "AIC: Storm Surge Example",
    "text": "AIC: Storm Surge Example\n\n\nQuestion: Do climate oscillations (e.g. the Pacific Decadal Oscillation) result in variations in tidal extremes at the San Francisco tide gauge station?\n\n\n\nCode\n# load SF tide gauge data\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(DataFrames.transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\n\ndat_annmax.residual = dat_annmax.residual  # convert to m\n\n# make plots\npsurge = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide (mm)\",\n    label=false,\n    marker=:circle,\n    markersize=5\n)\n\nplot!(psurge, size=(600, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: San Francisco tide gauge data"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-storm-surge-example-1",
    "href": "slides/lecture11-1-information-criteria.html#aic-storm-surge-example-1",
    "title": "Information Criteria",
    "section": "AIC: Storm Surge Example",
    "text": "AIC: Storm Surge Example\nModels:\n\nStationary (“null”) model, \\(y_t \\sim \\text{GEV}(\\mu, \\sigma, \\xi);\\)\nTime nonstationary (“null-ish”) model, \\(y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 t, \\sigma, \\xi);\\)\nPDO nonstationary model, \\(y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 \\text{PDO}_t, \\sigma, \\xi)\\)"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-example",
    "href": "slides/lecture11-1-information-criteria.html#aic-example",
    "title": "Information Criteria",
    "section": "AIC Example",
    "text": "AIC Example\n\n\nCode\nstat_mle = [1258.71, 56.27, 0.017]\nstat_ll = -707.67\nnonstat_mle = [1231.58, 0.42, 52.07, 0.075]\nnonstat_ll = -702.45\npdo_mle = [1255.87, -12.39, 54.73, 0.033]\npdo_ll = -705.24\n\n# compute AIC values\nstat_aic = stat_ll - 3\nnonstat_aic = nonstat_ll - 4\npdo_aic = pdo_ll - 4\n\nmodel_aic = DataFrame(Model=[\"Stationary\", \"Time\", \"PDO\"], LogLik=trunc.(Int64, round.([stat_ll, nonstat_ll, pdo_ll]; digits=0)), AIC=trunc.(Int64, round.(-2 * [stat_aic, nonstat_aic, pdo_aic]; digits=0)))\n\n\n3×3 DataFrame\n\n\n\nRow\nModel\nLogLik\nAIC\n\n\n\nString\nInt64\nInt64\n\n\n\n\n1\nStationary\n-708\n1421\n\n\n2\nTime\n-702\n1413\n\n\n3\nPDO\n-705\n1418"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-is-efficient-not-consistent",
    "href": "slides/lecture11-1-information-criteria.html#aic-is-efficient-not-consistent",
    "title": "Information Criteria",
    "section": "AIC is Efficient, not Consistent",
    "text": "AIC is Efficient, not Consistent\n\nEfficiency: Metric converges to K-L divergence/LOO-CV (Stone, 1977)\nConsistency: Metric will tend to choose the “true” model if it is included in the set.\n\nThis means AIC selects for predictive performance, not “truth”.\nAIC can tend to overfit due to this prioritization of efficiency."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-for-small-samples",
    "href": "slides/lecture11-1-information-criteria.html#aic-for-small-samples",
    "title": "Information Criteria",
    "section": "AIC for Small Samples",
    "text": "AIC for Small Samples\nAIC will tend to overfit (more parameters ⇒ more variance), penalty term isn’t strong enough to compensate.\n\\[\\text{AIC}_c = \\text{AIC} + \\frac{2k^2 + 2k}{n - k - 1},\\]\nwhere\n\n\\(k = \\text{dim}(\\mathcal{M})\\)\n\\(n\\) is the sample size."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-interpretation",
    "href": "slides/lecture11-1-information-criteria.html#aic-interpretation",
    "title": "Information Criteria",
    "section": "AIC Interpretation",
    "text": "AIC Interpretation\nAbsolute AIC values have no meaning, only the differences \\(\\Delta_i = \\text{AIC}_i - \\text{AIC}_\\text{min}\\).\nSome basic rules of thumb (from Burnham & Anderson (2004)):\n\n\\(\\Delta_i &lt; 2\\) means the model has “strong” support across \\(\\mathcal{M}\\);\n\\(4 &lt; \\Delta_i &lt; 7\\) suggests “less” support;\n\\(\\Delta_i &gt; 10\\) suggests “weak” or “no” support."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#model-averaging-vs.-selection",
    "href": "slides/lecture11-1-information-criteria.html#model-averaging-vs.-selection",
    "title": "Information Criteria",
    "section": "Model Averaging vs. Selection",
    "text": "Model Averaging vs. Selection\nModel averaging can sometimes be beneficial vs. model selection.\nModel selection can introduce bias from the selection process (this is particularly acute for stepwise selection due to path-dependence)."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-and-model-evidence",
    "href": "slides/lecture11-1-information-criteria.html#aic-and-model-evidence",
    "title": "Information Criteria",
    "section": "AIC and Model Evidence",
    "text": "AIC and Model Evidence\n\\(\\exp(-\\Delta_i/2)\\) can be thought of as a measure of the likelihood of the model given the data \\(y\\).\nThe ratio \\[\\exp(-\\Delta_i/2) / \\exp(-\\Delta_j/2)\\] can approximate the relative evidence for \\(M_i\\) versus \\(M_j\\)."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-and-model-averaging",
    "href": "slides/lecture11-1-information-criteria.html#aic-and-model-averaging",
    "title": "Information Criteria",
    "section": "AIC and Model Averaging",
    "text": "AIC and Model Averaging\nThis gives rise to the idea of Akaike weights: \\[w_i = \\frac{\\exp(-\\Delta_i/2)}{\\sum_{m=1}^M \\exp(-\\Delta_m/2)}.\\]\nModel projections can then be weighted based on \\(w_i\\), which can be interpreted as the probability that \\(M_i\\) is the best (in the sense of approximating the “true” predictive distribution) model in \\(\\mathcal{M}\\)."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#bayesian-information-criteria-bic",
    "href": "slides/lecture11-1-information-criteria.html#bayesian-information-criteria-bic",
    "title": "Information Criteria",
    "section": "Bayesian Information Criteria (BIC)",
    "text": "Bayesian Information Criteria (BIC)\n\\[\\text{BIC} = -2 \\left(\\log p(y | \\hat{\\theta}_\\text{MLE})\\right) + k\\log n\\]\nApproximation of log-marginal likelihood \\[\\log p(\\mathcal{M}) = \\log \\int_\\theta p(y | \\theta, \\mathcal{M}) p(\\theta | \\mathcal{M}) d\\theta\\] under a whole host of assumptions related to large-sample approximation (so priors don’t matter).\nNote that \\(\\log p(\\mathcal{M})\\) is the prior predictive density, which is why BIC penalizes model complexity more than AIC."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#bic-for-model-comparison",
    "href": "slides/lecture11-1-information-criteria.html#bic-for-model-comparison",
    "title": "Information Criteria",
    "section": "BIC For Model Comparison",
    "text": "BIC For Model Comparison\nWhen comparing two models \\(\\mathcal{M}_1\\), \\(\\mathcal{M}_2\\), get an approximation of the log-Bayes Factor (BF):\n\\[\\Delta \\text{BIC} = \\text{BIC}_1 - \\text{BIC}_2 \\approx \\log \\left(\\frac{p(\\mathcal{M}_1)}{p(\\mathcal{M_2})}\\right)\\]\nThis means that \\(\\Delta \\text{BIC}\\) gives an approximation of the posterior model probabilities across a model set assuming equal prior probabilities."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#bic-vs.-aic",
    "href": "slides/lecture11-1-information-criteria.html#bic-vs.-aic",
    "title": "Information Criteria",
    "section": "BIC vs. AIC",
    "text": "BIC vs. AIC\n\nBIC tends to select more parsimonious models due to stronger penalty;\nAIC will tend to overfit, BIC to underfit.\nBIC is consistent but not efficient.\n\n\nBIC vs. AIC is analogous to the tradeoff between causal vs. predictive analyses. Generally not coherent to use both for the same problem."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#other-information-criteria-1",
    "href": "slides/lecture11-1-information-criteria.html#other-information-criteria-1",
    "title": "Information Criteria",
    "section": "Other Information Criteria",
    "text": "Other Information Criteria\nFollow the same pattern: compute \\(\\text{elpd}\\) based on some estimate and penalize for model degrees of freedom."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#deviance-information-criteria-dic",
    "href": "slides/lecture11-1-information-criteria.html#deviance-information-criteria-dic",
    "title": "Information Criteria",
    "section": "Deviance Information Criteria (DIC)",
    "text": "Deviance Information Criteria (DIC)\nThe Deviance Information Criterion (DIC) is a more Bayesian generalization of AIC which uses the posterior mean \\[\\hat{\\theta}_\\text{Bayes} = \\mathbb{E}\\left[\\theta | y\\right]\\] and a bias correction derived from the data."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#dic-formula",
    "href": "slides/lecture11-1-information-criteria.html#dic-formula",
    "title": "Information Criteria",
    "section": "DIC Formula",
    "text": "DIC Formula\n\\[\\widehat{\\text{elpd}}_\\text{DIC} = \\log p(y | \\hat{\\theta}_\\text{Bayes}) - d_{\\text{DIC}},\\] where \\[d_\\text{DIC} = 2\\left(\\log p(y | \\hat{\\theta}_\\text{Bayes}) - \\mathbb{E}_\\text{post}\\left[\\log p(y | \\theta)\\right]\\right).\\]\nThen, as with AIC, \\[\\text{DIC} = -2\\widehat{\\text{elpd}}_\\text{DIC}.\\]"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#dic-effective-number-of-parameters",
    "href": "slides/lecture11-1-information-criteria.html#dic-effective-number-of-parameters",
    "title": "Information Criteria",
    "section": "DIC: Effective Number of Parameters",
    "text": "DIC: Effective Number of Parameters\nWhat is the meaning of \\(p_\\text{DIC}\\)?\n\nThe difference between the average log-likelihood (across parameters) and the log-likelihood at a parameter average measures “degrees of freedom”.\nThe DIC adjustment assumes independence of residuals for fixed \\(\\theta\\)."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#aic-vs.-dic",
    "href": "slides/lecture11-1-information-criteria.html#aic-vs.-dic",
    "title": "Information Criteria",
    "section": "AIC vs. DIC",
    "text": "AIC vs. DIC\nAIC and DIC often give similar results, but don’t have to.\nThe key difference is the impact of priors on parameter estimation and model degrees of freedom."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#watanabe-akaike-information-criterion-waic",
    "href": "slides/lecture11-1-information-criteria.html#watanabe-akaike-information-criterion-waic",
    "title": "Information Criteria",
    "section": "Watanabe-Akaike Information Criterion (WAIC)",
    "text": "Watanabe-Akaike Information Criterion (WAIC)\n\\[\\widehat{\\text{elpd}}_\\text{WAIC} = \\sum_{i=1}^n \\log \\int p(y_i | \\theta) p_\\text{post}(\\theta)\\,d\\theta - d_{\\text{WAIC}},\\]\nwhere \\[d_\\text{WAIC} = \\sum_{i=1}^n \\text{Var}_\\text{post}\\left(\\log p(y_i | \\theta)\\right).\\]"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#waic-correction-factor",
    "href": "slides/lecture11-1-information-criteria.html#waic-correction-factor",
    "title": "Information Criteria",
    "section": "WAIC Correction Factor",
    "text": "WAIC Correction Factor\n\\(p_\\text{WAIC}\\) is an estimate of the number of “unconstrained” parameters in the model.\n\nA parameter counts as 1 if its estimate is “independent” of the prior;\nA parameter counts as 0 if it is fully constrained by the prior.\nA parameter gives a partial value if both the data and prior are informative."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#waic-vs.-aic-and-dic",
    "href": "slides/lecture11-1-information-criteria.html#waic-vs.-aic-and-dic",
    "title": "Information Criteria",
    "section": "WAIC vs. AIC and DIC",
    "text": "WAIC vs. AIC and DIC\n\nWAIC can be viewed as an approximation to leave-one-out CV, and averages over the entire posterior, vs. AIC and DIC which use point estimates.\nBut it doesn’t work well with highly structured data; no real alternative to more clever uses of Bayesian cross-validation."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#key-takeaways",
    "href": "slides/lecture11-1-information-criteria.html#key-takeaways",
    "title": "Information Criteria",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nLOO-CV is ideal for navigating bias-variance tradeoff but can be computationally prohibitive.\nInformation Criteria are an approximation to LOO-CV based on “correcting” for model complexity.\nApproximation to out of sample predictive error as a penalty for potential to overfit.\nSome ICs approximate K-L Divergence/LOO-CV, others approximate marginal likelihood. Different implications for predictive vs. causal modeling."
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#next-classes",
    "href": "slides/lecture11-1-information-criteria.html#next-classes",
    "title": "Information Criteria",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Modeling Extreme Values"
  },
  {
    "objectID": "slides/lecture11-1-information-criteria.html#references-1",
    "href": "slides/lecture11-1-information-criteria.html#references-1",
    "title": "Information Criteria",
    "section": "References",
    "text": "References\n\n\n\n\nBurnham, K. P., & Anderson, D. R. (2004). Multimodel Inference: Understanding AIC and BIC in Model Selection. Sociol. Methods Res., 33, 261–304. https://doi.org/10.1177/0049124104268644\n\n\nStone, M. (1977). An asymptotic equivalence of choice of model by cross-validation and Akaike’s criterion. J. R. Stat. Soc. Series B Stat. Methodol., 39, 44–47. https://doi.org/10.1111/j.2517-6161.1977.tb01603.x"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Website",
    "section": "",
    "text": "This website contains course materials for the Spring 2026 edition of Environmental Data Analysis and Simulation, taught by Vivek Srikrishnan at Cornell University."
  },
  {
    "objectID": "about.html#acknowledgements",
    "href": "about.html#acknowledgements",
    "title": "About This Website",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMuch of the material for this course has evolved from discussions and work with Klaus Keller, Tony Wong, Casey Helgeson, Ben Seiyon Lee and James Doss-Gollin. Many thanks also to Andrew Gelman, Richard McElreath and Christian Robert, whose work heavily inspired many aspects of this course and which I refer back to regularly. Kieran Healy also wrote an outstanding book on Data Visualization which I learned a great deal from and still reference.\nThe layout for this site was also inspired by and draws from STA 210 at Duke University and Andrew Heiss’s course materials at Georgia State."
  },
  {
    "objectID": "about.html#tools-and-generation-workflow",
    "href": "about.html#tools-and-generation-workflow",
    "title": "About This Website",
    "section": "Tools and Generation Workflow",
    "text": "Tools and Generation Workflow\nThis website was built with Quarto, which allows me to integrate Julia code and output with the web content, pdfs, and slides in an amazingly clean fashion, while simplifying the process of generation. All materials can be generated through a simple workflow from the [GitHub Repository]."
  },
  {
    "objectID": "project/report.html",
    "href": "project/report.html",
    "title": "Project Report Instructions",
    "section": "",
    "text": "These are the instructions for your final project report. The report should be submitted on Gradescope during finals week, but no later than May 17, 2024.",
    "crumbs": [
      "Report"
    ]
  },
  {
    "objectID": "project/report.html#report-guidelines",
    "href": "project/report.html#report-guidelines",
    "title": "Project Report Instructions",
    "section": "Report Guidelines",
    "text": "Report Guidelines\nThe proposal should have the following components with 1 inch margins (on all sides) and at least 11 point font. Some of this may be repeated from your proposal and/or simulation study; this is fine.\n\nBackground: Provide background on your project topic and the relevant environmental system. Why is this topic interesting or valuable to understand? What are you trying to understand or what do you hyptohesize?\nData and Models: What data are you using? Provide exploratory plots. What model(s) are you using to explore the data and your hypotheses? Include details about the probability or statistical assumptions your model is making and how well-founded you think these are in light of the system and the data.\nResults: What have you learned from your analysis? Were your hypotheses well founded? What visual or quantative evidence are you using to support these conclusions?\nDiscussion: Reflect on the data and models that you used. How well suited were they for the analysis? In retrospect, what might have been better? How might they have biased or influenced your results?\nReferences: Make sure to include any external sources referenced during the course of completing your project.\n\nYou do not need to include any code in your reports, but if you do, please reserve this for an appendix at the end of the report, not mixed in with the above sections. If you generated a figure or fit a model using the code, include the figure along with the relevant text but do not include code. There is no page minimum or maximum on the report, but it should include, clearly, all of the above content.",
    "crumbs": [
      "Report"
    ]
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "This section contains examples of Julia code or other markup that you can use as a reference throughout the semester. Please feel free to ask questions if you don’t understand any of the syntax or why something works the way it does!",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This is a 3 credit course offered as an elective.\n\n\n\n\n\n Vivek Srikrishnan\n vs498@cornell.edu\n 318 Riley-Robb Hall\n\n\n\n\n\n\n Shikhar Prakash\n sp868@cornell.edu\n TBD\n\n\n\n\n\n\n MWF\n 11:15-12:05\n 401 Riley-Robb Hall\n\n\n\n\n\n\nUnderstanding data is an increasingly integral part of working with environmental systems. Data analysis is a critical part of understanding system dynamics and projecting future conditions and outcomes. This course will provide an overview of a generative approach to environmental data analysis, which uses simulation and assessments of predictive performance to provide insight into the structure of data and its data-generating process. We will discuss exploratory analysis and visualization, model development and fitting, uncertainty quantification, and model assessment. The goal is to provide students with a framework and an initial toolkit of methods that they can use to formulate and update hypotheses about data and models. Students will actively analyze and use real data from a variety of environmental systems.\nIn particular, over the course of the semester, we will:\n\nconduct exploratory analyses of environmental datasets;\ndiscuss best practices for and complexities of data visualization;\ncalibrate statistical and process-based numerical models using environmental data;\nuse simulations from calibrated models to identify key sources of uncertainty and model error;\nassess model fit and adequacy through predictive ability."
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "",
    "text": "This is a 3 credit course offered as an elective.\n\n\n\n\n\n Vivek Srikrishnan\n vs498@cornell.edu\n 318 Riley-Robb Hall\n\n\n\n\n\n\n Shikhar Prakash\n sp868@cornell.edu\n TBD\n\n\n\n\n\n\n MWF\n 11:15-12:05\n 401 Riley-Robb Hall\n\n\n\n\n\n\nUnderstanding data is an increasingly integral part of working with environmental systems. Data analysis is a critical part of understanding system dynamics and projecting future conditions and outcomes. This course will provide an overview of a generative approach to environmental data analysis, which uses simulation and assessments of predictive performance to provide insight into the structure of data and its data-generating process. We will discuss exploratory analysis and visualization, model development and fitting, uncertainty quantification, and model assessment. The goal is to provide students with a framework and an initial toolkit of methods that they can use to formulate and update hypotheses about data and models. Students will actively analyze and use real data from a variety of environmental systems.\nIn particular, over the course of the semester, we will:\n\nconduct exploratory analyses of environmental datasets;\ndiscuss best practices for and complexities of data visualization;\ncalibrate statistical and process-based numerical models using environmental data;\nuse simulations from calibrated models to identify key sources of uncertainty and model error;\nassess model fit and adequacy through predictive ability."
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter completing this class, students will be able to:\n\nconduct exploratory analyses of data, including creating, interpreting, and critiquing data visualizations;\ncalibrate environmental models to observations, including missing data;\nquantify and propagate uncertainty using simulation methods such as the bootstrap and Monte Carlo;\nassess model adequacy and performance using predictive simulations;\nevaluate evidence for and against hypotheses about environmental systems using model simulations."
  },
  {
    "objectID": "syllabus.html#prerequisites-preparation",
    "href": "syllabus.html#prerequisites-preparation",
    "title": "Syllabus",
    "section": "Prerequisites & Preparation",
    "text": "Prerequisites & Preparation\nThe following courses/material would be ideal preparation:\n\nOne course in programming (e.g. CS 1110, 1112 or ENGRD/CEE 3200)\nOne course in probability or statistics (ENGRD 2700, CEE 3040, or equivalent)\n\nIn the absence of one or more these prerequisites, you can seek the permission of instructor.\n\n\n\n\n\n\nTipWhat If My Programming or Stats Skills Are Rusty?\n\n\n\nIf your programming or statistics skills are a little rusty, don’t worry! We will review concepts and build skills as needed."
  },
  {
    "objectID": "syllabus.html#typical-topics",
    "href": "syllabus.html#typical-topics",
    "title": "Syllabus",
    "section": "Typical Topics",
    "text": "Typical Topics\n\nIntroduction to exploratory data analysis;\nPrinciples of data visualization;\nProbability models for data;\nExtreme values;\nMissing data;\nModel fitting;\nUncertainty quantification with the bootstrap and Monte Carlo;\nModel assessment and comparison."
  },
  {
    "objectID": "syllabus.html#course-meetings",
    "href": "syllabus.html#course-meetings",
    "title": "Syllabus",
    "section": "Course Meetings",
    "text": "Course Meetings\nThis course meets MWF from 11:15-12:05 in 401 Riley-Robb. In addition to the course meetings (a total of 42 lectures, 50 minutes each), the final project will be due during the university finals period. Students can expect to devote, on average, 6 hours of effort during the exam period."
  },
  {
    "objectID": "syllabus.html#course-philosophy-and-expectations",
    "href": "syllabus.html#course-philosophy-and-expectations",
    "title": "Syllabus",
    "section": "Course Philosophy and Expectations",
    "text": "Course Philosophy and Expectations\nThe goal of our course is to help you gain competancy and knowledge in the area of data analysis. This involves a dual responsibility on the part of the instructor and the student. As the instructor, my responsibility is to provide you with a structure and opportunity to learn. To this end, I will commit to:\n\nprovide organized and focused lectures, in-class activities, and assignments;\nencourage students to regularly evaluate and provide feedback on the course;\nmanage the classroom atmosphere to promote learning;\nschedule sufficient out-of-class contact opportunities, such as office hours;\nallow adequate time for assignment completion;\nmake lecture materials, class policies, activities, and assignments accessible to students.\n\nI encourage you to discuss any concerns with me during office hours or through a course communications channel! Please let me know if you do not feel that I am holding up my end of the bargain.\nStudents can optimize their performance in the course by:\n\nattending all lectures;\ndoing any required preparatory work before class;\nactively participating in online and in-class discussions;\nbeginning assignments and other work early;\nand attending office hours as needed."
  },
  {
    "objectID": "syllabus.html#textbooks-and-course-materials",
    "href": "syllabus.html#textbooks-and-course-materials",
    "title": "Syllabus",
    "section": "Textbooks and Course Materials",
    "text": "Textbooks and Course Materials\nThere is no required text for this class, and all course materials will be made available on the course website or through the Cornell library. However, the following books might be useful as a supplement to/expansion on the topics covered in class:\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). http://www.stat.columbia.edu/~gelman/book/BDA3.pdf\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.). https://xcelab.net/rm/\nD’Agostini, G. (2003). Bayesian Reasoning in Data Analysis: A Critical Introduction.\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and Other Stories. https://avehtari.github.io/ROS-Examples/index.html"
  },
  {
    "objectID": "syllabus.html#community",
    "href": "syllabus.html#community",
    "title": "Syllabus",
    "section": "Community",
    "text": "Community\n\nDiversity and Inclusion\nOur goal in this class is to foster an inclusive learning environment and make everyone feel comfortable in the classroom, regardless of social identity, background, and specific learning needs. As engineers, our work touches on many critical aspects of society, and questions of inclusion and social justice cannot be separated from considerations of systems analysis, objective selection, risk analysis, and trade-offs.\nIn all communications and interactions with each other, members of this class community (students and instructors) are expected to be respectful and inclusive. In this spirit, we ask all participants to:\n\nshare their experiences, values, and beliefs;\nbe open to and respectful of the views of others; and\nvalue each other’s opinions and communicate in a respectful manner.\n\nPlease let me know if you feel any aspect(s) of class could be made more inclusive. Please also share any preferred name(s) and/or your pronouns with me if you wish: I use he/him/his, and you can refer to me either as Vivek or Prof. Srikrishnan.\nPlease be professional and courteous on all course interactions and platforms, and (except in designated off-topic boards or forums) please keep all online discussion relevant to the course. We do not anticipate this as a problem given our experience; almost all students in almost all classes meet these expectations. However, even a single incident can do serious damage to the learning environment and the well-being of your fellow students.\nSexually explicit, harassing, threatening, bullying, trolling, racist, sexist, homophobic, transphobic, or otherwise grossly unprofessional content will be removed. Anyone behaving in these fashions or posting such content will be blocked/banned from the appropriate platform and may be given an F if they are consistently disruptive.\n\n\n\n\n\n\nImportantPlease, Be Excellent To Teach Other\n\n\n\nWe all make mistakes in our communications with one another, both when speaking and listening. Be mindful of how spoken or written language might be misunderstood, and be aware that, for a variety of reasons, how others perceive your words and actions may not be exactly how you intended them. At the same time, it is also essential that we be respectful and interpret each other’s comments and actions in good faith.\n\n\n\n\nStudent Accomodations\nLet me know if you have any access barriers in this course, whether they relate to course materials, assignments, or communications. If any special accomodations would help you navigate any barriers and improve your chances of success, please exercise your right to those accomodations and reach out to me as early as possible with your Student Disability Services (SDS) accomodation letter. This will ensure that we have enough time to make appropriate arrangements.\nIf you need more immediate accomodations, but do not yet have a letter, please let me know and then follow up with SDS.\n\n\nCourse Communications\nMost course communications will occur via Ed Discussion. Public Ed posts are generally preferred to private posts or emails, as other students can benefit from the discussions. If you would like to discuss something privately, please do reach out through email or a private Ed post (which will only be viewable by you and the course staff).\nAnnouncements will be made on the course website and in Ed. Emergency announcements will also be made on Canvas.\n\n\n\n\n\n\nImportantEd Tips\n\n\n\n\nDo not take screenshots of code. I will not respond. Screenshots can be difficult to read and limit accessibility. Put your code on GitHub, share the link, and point to specific line numbers if relevant, or provide a simple, self-contained example of the problem you’re running into.\nIf you wait until the day an assignment is due (or even late the previous night) to ask a question on Ed, there is a strong chance that I will not see your post prior to the deadline.\nIf you see unanswered questions and you have some insight, please answer! This class will work best when we all work together as a community.\n\n\n\n\n\nMental Health Resources\nWe all have to take care of our mental health, just as we would our physical health. As a student, you may experience a range of issues which can negatively impact your mental health. Please do not ignore any of these stressors, or feel like you have to navigate these challenges alone! You are part of a community of students, faculty, and staff, who have a responsibility to look for one another’s well-being. If you are struggling with managing your mental health, or if you believe a classmate may be struggling, please reach out to the course instructor, the TA, or, for broader support, please take advantage of Cornell’s mental health resources.\n\n\n\n\n\n\nImportantMental Health And This Class\n\n\n\nI am not a trained counselor, but I am here to support you in whatever capacity we can. You should never feel that you need to push yourself past your limits to complete any assignment for this class or any other. If we need to make modifications to the course or assignment schedule, you can certainly reach out to me, and all relevant discussions will be kept strictly confidential."
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\nMany policies below (including grading policies) are broken out and discussed further on the course website. Lack of familiarity with any of these policies is not an excuse for violating any of them.\n\nAttendance\nAttendance is not required, but in general, students who attend class regularly will do better and get more out of the class than students who do not. Your class participation grade will reflect both the quantity and quality of your participation, only some of which can occur asynchronously. I will put as many course materials, such as lecture notes and announcements, as possible online, but viewing materials online is not the same as active participation and engagement. Life happens, of course, and this may lead you to miss class. Let me know if you need any appropriate arrangements ahead of time.\n\n\n\n\n\n\nImportantWhat If I’m Sick?\n\n\n\nPlease stay home if you’re feeling sick! This is beneficial for both for your own recovery and the health and safety of your classmates. We will also make any necessary arrangements for you to stay on top of the class material and if whatever is going on will negatively impact your grade, for example by causing you to be unable to submit an assignment on time.\n\n\n\n\nMask Policies\nPlease stay home and rest if you have symptoms of COVID-19 or any other respiratory illness. No masking will be required, but please be respectful of others who may wear masks or take other precautions to avoid illness. This policy may change if there is another outbreak of COVID-19 (or other illness), but will be kept consistent with broader Cornell mask policies."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\n\n\n\n\n\n\nTL;DR: Don’t cheat, copy, or plagiarize!\n\n\n\nThis class is designed to encourage collaboration, and students are encouraged to discuss their work with other students. However, I expect students to abide by the Cornell University Code of Academic Integrity in all aspects of this class. All work submitted must represent the students’ own work and understanding, whether individually or as a group (depending on the particulars of the assignment). This includes analyses, code, software runs, and reports. Engineering as a profession relies upon the honesty and integrity of its practitioners (see e.g. the American Society for Civil Engineers’ Code of Ethics).\nViolations of the academic integrity policies below\n\n\n\n\n\n\nTipWhat If I’m Unsure About The Policies?\n\n\n\nIf you are unsure about whether a particular action (such as a use of external resources) is permittable under this policy, please consult with Prof. Srikrishnan before using them and particularly before submitting anything. If you realize after the fact (but before submitting, ideally) that you may have unintentionally violated the policy, it is your responsibility to let Prof. Srikrishnan know ASAP so we can figure out a solution.\n\n\n\nExternal Resources\nThe collaborative environment in this class should not be viewed as an invitation for plagiarism. Plagiarism occurs when a writer intentionally misrepresents another’s words or ideas (including code!) as their own without acknowledging the source. All external resources1 which are consulted while working on an assignment should be referenced, including other students and faculty with whom the assignment is discussed. You will never be penalized for consulting an external source for help and referencing it, but plagiarism will result in a zero for that assignment as well as the potential for your case to be passed on for additional disciplinary action.\n1 An external resource is one which was not provided by the instructor as part of the course material; these policy does not apply to lecture notes, assignments, or readings from the current course offering.An exception is for homework, lab, or quiz solutions from previous offerings of this course. The use of these is strictly prohibited. You may not consult these solutions, read them, or ask students who took the course previously for help.\n\n\n\n\n\n\nWarningWhy Can’t I Consult Old Solutions?\n\n\n\nThe core problem is that relying on previous solutions (or prior knowledge of the course from others) does not allow you to practice the course material and makes feedback and assessment of your work meaningless. It also is unfair to others in the course who may not have access to these materials.\nThis is fundamentally different from working with your classmates, who are also trying out the problems for the first time.\n\n\n\n\nAI/ML Resource Policy\nLarge language models (LLMs), such as GPT, and other generative AI models are powerful tools for predicting text and code patterns. However, while they can save time (though you’d be surprised at how little time their careful use saves), they often make mistakes that can be hard to detect or fail to communicate key ideas or insights at the expense of more general and banal text. As you are likely to encounter an ever-widening use of these tools when you leave Cornell, it is critical that you learn how to use these tools responsibly and effectively.\nHowever, this class will not focus on teaching you how to use LLMs responsibly; we assume that if you are using these tools, you are doing so in a way which benefits your learning and helps you communicate your already-existing understanding, rather than substituting for it. You are generally permitted to use these tools, but you are ultimately responsible for guaranteeing, understanding, and interpreting your results. If you submit work that was LLM generated, and this work receives a poor grade due to the LLM’s inability to demonstrate understanding, your grade will reflect that substantive assessment.\nAs a result, use of these tools is highly discouraged. You will have to be honest with yourself about your ability to check and correct their output, whether that is code or writing. Moreover, using them to drafting initial code or text fundamentally inhibits the learning process; working through the friction of expressing and articulating your thoughts in code or text is an important part of learning!\nGeneral guidelines for AI/ML use:\n\nAI tools for code: You may use LLM tools for code development and debugging, particularly for translating between other languages that you already have a better knowledge of (e.g. Python) and Julia. However, LLMs often make bad decisions about how to structure code, can introduce bugs (including suggesting packages that do not exist or outdated syntax), and can mislead you about what your code is doing. You are responsible for understanding and debugging any code involved in solving computational exercises. Notably, if you ask for help debugging LLM-generated code, this will not be possible unless you have already developed your own understanding of what each piece of the code does (or should do) and which parts work and don’t work.\nAI tools for writing: LLMs should not be used to generate text that you submit as your own work. The point of written assessments (including model derivations and interpretations) is to stimulate your own critical thinking and mathematical skills, and you short-cut this process by substituting LLM use for your own process of formulating and articulating ideas. Even using LLMs to do the initial writing and editing that output will result in shallower thinking. As we are not teaching you in this class to critically engage with LLMs, However, you may use LLMs or similar tools (e.g. Grammarly) to help you edit your writing. As mentioned before, you are ultimately responsible for the content of any work you turn in: if a tool used to edit grammar changed the substance of your writing, you will be responsible for the submission.\n\nTo distinguish between these permissible and impermissible uses, you are required to cite your use of the tool (as with any other external reference). In particular, you must disclose how you use the LLM and how you incorporated any output into your final submission. Failure to fully reference the interaction, as described above, will be treated as plagiarism and referred to the University accordingly. If you have any questions about whether your planned use of an AI/ML tool complies with the academic integrity policy, please consult a member of the course staff ahead of its use."
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\nOffice hours with both Prof. Srikrishnan and the TA will be available each week at times specified at the top of this syllabus. Changes to the office hour schedule (cancellations/rescheduling) will be announced in class and on Ed Discussion.\nOffice hours are intended to help all students who attend. This time is limited, and is best spent on issues that are relevant to as many students as possible. While we will do our best to answer individual questions, students asking us to verify or debug homework solutions or help with syntax will have the lowest priority (but please do ask about how to verify or debug your own solutions!). However, we are happy to discuss conceptual approaches to solving homework problems, which may help to reveal bugs.\nSpace at office hours can be limited (we may shift to the conference room in 316 Riley-Robb if offices are full and it is available). If the room is crowded and you can find an alternative source of assistance, or if your question is low priority (e.g. debugging) please be kind and make room for others.\n\nParticipation\nParticipating fully in the class allows you to gain more from the class and contribute more to the learning of your classmates. Some ways to participate include:\n\nAsking questions in class or on Ed;\nAnswering questions in class or on Ed;\nActively engaging in in-class activities;\nComing to office hours.\n\n\n\n\n\n\n\nTipWhat If I Can’t Make Office Hours?\n\n\n\nWhile we will try to select office hours that work for as much of the class as possible, both the course staff and students have busy schedules and no time will work for everyone. If you need help outside of office hours (e.g. office hours do not fit your schedule), please send an email to the TA or Prof. Srikrishnan as soon as possible. These requests may not be accepted on short notice (e.g. if you have a question about a homework due on Thursday and send a request on the immediately prior Wednesday; schedules for course staff may already be full). We recommend starting your homework promptly so you can take advantage of office hours or make an appointment over a longer period."
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\n\nTechnologies\nWe will use Canvas as a gradebook, and to distribute PDFs of readings (which also be made available through the website when possible, via the Cornell library). Ed Discussion will be used for course communications. Assignments will be submitted and graded in Gradescope.\nStudents can use any programming language they like to solve problems, though we will make notebooks and package environments available for Julia (which may help structure your assignments if you use a different language) via GitHub. If students use a language other than Julia, we may limited in the programming assistance we can provide (though we’re happy to try to help!).\nWe recommend students create a GitHub account and use GitHub to version control and share their code throughout the semester.\n\n\nGrading\nFinal grades will be computed based on the following assessment weights:\n\n\n\nAssessment\nWeight\n\n\n\n\nExercises\n5%\n\n\nQuizzes\n25%\n\n\nReadings\n10%\n\n\nHomework Assignments\n30%\n\n\nTerm Project\n30%\n\n\n\nThe following grading scale will be used to convert the numerical weighted average to letter grades:\n\n\n\nGrade\nRange\n\n\n\n\nA+\n97–100\n\n\nA\n93–97\n\n\nA-\n90–93\n\n\nB+\n87–90\n\n\nB\n83–87\n\n\nB-\n80–83\n\n\nC+\n77–80\n\n\nC\n73–77\n\n\nC-\n70–73\n\n\nD+\n67–70\n\n\nD\n63–67\n\n\nD-\n60–63\n\n\nF\n&lt; 60\n\n\n\n\n\nExercises\nExercises will be assigned in Gradescope most weeks based on recently discussed content. These will be relatively short and are intended to consolidate material recently discussed in class. The exercises will be released after Wednesday’s class and will be due prior to next Monday’s. Exercises can be retaken as many times as are desired prior to the submission deadline. They may consist of multiple choice questions or questions involving small mathematical, computational, or open-ended problems. One exercise will be dropped automatically (this is intended to account for a week in which you forget or are too busy to submit).\n\n\nQuizzes\nQuizzes will be given in-class at the end of several units. These will be in the first half of the relevant class, approximately 25 minutes long. No materials (books, notes, computers, or calculators) are allowed. Quizzes will be scanned into Gradescope for grading. The material which will be covered on the quiz will be specified ahead of time in class or on Ed Discussion. No quizzes will be dropped for students in BEE 5850, but one quiz will be dropped for students in 4850.\n\n\nReadings\nReadings will be assigned throughout the semester to help students engage with background or uses of the material for that week. These readings will be provided as PDFs on Canvas through Perusall and (when possible through open access or the Cornell library) links from the website. Students are expected to demonstrate engagement with the material through collaborative annotations, whether their own comments on parts of the reading they found interesting or had questions on or responses to other students’ annotations. Students should post overall reflections, synthesizing their takeaways from the reading with content from this or other classes or other experiences, on Ed Discussion. Both the annotations and the reflections should be completed by the start of the first lecture the next week. Students will not be graded on the quantity of annotations, but the degree to which their annotations reflect critical engagement with the reading. One reading assignment will be dropped automatically.\nStudents in BEE 5850 will additionally select a peer-reviewed journal article related to an application of data analysis and will write a short discussion paper (2-3 pages) analyzing the hypotheses and statistical choices. Students should feel free to select their own paper or can work with Prof. Srikrishnan to identify one of interest, but in all cases should discuss with Prof. Srikrishnan to ensure that the article is appropriate. The discussion paper will be due towards at the end of the semester.\n\n\nHomework Assignments\nThere will be approximately 5 homework assignments assigned. Homework assignments are intended to be more in-depth applications of course material to data analysis problems.\nYou will generally have two class weeks to work on an assignment. This is intended to provide you enough time to work on the problem and debug and evaluate your code (including troubleshooting any technical problems); these are not reasons for late submission. Each homework assignment will build on material from the prior classes and possibly from the day the homework is assigned.\nStudents are encouraged to collaborate and learn from each other on homework assignments, but students must submit their own assignments which represent their own understanding of the material.\nConsulting and referencing external resources and your peers is encouraged (engineering is a collaborative discipline!), but plagiarism is a violation of academic integrity.\nSome notes on assignment and grading logistics:\n\nHomework assignments will be distributed using GitHub Classroom. While GitHub use is not required for the class aside from accepting and cloning assignments, students are encouraged to update their GitHub repositories as they work on the assignments; this helps with answering questions, keeping solutions synced across groups, and gives you a backstop in case something goes wrong and you can’t submit your assignment on time.\nHomeworks are due by 9:00pm Eastern Time on the designed due date (usually a Thursday). Your assignment writeup should be submitted to Gradescope as a PDF with the answers to each question tagged (a failure to do this will result in a non-negotiable 10% point deduction).\nA meta-rubric is provided on the website, under the Homework page. These are not customized for each assignment but the principles will apply generally.\nNo homework assignments will be dropped, but you can turn in assignments within 24 hours of the due date with a 50% penalty. If you need a further accomodation for a particular assignment, talk to Prof. Srikrishnan before the due date. Requests for extensions made after the due date will only be considered under extraordinary and unexpected circumstances. Technical challenges submitting assignments are not acceptable reasons for extensions to be granted, and late penalties will apply.\nYour submitted homework must stand on its own! We cannot grade you on the basis of information which was not included in the submitted assignment. While regrade requests should include a justification for why your grade is incorrect, we will not consider explanations or additional reasoning outside of the submission.\n\nNo homework assignments will be automatically dropped.\n\n\nTerm Project\nThroughout the semester, students will apply the concepts and methods from class to a data set of their choosing. If a student does not have a data set in mind, we will find one which aligns with their interests.\nThe term project can be completed individually or in groups of 2. There will be three deliverables throughout the semester:\n\nA proposal (no more than 2 pages with 11 point font, 1 inch margins, not including figures or references) including:\n\nThe underlying science question for your project and any associated hypotheses;\nThe dataset(s) you will analyze and use to test your hypotheses;\nStatistical model(s) for the hypotheses and your planned strategy for analysis.\n\nAn interim report (no more than 2 pages with 11 point font, 1 inch margins, not including figures or references) summarizing progress to date, including changes from the original proposal or challenges faced and plans to overcome them.\nA final report. The report should be no more than 5 pages (11 point font, 1 inch margins), not including references and figures. More details and rubrics will be provided later in the semester.\n\n\n\nLate Work Policy\nIn general, late work can be submitted up to 24 hours late at a 50% penalty, and will not be accepted after that point. This policy may seem strict, but allows for prompt release of solutions and discussion of assignments. Please reach out as soon as possible (ideally before the due date) if legitimate circumstances emerge which prevent you from submitting work within 24 hours of the due date; we will make accomodations for approved reasons, which might included a limited extension or dropping the assignment.\n\n\nRegrade Requests\nRegrade requests can be submitted up to one week after the graded work is released on Gradescope.\nAll regrade requests must include a brief justification for the request or they will not be considered. Good justifications include (but are not limited to): - My answer agrees with the posted solution, but I still lost points. - I lost 4 points for something, but the rubric says it should only be worth 2 points. - You took points off for something, but it’s right here. - My answer is correct, even though it does not match the posted solution; here is an explanation. - There is no explanation for my grade. - I got a perfect score, but my solution has a mistake (you will receive extra credit for this! see below!) - There is a major error in the posted solution; here is an explanation (full credit for everyone, but Prof. Srikrishnan will decide what constitutes a “major error”! see below!).\n\n\n\n\n\n\nImportantWe Can Only Grade What You Submitted\n\n\n\nAll regrades will be assessed based only on the submitted work. You cannot get a higher grade by explanating what you meant (either in person or online) or by adding information or reasoning to what is submitted after the fact. The goal of the regrade is to draw attention to a potential grading problem, not to supplement the submission.\n\n\nThe first regrade request for any submission will be handled by the person who graded that homework problem. The first regrade request for any exam submission will be handled by whoever graded that exam problem. If the submission was graded by the TA, additional regrade requests for the same submission will be handled directly by the instructor. Once Prof. Srikrishnan issues a final response to a regrade request, further requests for that submission will be ignored.\n\n\n\n\n\n\nWarningRegrade Requests Can Be A Gamble!\n\n\n\nWhile you should submit regrade requests for legitimate errors, using them for fishing expeditions can also result in lost points if the TA or Prof. Srikrishnan decide that your initial grade was too lenient or if additional errors are identified.\n\n\n\n\n\n\n\n\nTipWhat If I Find A Different Type of Mistake?\n\n\n\n\nIf you submit a regrade request correctly reporting that a problem was graded too leniently — that is, that your score was higher than it should be based on the rubric — your score will be increased by the difference. For example, if your original score on a problem was 8/10 and you successfully argue that your score should have been 3/10, your new score will be 13/10. However, don’t fish — your grade might be lowered if the TA finds an independent mistake while regrading.\nIf a significant error is discovered in a posted homework solution or in the exam solutions, everyone will in the class will receive full credit for the (sub)problem. Prof. Srikrishnan will decide what is “significant”."
  }
]