{
  "hash": "12325e6a444d13323e9a651ca59bf8be",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"Probability Models and EDA\"\nsubtitle: \"Lecture 03\"\nauthor: \"Vivek Srikrishnan\"\ncourse: \"BEE 4850\"\ninstitution: \"Cornell University\"\ndate: \"January 28, 2026\"\nformat:\n    revealjs: default\nengine: julia\nfilters:\n  - code-fullscreen\n---\n\n\n\n\n\n\n\n# Review\n\n## Statistics as Decision-Making Under Uncertainty\n\n- Went over \"standard\" null hypothesis significance approach.\n- Null vs. alternative hypotheses\n- $p$-values: (continuous) assessment of probability of seeing test statistic under null hypothesis.\n\n# Probability Models\n\n## Why Do We Need Models For Data?\n\n- **Data are imperfect**: Data $X$ are only one realization of the data that **could** have been observed and/or we can only measure indirect proxies of what we care about.\n- Over time, statisticians learned to treat these imperfections as the results of **random** processes (for some of this history, see @Hacking1990-yr), requiring probability models.\n\n\n## Predicting Random Variables\n\nLet's say that we want to predict the value of a variable $y \\sim Y$. We need a criteria to define the \"best\" point prediction.\n\nReasonable starting point: \n\n$$\\text{MSE}(m) = \\mathbb{E}\\left[(Y - m)^2\\right]$$\n\n## MSE As (Squared) Bias + Variance\n\n$$\\begin{aligned}\n\\mathbb{V}(Y) &= \\mathbb{E}\\left[(Y - \\mathbb{E}[Y])^2\\right] \\\\\n&= \\mathbb{E}\\left[Y^2 - 2Y\\mathbb{E}[Y] + \\mathbb{E}[Y]^2\\right] \\\\\n&= \\mathbb{E}[Y^2] - 2\\mathbb{E}[Y]^2 + \\mathbb{E}[Y]^2 = \\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2 .\n\\end{aligned}$$\n\n::: {.fragment .fade-in}\n$$\\Rightarrow \\text{MSE}(m) = \\mathbb{E}\\left[(Y - m)^2\\right] = \\mathbb{E}\\left[(Y-m)\\right]^2 + \\mathbb{V}(Y - m).$$\n:::\n\n## Bias-Variance Decomposition of MSE\n\nThen:\n\n$$\\begin{aligned}\n\\text{MSE}(m) &= \\mathbb{E}\\left[(Y-m)\\right]^2 + \\mathbb{V}(Y - m)  \\\\\n&= \\mathbb{E}\\left[(Y-m)\\right]^2 + \\mathbb{V}(Y) \\\\\n&= \\left(\\mathbb{E}[Y] - m\\right)^2 + \\mathbb{V}(Y).\n\\end{aligned}$$\n\nThis is the source of the so-called \"bias-variance tradeoff\" (more on this later).\n\n## Optimizing...\n\nWe want to find the minimum value of $\\text{MSE}(m)$ (denote the optimal prediction by $\\mu$):\n\n::: {.fragment .fade-in}\n$$\n\\begin{aligned}\n\\frac{d\\text{MSE}}{dm} &= -2(\\mathbb{E}[Y] - m) + 0 \\\\\n0 = \\left.\\frac{d\\text{MSE}}{dm}\\right|_{m = \\mu} &= -2(\\mathbb{E}[Y] - \\mu) \n\\end{aligned}$$\n\n$$\\Rightarrow \\mu = \\mathbb{E}[Y].$$\n:::\n\n## Expected Value As Best Prediction\n\nIn other words, **the best predicted value of a random variable is its expectation**.\n\nBut:\n\n1. We usually don't have enough data to know what $\\mathbb{E}[Y]$ is: need to make probabilistic assumptions;\n2. In many applications, we don't just want a point estimate, we need some estimate of ranges of values **we might observe**.\n\n## Uses of Models\n\nWhat can we use a probability model for?\n\n1. **Summaries** of data: store $y = f(\\mathbf{x})$ instead of all data points $(y, x_1, \\ldots, x_n)$\n2. **Smooth** values by removing noise\n3. **Predict** new data (interpolation/extrapolation): $\\hat{f} = f(\\hat{\\mathbf{x}})$\n4. **Infer** relationships between variables (interpret coefficients of $f$)\n\n## My Philosophical Position\n\n:::: {.columns}\n::: {.column width=50%}\n- Probability theory helps us deduce logical implications of theories **conditional on our assumptions**\n- Cannot use an \"objective\" procedure to avoid **subjective responsibility**\n:::\n\n::: {.column width=50%}\n::: {.center}\n![Spiderman Meme](memes/peter_parker_method_assumptions.png){width=90%}\n:::\n:::\n::::\n\n# Linear Regression\n\n## Linear Regression\n\nThe \"simplest\" model is **linear**:\n\n:::: {.columns}\n::: {.column width=50%}\n$$\\begin{aligned}\ny_i &\\sim N(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\sum_j \\beta_j x^j_i\n\\end{aligned}\n$$\n:::\n::: {.column width=50%}\n$$\\begin{aligned}\ny_i &= \\sum_j \\beta_j x^j_i + \\varepsilon_i \\\\\n\\varepsilon_i &\\sim N(0, \\sigma^2)\n\\end{aligned}\n$$\n:::\n::::\n\n\n## Why Linear Models?\n\n:::: {.columns}\n::: {.column width=60%}\nTwo main reasons to use linear models/normal distributions:\n\n1. **Inferential**: \"Least informative\" distribution assuming only finite mean/variance;\n2. **Generative**: Central Limit Theorem (summed fluctuations are asymptotically normal)\n\n:::\n::: {.column width=40%}\n![Weight stack Gaussian distribution](https://i.redd.it/zl5mo1n45wyb1.jpg)\n\n::: {.caption}\nSource: r/GymMemes\n:::\n:::\n::::\n\n::: {.notes}\nOne key thing: normal distributions are the \"least informative\" distribution given constraints on mean and variance. So all else being equal, this is a useful machine if all we're interested in are those two moments.\n:::\n\n## Linear Regression As Probability Model\n\n$$\ny &= \\underbrace{\\sum_j \\beta_j x^j_i)}_{\\mathbb{E}[y | x^j]} + \\underbrace{\\varepsilon_i}_{\\text{noise}}, \\quad \\varepsilon_i \\sim N(0, \\sigma^2) \n$$\n\n1. $\\mathbb{E}[y | x^j]$: Best linear prediction of $y$ conditional on choice of predictors $x^j$.\n2. The noise defines the probability distribution (here: Gaussian) of the observations: $$y \\sim N(\\sum_j \\beta_j x^j_i), \\sigma^2).$$\n\n\n## Implications of Gaussian Noise\n\nWithout explicit modeling of sources of measurement uncertainty, in LR we can't separate a noisy system state (due to *e.g.* omitted variables) from an uncertain measurement error (more on this later).\n\n$$\n\\begin{array}{l}\nX \\sim N(\\mu_1, \\sigma_1^2) \\\\\nY \\sim N(\\mu_2, \\sigma_2^2) \\\\\nZ = X + Y\n\\end{array}\n\\qquad \\Rightarrow \\qquad Z \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\n$$\n\n# Linear Model Example\n\n## \n\n\n## Central Limit Theorem \n\nIf \n\n- $\\mathbb{E}[X_i] = \\mu$ \n- and $\\text{Var}(X_i) = \\sigma^2 < \\infty$, \n\n$$\\begin{aligned}\n\\lim_{n \\to \\infty} \\sqrt{n}(\\bar{X}_n - \\mu ) &= \\mathcal{N}(0, \\sigma^2) \\\\\n\\Rightarrow & \\bar{X}_n \\overset{\\text{approx}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2/n)\n\\end{aligned}$$\n\n## Central Limit Theorem (More Intuitive)\n\n:::: {.columns}\n::: {.column width=50%}\nFor **a large enough set of samples**: \n\nThe sampling distribution of a sum or mean of random variables is approximately normal distribution, **even if the random variables themselves are not**.\n:::\n::: {.column width=50%}\n![Small n Meme](memes/sampling_distribution_small_n.jpg)\n\n::: {.caption}\nSource: Unknown\n:::\n:::\n::::\n\n\n\n# Key Points\n\n## Probability Models\n\n- Almost anything we want to use data for involves an assumption about the underlying probability model for the data.\n- Our goal this semester is to develop a toolkit to specify, test, and use these probability models.\n\n# Upcoming Schedule\n\n## Next Classes\n\n**Wednesday**: Fitting models with maximum likelihood\n\n**Friday**: Maximum likelihood uncertainty\n\n## Assessments\n\n**Homework 1** Due Friday, 2/6.\n\n# References\n\n## References (Scroll for Full List)\n\n",
    "supporting": [
      "lecture03-1-probmodels_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}